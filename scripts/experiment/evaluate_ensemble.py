"""
Stage 2 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (TabNet + LightGBM)

è©•ä¾¡é …ç›®:
- é‡ã¿ä»˜ã‘æœ€é©åŒ– (F1æœ€å¤§åŒ–)
- Recall 99%/98%/95% æ™‚ã® Precision (å®‰å…¨è¦ä»¶)
- ãƒ¢ãƒ‡ãƒ«é–“ç›¸é–¢

æ³¨æ„: ã“ã“ã§è¨ˆç®—ã•ã‚Œã‚‹ Precision ã¯ã€ŒStage 2 å˜ä½“ã€ã®å€¤ã§ã™ã€‚
      ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã® Precision ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯ Stage 1 ã®åˆ†æ¯ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, precision_score, recall_score
import os

def evaluate_ensemble():
    print("ğŸŒ¿ Stage 2: Ensemble Evaluation (TabNet + LightGBM)")
    print("   â€» æ³¨æ„: Precision ã¯ Stage 2 å˜ä½“ã®å€¤ã§ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è©•ä¾¡ã«ã¯ Stage 1 ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚\n")
    
    path_tabnet = 'results/oof/oof_stage2_tabnet.csv'
    path_lgb = 'results/oof/oof_stage2_lightgbm.csv'
    
    if not os.path.exists(path_tabnet) or not os.path.exists(path_lgb):
        print(f"âš ï¸ OOF prediction files not found.")
        print(f"   TabNet: {os.path.exists(path_tabnet)}")
        print(f"   LightGBM: {os.path.exists(path_lgb)}")
        return
    
    # Load Data
    df_tab = pd.read_csv(path_tabnet)
    df_lgb = pd.read_csv(path_lgb)
    
    # Check consistency
    print(f"   TabNet OOF: {len(df_tab):,} rows")
    print(f"   LightGBM OOF: {len(df_lgb):,} rows")
    
    # [ä¿®æ­£] indexã®ã¿ã§ãƒãƒ¼ã‚¸ã—ã€ãã®å¾Œãƒ©ãƒ™ãƒ«ã®ä¸€è‡´ã‚’ç¢ºèª
    df = pd.merge(
        df_tab.rename(columns={'prob': 'prob_tab', 'true_label': 'label_tab'}),
        df_lgb.rename(columns={'prob': 'prob_lgb', 'true_label': 'label_lgb'}),
        on='index',
        how='inner'
    )
    print(f"   Aligned Data: {len(df):,} rows")
    
    # ãƒ©ãƒ™ãƒ«ä¸€è‡´ç¢ºèª
    label_mismatch = (df['label_tab'] != df['label_lgb']).sum()
    if label_mismatch > 0:
        print(f"   âš ï¸ è­¦å‘Š: ãƒ©ãƒ™ãƒ«ä¸ä¸€è‡´ãŒ {label_mismatch} ä»¶ã‚ã‚Šã¾ã™ï¼")
        return
    print(f"   âœ… ãƒ©ãƒ™ãƒ«ä¸€è‡´ç¢ºèªå®Œäº†")
    
    y_true = df['label_tab'].values
    prob_tab = df['prob_tab'].values
    prob_lgb = df['prob_lgb'].values
    
    # Check Correlation
    corr = np.corrcoef(prob_tab, prob_lgb)[0, 1]
    print(f"   ğŸ“Š Correlation between models: {corr:.4f}")
    
    # Helper function: ç‰¹å®šã®Recallã‚’é”æˆã™ã‚‹é–¾å€¤ã§ã®Precisionã‚’å–å¾—
    def get_precision_at_recall(prob, y_true, target_recall):
        precisions, recalls, thresholds = precision_recall_curve(y_true, prob)
        idx = np.where(recalls >= target_recall)[0]
        if len(idx) > 0:
            idx = idx[-1]  # æœ€ã‚‚é«˜ã„Recallã‚’é”æˆã™ã‚‹æœ€å¾Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            thresh = thresholds[idx] if idx < len(thresholds) else 0.0
            prec = precisions[idx]
            return thresh, prec
        return 0.0, 0.0
    
    # Search for Best Weight
    best_f1_score = -1
    best_f1_weight = -1
    best_f1_metrics = {}
    
    # é«˜Recallæ™‚ã®æœ€é©é‡ã¿ã‚‚è¿½è·¡
    best_recall99_prec = -1
    best_recall99_weight = -1
    
    print("\n   ğŸ” Searching for best weight (w * LightGBM + (1-w) * TabNet)...")
    print(f"   {'Weight':<8} {'AUC':<8} {'F1':<8} {'Prec@R99':<10} {'Prec@R98':<10} {'Prec@R95':<10}")
    print("-" * 70)
    
    for w in np.arange(0.0, 1.01, 0.05):
        prob_ens = w * prob_lgb + (1 - w) * prob_tab
        
        # Calculate AUC
        auc = roc_auc_score(y_true, prob_ens)
        
        # Find Best F1 for this weight
        precisions, recalls, thresholds = precision_recall_curve(y_true, prob_ens)
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-15)
        best_f1_idx = np.argmax(f1_scores)
        f1 = f1_scores[best_f1_idx]
        
        # [è¿½åŠ ] Recall 99%/98%/95% æ™‚ã® Precision
        _, prec_r99 = get_precision_at_recall(prob_ens, y_true, 0.99)
        _, prec_r98 = get_precision_at_recall(prob_ens, y_true, 0.98)
        _, prec_r95 = get_precision_at_recall(prob_ens, y_true, 0.95)
        
        print(f"   {w:.1f}      {auc:.4f}   {f1:.4f}   {prec_r99:.4f}      {prec_r98:.4f}      {prec_r95:.4f}")
        
        if f1 > best_f1_score:
            best_f1_score = f1
            best_f1_weight = w
            prec = precisions[best_f1_idx]
            rec = recalls[best_f1_idx]
            best_f1_metrics = {
                'auc': auc, 'f1': f1, 'precision': prec, 'recall': rec,
                'threshold': thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5
            }
        
        # é«˜Recallæ™‚ã®æœ€é©é‡ã¿ã‚’è¿½è·¡
        if prec_r99 > best_recall99_prec:
            best_recall99_prec = prec_r99
            best_recall99_weight = w
            
    print("-" * 70)
    
    # æœ€çµ‚çµæœè¡¨ç¤º
    print(f"\nğŸ† Best F1 Ensemble (w_lgb={best_f1_weight:.1f})")
    print(f"   AUC: {best_f1_metrics['auc']:.4f}")
    print(f"   F1 Score: {best_f1_metrics['f1']:.4f}")
    print(f"   Precision: {best_f1_metrics['precision']:.4f}")
    print(f"   Recall: {best_f1_metrics['recall']:.4f}")
    print(f"   Threshold: {best_f1_metrics['threshold']:.4f}")
    
    # [è¿½åŠ ] é«˜Recallæ™‚ã®è©•ä¾¡
    print(f"\nğŸ¯ High Recall Evaluation (w_lgb={best_recall99_weight:.1f})")
    prob_best = best_recall99_weight * prob_lgb + (1 - best_recall99_weight) * prob_tab
    
    for target_recall in [0.99, 0.98, 0.95]:
        thresh, prec = get_precision_at_recall(prob_best, y_true, target_recall)
        print(f"   Recall â‰¥ {target_recall:.0%}: Threshold={thresh:.4f}, Precision={prec:.4f}")
    
    # å˜ä½“ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
    print("\nğŸ“Š å˜ä½“ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ (Recall â‰¥ 99%):")
    _, prec_tab_r99 = get_precision_at_recall(prob_tab, y_true, 0.99)
    _, prec_lgb_r99 = get_precision_at_recall(prob_lgb, y_true, 0.99)
    print(f"   TabNet:   Precision@R99 = {prec_tab_r99:.4f}")
    print(f"   LightGBM: Precision@R99 = {prec_lgb_r99:.4f}")
    print(f"   Ensemble: Precision@R99 = {best_recall99_prec:.4f}")

if __name__ == "__main__":
    evaluate_ensemble()

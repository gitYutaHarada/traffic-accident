"""
ç‰¹å¾´é‡é¸æŠã‚¹ã‚¯ãƒªãƒ—ãƒˆ
==================
Permutation Importanceï¼ˆé †åˆ—é‡è¦åº¦ï¼‰ã¨å¤šé‡å…±ç·šæ€§ã®ç¢ºèªã‚’è¡Œã„ã€
å‰Šé™¤å€™è£œã¨ãªã‚‹ç‰¹å¾´é‡ã‚’ãƒ¬ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

æ³¨æ„ç‚¹:
- ç›¸é–¢è¡Œåˆ—ã¯ç´”ç²‹ãªæ•°å€¤å¤‰æ•°ã®ã¿ã«é™å®šï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¯Pearsonç›¸é–¢ãŒç„¡æ„å‘³ï¼‰
- è©•ä¾¡æŒ‡æ¨™ã¯LogLossã‚’ä½¿ç”¨ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚ˆã‚Šæ•æ„Ÿï¼‰
- prob_stage1 / logits_stage1 ã¯å‰Šé™¤ç¦æ­¢ãƒªã‚¹ãƒˆã§ä¿è­·
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
from sklearn.metrics import log_loss, make_scorer
import lightgbm as lgb
import warnings

warnings.filterwarnings('ignore')


# ============================================================
# è¨­å®š
# ============================================================

# å‰Šé™¤ç¦æ­¢ãƒªã‚¹ãƒˆï¼ˆä¿è­·ã™ã‚‹åˆ—ï¼‰
KEEP_COLS = ['prob_stage1', 'logits_stage1', 'area_id']

# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ãƒªã‚¹ãƒˆï¼ˆç›¸é–¢ãƒã‚§ãƒƒã‚¯ã‹ã‚‰é™¤å¤–ã€LightGBMã«ã¯ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦æ¸¡ã™ï¼‰
CATEGORICAL_COLS = [
    'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
    'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
    'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
    'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
    'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
]

# Permutation Importanceã§ã€Œãƒã‚¤ã‚ºã€ã¨åˆ¤æ–­ã™ã‚‹é–¾å€¤
# importance_mean <= ã“ã®å€¤ãªã‚‰å‰Šé™¤å€™è£œ
NOISE_THRESHOLD = 0.0

# ç›¸é–¢ä¿‚æ•°ã®é–¾å€¤ï¼ˆã“ã‚Œã‚’è¶…ãˆã‚‹ã¨å†—é•·ã¨ã¿ãªã™ï¼‰
CORRELATION_THRESHOLD = 0.95

# Permutation Importanceè¨ˆç®—æ™‚ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
# å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ã™ã‚‹ã¨æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦è¨ˆç®—
PI_SAMPLE_SIZE = 50000

# Permutation Importanceã®ç¹°ã‚Šè¿”ã—å›æ•°ï¼ˆå®‰å®šåŒ–ã®ãŸã‚ï¼‰
N_REPEATS = 5


class FeatureSelector:
    """ç‰¹å¾´é‡é¸æŠãƒ»åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        data_path: str = "data/processed/honhyo_clean_with_features.csv",
        target_col: str = "æ­»è€…æ•°",
        test_size: float = 0.2,
        random_state: int = 42,
    ):
        self.data_path = data_path
        self.target_col = target_col
        self.test_size = test_size
        self.random_state = random_state
        
        self.output_dir = "results/analysis"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("=" * 60)
        print("ç‰¹å¾´é‡é¸æŠã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
        print("=" * 60)
        print(f"è©•ä¾¡æŒ‡æ¨™: LogLossï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰")
        print(f"å‰Šé™¤ç¦æ­¢ãƒªã‚¹ãƒˆ: {KEEP_COLS}")
        print(f"Permutation Importance ç¹°ã‚Šè¿”ã—å›æ•°: {N_REPEATS}")
        print(f"ç›¸é–¢ä¿‚æ•°é–¾å€¤: {CORRELATION_THRESHOLD}")
        print("=" * 60)
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨Train/Validationåˆ†å‰²"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        self.df = pd.read_csv(self.data_path)
        
        y_all = self.df[self.target_col].values
        X_all = self.df.drop(columns=[self.target_col])
        
        if 'ç™ºç”Ÿæ—¥æ™‚' in X_all.columns:
            X_all = X_all.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        # Train/Validationåˆ†å‰²ï¼ˆå±¤åŒ–æŠ½å‡ºï¼‰
        self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(
            X_all, y_all, test_size=self.test_size,
            random_state=self.random_state, stratify=y_all
        )
        
        print(f"   Train: {len(self.X_train):,} (æ­£ä¾‹: {self.y_train.sum():,})")
        print(f"   Valid: {len(self.X_valid):,} (æ­£ä¾‹: {self.y_valid.sum():,})")
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¨æ•°å€¤å¤‰æ•°ã‚’åˆ†é¡
        self.categorical_cols = []
        self.numeric_cols = []
        
        for col in self.X_train.columns:
            if col in CATEGORICAL_COLS or self.X_train[col].dtype == 'object':
                self.categorical_cols.append(col)
                self.X_train[col] = self.X_train[col].astype('category')
                self.X_valid[col] = self.X_valid[col].astype('category')
            else:
                self.numeric_cols.append(col)
                self.X_train[col] = self.X_train[col].astype(np.float32)
                self.X_valid[col] = self.X_valid[col].astype(np.float32)
        
        self.feature_names = list(self.X_train.columns)
        
        print(f"   ç‰¹å¾´é‡æ•°: {len(self.feature_names)}")
        print(f"   - æ•°å€¤å¤‰æ•°: {len(self.numeric_cols)}")
        print(f"   - ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: {len(self.categorical_cols)}")
    
    def train_baseline(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        print("\nğŸŒ¿ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³LightGBMå­¦ç¿’ä¸­...")
        
        lgb_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'num_leaves': 31,
            'max_depth': 8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'is_unbalance': True,  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
            'n_estimators': 500,
            'learning_rate': 0.05,
            'n_jobs': -1,
            'random_state': self.random_state,
        }
        
        self.model = lgb.LGBMClassifier(**lgb_params)
        self.model.fit(
            self.X_train, self.y_train,
            eval_set=[(self.X_valid, self.y_valid)],
            callbacks=[lgb.early_stopping(50, verbose=False)]
        )
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®LogLoss
        y_pred_proba = self.model.predict_proba(self.X_valid)[:, 1]
        self.baseline_logloss = log_loss(self.y_valid, y_pred_proba)
        
        print(f"   ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ LogLoss: {self.baseline_logloss:.6f}")
        
        # Feature Importance (split)
        self.feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"   ä¸Šä½10ç‰¹å¾´é‡ (Feature Importance):")
        for i, row in self.feature_importance_df.head(10).iterrows():
            print(f"      {row['feature']}: {row['importance']:.0f}")
    
    def calculate_permutation_importance(self):
        """Permutation Importanceï¼ˆé †åˆ—é‡è¦åº¦ï¼‰ã®è¨ˆç®—"""
        print(f"\nğŸ”€ Permutation Importance è¨ˆç®—ä¸­... (n_repeats={N_REPEATS})")
        print("   â€» æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§å„ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã€LogLossã®æ‚ªåŒ–ã‚’æ¸¬å®š")
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
        n_valid = len(self.X_valid)
        if n_valid > PI_SAMPLE_SIZE:
            print(f"   ğŸ“‰ é«˜é€ŸåŒ–ã®ãŸã‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {n_valid:,} â†’ {PI_SAMPLE_SIZE:,} ä»¶")
            np.random.seed(self.random_state)
            sample_idx = np.random.choice(n_valid, size=PI_SAMPLE_SIZE, replace=False)
            X_valid_sample = self.X_valid.iloc[sample_idx]
            y_valid_sample = self.y_valid[sample_idx]
        else:
            X_valid_sample = self.X_valid
            y_valid_sample = self.y_valid
        
        # LogLossã‚¹ã‚³ã‚¢ãƒ©ãƒ¼: çµ„ã¿è¾¼ã¿ã® 'neg_log_loss' ã‚’ä½¿ç”¨
        # ï¼ˆmake_scorerã§ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã‚’æ¸¡ã™ã¨Python 3.14ã§å•é¡ŒãŒç™ºç”Ÿã™ã‚‹ãŸã‚ï¼‰
        
        result = permutation_importance(
            self.model,
            X_valid_sample,
            y_valid_sample,
            scoring='neg_log_loss',  # çµ„ã¿è¾¼ã¿ã‚¹ã‚³ã‚¢ãƒ©ãƒ¼ã‚’ä½¿ç”¨
            n_repeats=N_REPEATS,
            random_state=self.random_state,
            n_jobs=-1
        )
        
        # çµæœã‚’DataFrameã«æ•´ç†
        # sklearn ã® permutation_importance ã¯:
        # - ã‚¹ã‚³ã‚¢ãŒæ‚ªåŒ–ï¼ˆï¼ãã®ç‰¹å¾´é‡ãŒé‡è¦ï¼‰ã—ãŸå ´åˆ â†’ æ­£ã®å€¤ã‚’è¿”ã™
        # - ã‚¹ã‚³ã‚¢ãŒå¤‰ã‚ã‚‰ãªã„ or è‰¯ããªã‚‹ï¼ˆï¼ãƒã‚¤ã‚ºï¼‰â†’ 0ä»¥ä¸‹ã‚’è¿”ã™
        # â€» neg_log_loss ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€LogLossãŒå¢—ãˆã‚‹ã¨è² ã®ã‚¹ã‚³ã‚¢ãŒæ¸›å°‘ã—ã€
        #   importances_mean ã¯æ­£ã®å€¤ã«ãªã‚‹
        self.perm_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance_mean': result.importances_mean,  # ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæ­£=é‡è¦ã€0ä»¥ä¸‹=ãƒã‚¤ã‚ºï¼‰
            'importance_std': result.importances_std
        }).sort_values('importance_mean', ascending=False)
        
        # ãƒã‚¤ã‚ºå€™è£œã®ç‰¹å®šï¼ˆKEEP_COLSã‚’é™¤å¤–ï¼‰
        noise_candidates = self.perm_importance_df[
            (self.perm_importance_df['importance_mean'] <= NOISE_THRESHOLD) &
            (~self.perm_importance_df['feature'].isin(KEEP_COLS))
        ]['feature'].tolist()
        
        self.noise_features = noise_candidates
        
        print(f"   è¨ˆç®—å®Œäº†")
        print(f"   ãƒã‚¤ã‚ºå€™è£œ (importance <= {NOISE_THRESHOLD}): {len(noise_candidates)} ä»¶")
        
        if noise_candidates:
            print("   ãƒã‚¤ã‚ºå€™è£œãƒªã‚¹ãƒˆ:")
            for feat in noise_candidates[:10]:  # æœ€å¤§10ä»¶è¡¨ç¤º
                imp = self.perm_importance_df[self.perm_importance_df['feature'] == feat]['importance_mean'].values[0]
                print(f"      - {feat}: {imp:.6f}")
            if len(noise_candidates) > 10:
                print(f"      ... ä»– {len(noise_candidates) - 10} ä»¶")
    
    def calculate_correlation_matrix(self):
        """å¤šé‡å…±ç·šæ€§ï¼ˆç›¸é–¢è¡Œåˆ—ï¼‰ã®ç¢ºèª - æ•°å€¤å¤‰æ•°ã®ã¿"""
        print(f"\nğŸ“Š ç›¸é–¢è¡Œåˆ—è¨ˆç®—ä¸­... (æ•°å€¤å¤‰æ•°ã®ã¿, é–¾å€¤: {CORRELATION_THRESHOLD})")
        
        # KEEP_COLSã¨ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’é™¤å¤–ã—ãŸæ•°å€¤å¤‰æ•°ã®ã¿
        numeric_for_corr = [
            col for col in self.numeric_cols
            if col not in KEEP_COLS and col not in CATEGORICAL_COLS
        ]
        
        if len(numeric_for_corr) < 2:
            print("   âš ï¸ ç›¸é–¢ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®æ•°å€¤å¤‰æ•°ãŒ2ã¤æœªæº€ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            self.high_corr_pairs = []
            self.skipped_cols_for_corr = self.categorical_cols
            return
        
        print(f"   å¯¾è±¡åˆ—æ•°: {len(numeric_for_corr)}")
        print(f"   ã‚¹ã‚­ãƒƒãƒ—åˆ—ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰: {len(self.categorical_cols)}")
        
        # ç›¸é–¢è¡Œåˆ—è¨ˆç®—
        corr_matrix = self.X_train[numeric_for_corr].corr().abs()
        
        # ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’å–å¾—ï¼ˆå¯¾è§’ç·šã¨ä¸‹ä¸‰è§’ã‚’é™¤å¤–ï¼‰
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        # é«˜ç›¸é–¢ãƒšã‚¢ã‚’æŠ½å‡º
        high_corr_pairs = []
        for col in upper_tri.columns:
            for idx in upper_tri.index:
                val = upper_tri.loc[idx, col]
                if pd.notna(val) and val > CORRELATION_THRESHOLD:
                    high_corr_pairs.append({
                        'feature_1': idx,
                        'feature_2': col,
                        'correlation': val
                    })
        
        self.high_corr_pairs = sorted(high_corr_pairs, key=lambda x: -x['correlation'])
        self.skipped_cols_for_corr = self.categorical_cols
        
        print(f"   é«˜ç›¸é–¢ãƒšã‚¢ (>{CORRELATION_THRESHOLD}): {len(self.high_corr_pairs)} ä»¶")
        
        if self.high_corr_pairs:
            print("   é«˜ç›¸é–¢ãƒšã‚¢ãƒªã‚¹ãƒˆ:")
            for pair in self.high_corr_pairs[:10]:
                print(f"      - {pair['feature_1']} âŸ· {pair['feature_2']}: {pair['correlation']:.4f}")
            if len(self.high_corr_pairs) > 10:
                print(f"      ... ä»– {len(self.high_corr_pairs) - 10} ä»¶")
    
    def generate_report(self):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownã§å‡ºåŠ›"""
        print("\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        
        report_path = os.path.join(self.output_dir, "feature_selection_report.md")
        
        # Permutation Importanceä¸Šä½ãƒ»ä¸‹ä½
        perm_top10 = self.perm_importance_df.head(10)
        perm_bottom10 = self.perm_importance_df.tail(10).sort_values('importance_mean')
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹
        report_lines = [
            "# ç‰¹å¾´é‡é¸æŠãƒ¬ãƒãƒ¼ãƒˆ",
            "",
            f"**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "---",
            "",
            "## è¨­å®š",
            "",
            "| é …ç›® | å€¤ |",
            "|------|-----|",
            f"| è©•ä¾¡æŒ‡æ¨™ | LogLoss |",
            f"| Permutation Importance ç¹°ã‚Šè¿”ã—å›æ•° | {N_REPEATS} |",
            f"| ãƒã‚¤ã‚ºåˆ¤å®šé–¾å€¤ | importance <= {NOISE_THRESHOLD} |",
            f"| ç›¸é–¢ä¿‚æ•°é–¾å€¤ | > {CORRELATION_THRESHOLD} |",
            f"| å‰Šé™¤ç¦æ­¢ãƒªã‚¹ãƒˆ | {', '.join(KEEP_COLS)} |",
            "",
            "---",
            "",
            "## ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«",
            "",
            f"- **LogLoss**: {self.baseline_logloss:.6f}",
            "",
            "---",
            "",
            "## Permutation Importanceï¼ˆé †åˆ—é‡è¦åº¦ï¼‰",
            "",
            "æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§å„ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã€LogLossã®æ‚ªåŒ–åº¦åˆã„ã‚’æ¸¬å®šã—ã¾ã—ãŸã€‚",
            "å€¤ãŒå¤§ãã„ã»ã©é‡è¦ã€0ä»¥ä¸‹ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ã‚‚ç²¾åº¦ãŒå¤‰ã‚ã‚‰ãªã„ï¼ˆãƒã‚¤ã‚ºï¼‰ã€‚",
            "",
            "### ä¸Šä½10ç‰¹å¾´é‡ï¼ˆé‡è¦ï¼‰",
            "",
            "| ç‰¹å¾´é‡ | Importance (LogLossæ‚ªåŒ–é‡) | Std |",
            "|--------|---------------------------|-----|",
        ]
        
        for _, row in perm_top10.iterrows():
            report_lines.append(
                f"| {row['feature']} | {row['importance_mean']:.6f} | {row['importance_std']:.6f} |"
            )
        
        report_lines.extend([
            "",
            "### ä¸‹ä½10ç‰¹å¾´é‡ï¼ˆå‰Šé™¤å€™è£œï¼‰",
            "",
            "| ç‰¹å¾´é‡ | Importance (LogLossæ‚ªåŒ–é‡) | Std | å‰Šé™¤æ¨å¥¨ |",
            "|--------|---------------------------|-----|----------|",
        ])
        
        for _, row in perm_bottom10.iterrows():
            is_noise = row['importance_mean'] <= NOISE_THRESHOLD
            is_protected = row['feature'] in KEEP_COLS
            if is_protected:
                status = "âŒ ä¿è­·å¯¾è±¡"
            elif is_noise:
                status = "âœ… æ¨å¥¨"
            else:
                status = "-"
            report_lines.append(
                f"| {row['feature']} | {row['importance_mean']:.6f} | {row['importance_std']:.6f} | {status} |"
            )
        
        report_lines.extend([
            "",
            f"### ãƒã‚¤ã‚ºå€™è£œä¸€è¦§ï¼ˆimportance <= {NOISE_THRESHOLD}ï¼‰",
            "",
        ])
        
        if self.noise_features:
            report_lines.append("> [!WARNING]")
            report_lines.append("> ä»¥ä¸‹ã®ç‰¹å¾´é‡ã¯å‰Šé™¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            report_lines.append("")
            for feat in self.noise_features:
                imp = self.perm_importance_df[self.perm_importance_df['feature'] == feat]['importance_mean'].values[0]
                report_lines.append(f"- `{feat}`: {imp:.6f}")
        else:
            report_lines.append("ãƒã‚¤ã‚ºå€™è£œã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        report_lines.extend([
            "",
            "---",
            "",
            "## å¤šé‡å…±ç·šæ€§ï¼ˆç›¸é–¢è¡Œåˆ—ï¼‰",
            "",
            f"**å¯¾è±¡**: æ•°å€¤å¤‰æ•°ã®ã¿ï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¯Pearsonç›¸é–¢ãŒç„¡æ„å‘³ãªãŸã‚é™¤å¤–ï¼‰",
            "",
            f"### ã‚¹ã‚­ãƒƒãƒ—ã—ãŸåˆ—ï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ï¼‰",
            "",
        ])
        
        if self.skipped_cols_for_corr:
            for col in self.skipped_cols_for_corr:
                report_lines.append(f"- `{col}`")
        else:
            report_lines.append("ãªã—")
        
        report_lines.extend([
            "",
            f"### é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆç›¸é–¢ä¿‚æ•° > {CORRELATION_THRESHOLD}ï¼‰",
            "",
        ])
        
        if self.high_corr_pairs:
            report_lines.append("> [!IMPORTANT]")
            report_lines.append("> ä»¥ä¸‹ã®ãƒšã‚¢ã¯ç‰‡æ–¹ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            report_lines.append("")
            report_lines.append("| ç‰¹å¾´é‡1 | ç‰¹å¾´é‡2 | ç›¸é–¢ä¿‚æ•° |")
            report_lines.append("|---------|---------|----------|")
            for pair in self.high_corr_pairs:
                report_lines.append(
                    f"| {pair['feature_1']} | {pair['feature_2']} | {pair['correlation']:.4f} |"
                )
        else:
            report_lines.append("é«˜ç›¸é–¢ãƒšã‚¢ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        report_lines.extend([
            "",
            "---",
            "",
            "## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
            "",
            "1. **ãƒã‚¤ã‚ºå€™è£œã®å‰Šé™¤**: ä¸Šè¨˜ã®ãƒã‚¤ã‚ºå€™è£œãƒªã‚¹ãƒˆã‹ã‚‰ã€ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ã„ã¦å‰Šé™¤ã™ã‚‹åˆ—ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚",
            "2. **é«˜ç›¸é–¢ãƒšã‚¢ã®æ•´ç†**: é«˜ç›¸é–¢ãƒšã‚¢ãŒã‚ã‚‹å ´åˆã€ç‰‡æ–¹ã‚’å‰Šé™¤ã™ã‚‹ã‹ã€ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç­‰ã§çµ±åˆã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
            "3. **å†å­¦ç¿’**: ç‰¹å¾´é‡ã‚’å‰Šé™¤å¾Œã€ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¦LogLossã‚„AUCã®å¤‰åŒ–ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "",
        ])
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {report_path}")
        
        # CSVã‚‚å‡ºåŠ›
        perm_csv_path = os.path.join(self.output_dir, "permutation_importance.csv")
        self.perm_importance_df.to_csv(perm_csv_path, index=False, encoding='utf-8-sig')
        print(f"   Permutation Importance CSV: {perm_csv_path}")
        
        fi_csv_path = os.path.join(self.output_dir, "feature_importance.csv")
        self.feature_importance_df.to_csv(fi_csv_path, index=False, encoding='utf-8-sig')
        print(f"   Feature Importance CSV: {fi_csv_path}")
        
        return report_path
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        start = datetime.now()
        
        self.load_data()
        self.train_baseline()
        self.calculate_permutation_importance()
        self.calculate_correlation_matrix()
        report_path = self.generate_report()
        
        elapsed = (datetime.now() - start).total_seconds()
        
        print("\n" + "=" * 60)
        print("âœ… å®Œäº†ï¼")
        print(f"   å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}ç§’")
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print("=" * 60)
        
        return {
            'noise_features': self.noise_features,
            'high_corr_pairs': self.high_corr_pairs,
            'baseline_logloss': self.baseline_logloss,
            'report_path': report_path
        }


if __name__ == "__main__":
    selector = FeatureSelector()
    selector.run()

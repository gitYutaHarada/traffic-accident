"""
Stage 1 OOFäºˆæ¸¬å€¤ã‚’ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆIntelæœ€é©åŒ–ç‰ˆï¼‰
=====================================================
æ—¢å­˜ã®train_two_stage_or_ensemble.pyã®Stage 1éƒ¨åˆ†ã®ã¿ã‚’å®Ÿè¡Œã—ã€
OOFäºˆæ¸¬å€¤ã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬å€¤ã‚’CSVã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚

æœ€é©åŒ–:
- Intel Extension for Scikit-learn (sklearnex) ã‚’ä½¿ç”¨
- LightGBM/CatBoost ã® n_jobs/thread_count ã‚’ 8 ã«åˆ¶é™ï¼ˆP-coreæœ€é©åŒ–ï¼‰

å‡ºåŠ›:
    data/processed/stage1_oof_predictions.csv   (Train OOF)
    data/processed/stage1_test_predictions.csv  (Test)

å®Ÿè¡Œæ–¹æ³•:
    python scripts/experiment/save_stage1_oof.py
"""

# Intel Extension for Scikit-learnï¼ˆæœ€åˆã«èª­ã¿è¾¼ã‚€ï¼‰
try:
    from sklearnex import patch_sklearn
    patch_sklearn()
    print("âœ… Intel Extension for Scikit-learn ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¾ã—ãŸ")
except ImportError:
    print("âš ï¸ sklearnex ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'pip install scikit-learn-intelex' ã‚’æ¨å¥¨")

import pandas as pd
import numpy as np
import os
import gc
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import recall_score, roc_auc_score, precision_recall_curve
import lightgbm as lgb
from catboost import CatBoostClassifier
import warnings

warnings.filterwarnings('ignore')

# Intel Core Ultra 9 285K å‘ã‘æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
# P-core 8å€‹ã‚’ä¸­å¿ƒã«ä½¿ç”¨ï¼ˆE-coreã‚’ç„¡ç†ã«ä½¿ã‚ãªã„ï¼‰
N_JOBS_OPTIMAL = 8


def save_stage1_oof(
    data_path: str = "data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv",
    target_col: str = "fatal",
    n_folds: int = 5,
    random_state: int = 42,
    undersample_ratio: float = 2.0,
    n_seeds: int = 3,
    test_size: float = 0.2,
    output_dir: str = "data/processed",
):
    """Stage 1ã®OOFäºˆæ¸¬å€¤ã¨ãƒ†ã‚¹ãƒˆäºˆæ¸¬å€¤ã‚’ç”Ÿæˆã—ã¦ä¿å­˜"""
    
    print("=" * 70)
    print("Stage 1 OOFäºˆæ¸¬å€¤ ç”Ÿæˆãƒ»ä¿å­˜ (Intelæœ€é©åŒ–ç‰ˆ)")
    print(f"CPUæœ€é©åŒ–: n_jobs={N_JOBS_OPTIMAL} (P-coreå‘ã‘)")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv(data_path)
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ
    df['original_index'] = df.index
    
    y_all = df[target_col].values
    X_all = df.drop(columns=[target_col])
    
    if 'ç™ºç”Ÿæ—¥æ™‚' in X_all.columns:
        X_all = X_all.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
    
    known_categoricals = [
        'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
        'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
        'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
        'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
        'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
    ]
    
    categorical_cols = []
    numerical_cols = []
    
    for col in X_all.columns:
        if col == 'original_index':
            continue
        if col in known_categoricals or X_all[col].dtype == 'object':
            categorical_cols.append(col)
        else:
            numerical_cols.append(col)
            X_all[col] = X_all[col].astype(np.float32)
    
    # Train/Teståˆ†å‰²
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (Train: {1-test_size:.0%} / Test: {test_size:.0%})")
    X, X_test, y, y_test, idx_train, idx_test = train_test_split(
        X_all, y_all, X_all['original_index'].values,
        test_size=test_size, random_state=random_state, stratify=y_all
    )
    X = X.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
    
    print(f"   Train: æ­£ä¾‹ {y.sum():,} / {len(y):,}")
    print(f"   Test:  æ­£ä¾‹ {y_test.sum():,} / {len(y_test):,}")
    
    # OOF/ãƒ†ã‚¹ãƒˆäºˆæ¸¬å€¤ã‚’æ ¼ç´
    oof_proba_lgbm = np.zeros(len(y))
    oof_proba_catboost = np.zeros(len(y))
    test_proba_lgbm = np.zeros(len(y_test))
    test_proba_catboost = np.zeros(len(y_test))
    
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    
    # LightGBMç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X_lgbm = X.drop(columns=['original_index']).copy()
    X_test_lgbm = X_test.drop(columns=['original_index']).copy()
    for col in categorical_cols:
        if col in X_lgbm.columns:
            X_lgbm[col] = X_lgbm[col].astype('category')
            X_test_lgbm[col] = X_test_lgbm[col].astype('category')
    
    # CatBoostç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X_cat = X.drop(columns=['original_index']).copy()
    X_test_cat = X_test.drop(columns=['original_index']).copy()
    for col in categorical_cols:
        if col in X_cat.columns:
            X_cat[col] = X_cat[col].astype(str)
            X_test_cat[col] = X_test_cat[col].astype(str)
    cat_feature_indices = [X_cat.columns.get_loc(c) for c in categorical_cols if c in X_cat.columns]
    
    # ========== LightGBM ==========
    print("\nğŸŒ² LightGBM å­¦ç¿’ä¸­...")
    lgb_params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'verbosity': -1,
        'num_leaves': 31,
        'max_depth': 8,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'n_estimators': 1000,
        'learning_rate': 0.05,
        'n_jobs': N_JOBS_OPTIMAL  # P-coreæœ€é©åŒ–
    }
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_lgbm, y)):
        print(f"   Fold {fold+1}/{n_folds}...")
        X_train_full = X_lgbm.iloc[train_idx]
        y_train_full = y[train_idx]
        X_val = X_lgbm.iloc[val_idx]
        y_val = y[val_idx]
        
        fold_proba = np.zeros(len(val_idx))
        fold_test_proba = np.zeros(len(y_test))
        
        for seed_offset in range(n_seeds):
            seed = random_state + fold * 100 + seed_offset
            
            # Under-sampling
            pos_idx = np.where(y_train_full == 1)[0]
            neg_idx = np.where(y_train_full == 0)[0]
            n_neg_sample = int(len(pos_idx) * undersample_ratio)
            np.random.seed(seed)
            sampled_neg_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
            sampled_idx = np.concatenate([pos_idx, sampled_neg_idx])
            np.random.shuffle(sampled_idx)
            
            X_train_under = X_train_full.iloc[sampled_idx].copy()
            y_train_under = y_train_full[sampled_idx]
            
            for col in categorical_cols:
                if col in X_train_under.columns:
                    X_train_under[col] = X_train_under[col].astype('category')
            
            model = lgb.LGBMClassifier(**lgb_params, random_state=seed)
            model.fit(X_train_under, y_train_under, eval_set=[(X_val, y_val)],
                      callbacks=[lgb.early_stopping(50, verbose=False)])
            
            fold_proba += model.predict_proba(X_val)[:, 1] / n_seeds
            fold_test_proba += model.predict_proba(X_test_lgbm)[:, 1] / n_seeds
        
        oof_proba_lgbm[val_idx] = fold_proba
        test_proba_lgbm += fold_test_proba / n_folds
        gc.collect()
    
    lgbm_auc = roc_auc_score(y, oof_proba_lgbm)
    print(f"   LightGBM OOF AUC: {lgbm_auc:.4f}")
    
    # ========== CatBoost ==========
    print("\nğŸ± CatBoost å­¦ç¿’ä¸­...")
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_cat, y)):
        print(f"   Fold {fold+1}/{n_folds}...")
        X_train_full = X_cat.iloc[train_idx]
        y_train_full = y[train_idx]
        X_val = X_cat.iloc[val_idx]
        y_val = y[val_idx]
        
        fold_proba = np.zeros(len(val_idx))
        fold_test_proba = np.zeros(len(y_test))
        
        for seed_offset in range(n_seeds):
            seed = random_state + fold * 100 + seed_offset
            
            # Under-sampling
            pos_idx = np.where(y_train_full == 1)[0]
            neg_idx = np.where(y_train_full == 0)[0]
            n_neg_sample = int(len(pos_idx) * undersample_ratio)
            np.random.seed(seed)
            sampled_neg_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
            sampled_idx = np.concatenate([pos_idx, sampled_neg_idx])
            np.random.shuffle(sampled_idx)
            
            X_train_under = X_train_full.iloc[sampled_idx]
            y_train_under = y_train_full[sampled_idx]
            
            model = CatBoostClassifier(
                iterations=1000,
                learning_rate=0.05,
                depth=8,
                l2_leaf_reg=3,
                loss_function='Logloss',
                eval_metric='AUC',
                random_seed=seed,
                verbose=False,
                early_stopping_rounds=50,
                task_type='CPU',
                thread_count=N_JOBS_OPTIMAL,  # P-coreæœ€é©åŒ–
                cat_features=cat_feature_indices
            )
            model.fit(X_train_under, y_train_under, eval_set=(X_val, y_val), verbose=False)
            
            fold_proba += model.predict_proba(X_val)[:, 1] / n_seeds
            fold_test_proba += model.predict_proba(X_test_cat)[:, 1] / n_seeds
        
        oof_proba_catboost[val_idx] = fold_proba
        test_proba_catboost += fold_test_proba / n_folds
        gc.collect()
    
    catboost_auc = roc_auc_score(y, oof_proba_catboost)
    print(f"   CatBoost OOF AUC: {catboost_auc:.4f}")
    
    # ========== ä¿å­˜ ==========
    print("\nğŸ’¾ äºˆæ¸¬å€¤ã‚’ä¿å­˜ä¸­...")
    os.makedirs(output_dir, exist_ok=True)
    
    # OOF (Train)
    oof_df = pd.DataFrame({
        'original_index': idx_train,
        'prob_lgbm': oof_proba_lgbm,
        'prob_catboost': oof_proba_catboost,
        'target': y
    })
    oof_path = os.path.join(output_dir, "stage1_oof_predictions.csv")
    oof_df.to_csv(oof_path, index=False)
    print(f"   OOFä¿å­˜å®Œäº†: {oof_path}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(oof_df):,}, æ­£ä¾‹: {oof_df['target'].sum():,}")
    
    # Test
    test_df = pd.DataFrame({
        'original_index': idx_test,
        'prob_lgbm': test_proba_lgbm,
        'prob_catboost': test_proba_catboost,
        'target': y_test
    })
    test_path = os.path.join(output_dir, "stage1_test_predictions.csv")
    test_df.to_csv(test_path, index=False)
    print(f"   Testä¿å­˜å®Œäº†: {test_path}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(test_df):,}, æ­£ä¾‹: {test_df['target'].sum():,}")
    
    # ========== ç¾çŠ¶è©•ä¾¡ï¼ˆå‚è€ƒç”¨ï¼‰==========
    print("\nğŸ“ˆ ç¾çŠ¶è©•ä¾¡ï¼ˆå‚è€ƒç”¨ï¼‰...")
    print("âš ï¸ æ³¨æ„: ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šç¢ºç‡ã®çµ¶å¯¾å€¤ã¯ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™")
    
    target_recall = 0.995
    prob_max = np.maximum(oof_proba_lgbm, oof_proba_catboost)
    precision_arr, recall_arr, thresh_arr = precision_recall_curve(y, prob_max)
    
    valid_idx = np.where(recall_arr >= target_recall)[0]
    if len(valid_idx) > 0:
        best_idx = valid_idx[-1]
        thresh_max = thresh_arr[best_idx] if best_idx < len(thresh_arr) else 0
        actual_recall = recall_arr[best_idx]
        pass_rate = (prob_max >= thresh_max).mean()
    else:
        thresh_max, actual_recall, pass_rate = 0, 1.0, 1.0
    
    print(f"   Max Probabilityé–¾å€¤: {thresh_max:.4f}")
    print(f"   Recall: {actual_recall:.4f}")
    print(f"   Pass Rate: {pass_rate:.2%}")
    
    print("\n" + "=" * 70)
    print("âœ… å®Œäº†ï¼")
    print("=" * 70)
    
    return oof_df, test_df


if __name__ == "__main__":
    save_stage1_oof()

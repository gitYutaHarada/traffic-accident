"""
4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« èª¤å·®åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ (v3: å¼·åŒ–ç‰ˆ)
==============================================================
ç›®çš„: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒé–“é•ãˆãŸäº‹ä¾‹ (FP/FN) ã®ç‰¹æ€§ã‚’ç‰¹å®šã—ã€
      ä»Šå¾Œã®ãƒ¢ãƒ‡ãƒ«æ”¹å–„æ–¹é‡ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã®æ ¹æ‹ ã‚’æä¾›ã™ã‚‹ã€‚

v3ä¿®æ­£ç‚¹:
- å…ƒãƒ‡ãƒ¼ã‚¿ã¨OOFäºˆæ¸¬å€¤ã‚’æ­£ç¢ºã«ç´ä»˜ã‘ï¼ˆå¤šé‡æ¤œè¨¼ä»˜ãï¼‰
- ãƒ¢ãƒ‡ãƒ«é–“ã®ç›¸é–¢ä¿‚æ•°åˆ†æã‚’è¿½åŠ 
- Seabornã‚’ä½¿ã£ãŸå¯è¦–åŒ–ï¼ˆKDEãƒ—ãƒ­ãƒƒãƒˆï¼‰
- ç‰¹å¾´é‡åˆ¥ã‚¨ãƒ©ãƒ¼ç‡ã®æ£’ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
- æ··åŒè¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
- ã‚³ãƒ¼ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

ä½¿ç”¨æ–¹æ³•:
    python scripts/experiment/analyze_ensemble_errors.py
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_recall_curve, 
    f1_score, confusion_matrix
)

# ç‰¹å®šã®è­¦å‘Šã®ã¿æŠ‘åˆ¶ï¼ˆFutureWarningç­‰ï¼‰
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# --- è¨­å®š ---
DATA_PATH = Path("data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv")
STAGE1_OOF_PATH = Path("data/processed/stage1_oof_predictions.csv")
ENSEMBLE_OOF_PATH = Path("results/tabnet_optimized/oof_predictions.csv")
OUTPUT_DIR = Path("results/error_analysis")
os.makedirs(OUTPUT_DIR, exist_ok=True)

RANDOM_STATE = 42
TEST_SIZE = 0.2
STAGE1_RECALL_TARGET = 0.98

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š (Matplotlib/Seaborn)
plt.rcParams['font.family'] = 'MS Gothic'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# åˆ†æå¯¾è±¡ã®ç‰¹å¾´é‡ã‚«ãƒ©ãƒ 
ANALYSIS_COLS = [
    'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹',
    'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ', 'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'æ™‚', 'æœˆ',
    'æ­©è»Šé“åŒºåˆ†', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'road_type'
]

# ãƒ¢ãƒ‡ãƒ«åã®ãƒãƒƒãƒ”ãƒ³ã‚°
MODEL_COLS = ['pred_lgbm', 'pred_catboost', 'pred_mlp', 'pred_tabnet_optimized']
MODEL_NAMES = ['LightGBM', 'CatBoost', 'MLP', 'TabNet']


def load_and_align_data():
    """
    å…ƒãƒ‡ãƒ¼ã‚¿ã¨OOFäºˆæ¸¬å€¤ã‚’æ­£ç¢ºã«ç´ä»˜ã‘ã‚‹
    
    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    1. å…ƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    2. åŒã˜ã‚·ãƒ¼ãƒ‰ã§80/20åˆ†å‰²ã—ã¦Trainã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    3. Stage 1ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å†ç¾ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿é€šéã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    4. å¤šé‡æ¤œè¨¼å¾Œã€å…ƒãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã¨OOFäºˆæ¸¬å€¤ã‚’ç´ä»˜ã‘
    """
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ç´ä»˜ã‘ä¸­...")
    
    # å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_full = pd.read_csv(DATA_PATH)
    df_full['fatal'] = df_full['fatal'].astype(int)
    print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(df_full):,} è¡Œ")
    
    # Stage 1 OOFèª­ã¿è¾¼ã¿ï¼ˆTrainéƒ¨åˆ†ã®äºˆæ¸¬å€¤ï¼‰
    stage1_oof = pd.read_csv(STAGE1_OOF_PATH)
    print(f"   Stage1 OOF: {len(stage1_oof):,} è¡Œ")
    
    # Ensemble OOFèª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬å€¤ï¼‰
    ensemble_oof = pd.read_csv(ENSEMBLE_OOF_PATH)
    print(f"   Ensemble OOF: {len(ensemble_oof):,} è¡Œ")
    
    # === Step 1: 80/20åˆ†å‰²ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†ç¾ ===
    all_indices = np.arange(len(df_full))
    train_indices, _ = train_test_split(
        all_indices, 
        test_size=TEST_SIZE, 
        random_state=RANDOM_STATE, 
        stratify=df_full['fatal']
    )
    print(f"   Trainåˆ†å‰²: {len(train_indices):,} è¡Œ")
    
    # === Step 2: Stage 1ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤ã‚’å†è¨ˆç®— ===
    y_train = df_full.iloc[train_indices]['fatal'].values
    stage1_prob = 0.85 * stage1_oof['prob_catboost'].values + 0.15 * stage1_oof['prob_lgbm'].values
    
    precision, recall, thresholds = precision_recall_curve(y_train, stage1_prob)
    valid_idx = np.where(recall[:-1] >= STAGE1_RECALL_TARGET)[0]
    stage1_threshold = thresholds[valid_idx[-1]] if len(valid_idx) > 0 else 0.0
    print(f"   Stage1é–¾å€¤: {stage1_threshold:.6f}")
    
    # === Step 3: ãƒ•ã‚£ãƒ«ã‚¿é€šéã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾— ===
    filter_mask = stage1_prob >= stage1_threshold
    filtered_train_indices = train_indices[filter_mask]
    print(f"   ãƒ•ã‚£ãƒ«ã‚¿é€šé: {len(filtered_train_indices):,} è¡Œ")
    
    # === Step 4: å¤šé‡æ¤œè¨¼ ===
    if len(filtered_train_indices) != len(ensemble_oof):
        raise ValueError(f"âŒ è¡Œæ•°ä¸ä¸€è‡´: filtered={len(filtered_train_indices)}, oof={len(ensemble_oof)}")
    
    original_target = df_full.iloc[filtered_train_indices]['fatal'].values
    oof_target = ensemble_oof['target'].values
    
    if not np.array_equal(original_target, oof_target):
        raise ValueError("âŒ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚ç´ä»˜ã‘ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
    
    original_fatal_positions = np.where(original_target == 1)[0]
    oof_fatal_positions = np.where(oof_target == 1)[0]
    
    if not np.array_equal(original_fatal_positions, oof_fatal_positions):
        raise ValueError("âŒ fatal=1ã®ä½ç½®ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚ç´ä»˜ã‘ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
    
    print("   âœ… å¤šé‡æ¤œè¨¼ãƒ‘ã‚¹:")
    print(f"      - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®Œå…¨ä¸€è‡´: True")
    print(f"      - fatal=1ä½ç½®ä¸€è‡´: True ({len(original_fatal_positions):,} ä»¶)")
    print(f"      - ä¿¡é ¼åº¦: HIGH")
    
    # === Step 5: ç´ä»˜ã‘ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ ===
    df_aligned = df_full.iloc[filtered_train_indices].reset_index(drop=True).copy()
    
    for col in ensemble_oof.columns:
        df_aligned[f'pred_{col}'] = ensemble_oof[col].values
    
    return df_aligned


def find_optimal_threshold(y_true, y_prob):
    """F1ã‚¹ã‚³ã‚¢æœ€å¤§åŒ–é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹"""
    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
    
    f1_scores = np.where(
        (precision + recall) > 0,
        2 * precision * recall / (precision + recall),
        0
    )
    
    best_idx = np.argmax(f1_scores[:-1])
    return thresholds[best_idx], f1_scores[best_idx]


def extract_hard_examples(df, threshold):
    """FP/FN/TP/TN ã‚’æŠ½å‡º"""
    y_true = df['fatal'].values
    y_prob = df['pred_ensemble'].values
    y_pred = (y_prob >= threshold).astype(int)
    
    tp_mask = (y_true == 1) & (y_pred == 1)
    tn_mask = (y_true == 0) & (y_pred == 0)
    fp_mask = (y_true == 0) & (y_pred == 1)
    fn_mask = (y_true == 1) & (y_pred == 0)
    
    stats = {
        'TP': tp_mask.sum(),
        'TN': tn_mask.sum(),
        'FP': fp_mask.sum(),
        'FN': fn_mask.sum(),
    }
    
    print(f"\nğŸ“Š åˆ†é¡çµæœ (é–¾å€¤: {threshold:.4f})")
    print(f"   TP (æ­£ã—ãæ¤œå‡º): {stats['TP']:,}")
    print(f"   TN (æ­£ã—ãæ£„å´): {stats['TN']:,}")
    print(f"   FP (èª¤æ¤œçŸ¥):    {stats['FP']:,}")
    print(f"   FN (è¦‹é€ƒã—):    {stats['FN']:,}")
    
    return tp_mask, tn_mask, fp_mask, fn_mask, stats


def plot_confusion_matrix(stats, output_dir):
    """æ··åŒè¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ"""
    print("\nğŸ“Š æ··åŒè¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ...")
    
    cm = np.array([
        [stats['TN'], stats['FP']],
        [stats['FN'], stats['TP']]
    ])
    
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt=',d', cmap='Blues',
        xticklabels=['äºˆæ¸¬: éæ­»äº¡', 'äºˆæ¸¬: æ­»äº¡'],
        yticklabels=['å®Ÿéš›: éæ­»äº¡', 'å®Ÿéš›: æ­»äº¡'],
        ax=ax
    )
    ax.set_title('æ··åŒè¡Œåˆ—', fontsize=14)
    ax.set_xlabel('äºˆæ¸¬', fontsize=12)
    ax.set_ylabel('å®Ÿéš›', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(output_dir / "confusion_matrix.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: confusion_matrix.png")


def analyze_prediction_distribution(df, tp_mask, tn_mask, fp_mask, fn_mask, output_dir):
    """FP/FN vs TP/TN ã®äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒã‚’æ¯”è¼ƒï¼ˆSeaborn KDEãƒ—ãƒ­ãƒƒãƒˆï¼‰"""
    print("\nğŸ“ˆ äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒã®åˆ†æ (Seaborn)...")
    
    y_prob = df['pred_ensemble'].values
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # FP vs TN ã®æ¯”è¼ƒ
    ax1 = axes[0]
    df_plot1 = pd.DataFrame({
        'pred': np.concatenate([y_prob[fp_mask], y_prob[tn_mask]]),
        'type': ['FP (èª¤æ¤œçŸ¥)'] * fp_mask.sum() + ['TN (æ­£è§£)'] * tn_mask.sum()
    })
    sns.histplot(
        data=df_plot1, x='pred', hue='type', kde=True,
        palette={'FP (èª¤æ¤œçŸ¥)': 'red', 'TN (æ­£è§£)': 'blue'},
        alpha=0.5, ax=ax1, stat='density'
    )
    ax1.set_xlabel('äºˆæ¸¬ç¢ºç‡')
    ax1.set_ylabel('å¯†åº¦')
    ax1.set_title('FP vs TN ã®äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ')
    ax1.legend(title='')
    
    # FN vs TP ã®æ¯”è¼ƒ
    ax2 = axes[1]
    df_plot2 = pd.DataFrame({
        'pred': np.concatenate([y_prob[fn_mask], y_prob[tp_mask]]),
        'type': ['FN (è¦‹é€ƒã—)'] * fn_mask.sum() + ['TP (æ­£è§£)'] * tp_mask.sum()
    })
    sns.histplot(
        data=df_plot2, x='pred', hue='type', kde=True,
        palette={'FN (è¦‹é€ƒã—)': 'orange', 'TP (æ­£è§£)': 'green'},
        alpha=0.5, ax=ax2, stat='density'
    )
    ax2.set_xlabel('äºˆæ¸¬ç¢ºç‡')
    ax2.set_ylabel('å¯†åº¦')
    ax2.set_title('FN vs TP ã®äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ')
    ax2.legend(title='')
    
    plt.tight_layout()
    plt.savefig(output_dir / "prediction_distribution.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: prediction_distribution.png")


def analyze_model_correlation(df, tp_mask, tn_mask, fp_mask, fn_mask, output_dir):
    """
    ã‚¨ãƒ©ãƒ¼äº‹ä¾‹ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«é–“ã®ç›¸é–¢ä¿‚æ•°ã‚’åˆ†æ
    
    ç›¸é–¢ãŒé«˜ã„ â†’ å…¨ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜é–“é•ã„ã‚’ã—ã¦ã„ã‚‹ â†’ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®é›£æ˜“åº¦ãŒé«˜ã„
    ç›¸é–¢ãŒä½ã„ â†’ ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã ã‘é–“é•ãˆã¦ã„ã‚‹ â†’ ãã®ãƒ¢ãƒ‡ãƒ«ã«æ”¹å–„ä½™åœ°ã‚ã‚Š
    """
    print("\nğŸ”— ãƒ¢ãƒ‡ãƒ«é–“ç›¸é–¢ä¿‚æ•°åˆ†æ...")
    
    model_cols = [c for c in MODEL_COLS if c in df.columns]
    
    # å„ã‚±ãƒ¼ã‚¹ã§ã®ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
    cases = {
        'FP (èª¤æ¤œçŸ¥)': fp_mask,
        'FN (è¦‹é€ƒã—)': fn_mask,
        'TP (æ­£æ¤œå‡º)': tp_mask,
        'TN (æ­£æ£„å´)': tn_mask,
        'å…¨ãƒ‡ãƒ¼ã‚¿': np.ones(len(df), dtype=bool)
    }
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    correlation_results = {}
    
    for idx, (case_name, mask) in enumerate(cases.items()):
        if mask.sum() < 10:  # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
        
        df_subset = df.loc[mask, model_cols]
        corr_matrix = df_subset.corr()
        
        # ç›¸é–¢è¡Œåˆ—ã®è¦ç´„çµ±è¨ˆ
        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        avg_corr = upper_tri.stack().mean()
        
        correlation_results[case_name] = {
            'avg_correlation': avg_corr,
            'sample_count': mask.sum()
        }
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if idx < len(axes):
            ax = axes[idx]
            sns.heatmap(
                corr_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r',
                vmin=0, vmax=1, center=0.5,
                xticklabels=MODEL_NAMES, yticklabels=MODEL_NAMES,
                ax=ax
            )
            ax.set_title(f'{case_name}\n(N={mask.sum():,}, å¹³å‡ç›¸é–¢={avg_corr:.2f})')
    
    # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
    for idx in range(len(cases), len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.savefig(output_dir / "model_correlation.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: model_correlation.png")
    
    # ç›¸é–¢åˆ†æã®è§£é‡ˆ
    print("\n   ğŸ“Š ç›¸é–¢åˆ†æçµæœ:")
    for case_name, result in correlation_results.items():
        interpretation = ""
        if case_name in ['FP (èª¤æ¤œçŸ¥)', 'FN (è¦‹é€ƒã—)']:
            if result['avg_correlation'] > 0.7:
                interpretation = "â†’ å…¨ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜é–“é•ã„ï¼ˆãƒ‡ãƒ¼ã‚¿é›£æ˜“åº¦ãŒé«˜ã„ï¼‰"
            elif result['avg_correlation'] < 0.4:
                interpretation = "â†’ ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã®å•é¡Œï¼ˆæ”¹å–„ä½™åœ°ã‚ã‚Šï¼‰"
            else:
                interpretation = "â†’ ä¸­ç¨‹åº¦ã®ç›¸é–¢"
        print(f"      {case_name}: å¹³å‡ç›¸é–¢={result['avg_correlation']:.3f} {interpretation}")
    
    return correlation_results


def analyze_model_disagreement(df, fp_mask, fn_mask, output_dir):
    """å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã®ä¸ä¸€è‡´ã‚’åˆ†æï¼ˆå¹³å‡å€¤æ¯”è¼ƒï¼‰"""
    print("\nğŸ” ãƒ¢ãƒ‡ãƒ«é–“ã®äºˆæ¸¬ä¸ä¸€è‡´åˆ†æ...")
    
    model_cols = [c for c in MODEL_COLS if c in df.columns]
    
    results = {'FP': {}, 'FN': {}}
    
    for model, name in zip(model_cols, MODEL_NAMES):
        results['FP'][name] = df.loc[fp_mask, model].mean()
        results['FN'][name] = df.loc[fn_mask, model].mean()
    
    print("\n   FP/FN æ™‚ã®å„ãƒ¢ãƒ‡ãƒ«å¹³å‡äºˆæ¸¬ç¢ºç‡:")
    print("   " + "-" * 50)
    print(f"   {'Model':<20} {'FP Mean':>12} {'FN Mean':>12}")
    print("   " + "-" * 50)
    for name in MODEL_NAMES:
        if name in results['FP']:
            print(f"   {name:<20} {results['FP'][name]:>12.4f} {results['FN'][name]:>12.4f}")
    print("   " + "-" * 50)
    
    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    ax1 = axes[0]
    fp_means = [results['FP'].get(n, 0) for n in MODEL_NAMES]
    colors = sns.color_palette("Reds", len(MODEL_NAMES))
    ax1.bar(MODEL_NAMES, fp_means, color=colors)
    ax1.axhline(df.loc[fp_mask, 'pred_ensemble'].mean(), color='darkred', linestyle='--', 
                label=f'Ensembleå¹³å‡: {df.loc[fp_mask, "pred_ensemble"].mean():.4f}')
    ax1.set_ylabel('å¹³å‡äºˆæ¸¬ç¢ºç‡')
    ax1.set_title('False Positive: å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡äºˆæ¸¬å€¤')
    ax1.legend()
    
    ax2 = axes[1]
    fn_means = [results['FN'].get(n, 0) for n in MODEL_NAMES]
    colors = sns.color_palette("Oranges", len(MODEL_NAMES))
    ax2.bar(MODEL_NAMES, fn_means, color=colors)
    ax2.axhline(df.loc[fn_mask, 'pred_ensemble'].mean(), color='darkorange', linestyle='--',
                label=f'Ensembleå¹³å‡: {df.loc[fn_mask, "pred_ensemble"].mean():.4f}')
    ax2.set_ylabel('å¹³å‡äºˆæ¸¬ç¢ºç‡')
    ax2.set_title('False Negative: å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡äºˆæ¸¬å€¤')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig(output_dir / "model_disagreement.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: model_disagreement.png")
    
    return results


def analyze_feature_distribution(df, tp_mask, tn_mask, fp_mask, fn_mask, output_dir):
    """FP/FN vs TP/TN ã®ç‰¹å¾´é‡åˆ†å¸ƒã‚’æ¯”è¼ƒåˆ†æ"""
    print("\nğŸ” ç‰¹å¾´é‡åˆ¥ã‚¨ãƒ©ãƒ¼ç‡åˆ†æ...")
    
    results = []
    
    for col in ANALYSIS_COLS:
        if col not in df.columns:
            continue
        
        for cat in df[col].dropna().unique():
            cat_mask = df[col] == cat
            
            n_total = cat_mask.sum()
            n_positive = (cat_mask & (df['fatal'] == 1)).sum()
            n_fp = (cat_mask & fp_mask).sum()
            n_fn = (cat_mask & fn_mask).sum()
            n_tp = (cat_mask & tp_mask).sum()
            n_tn = (cat_mask & tn_mask).sum()
            
            fp_rate = n_fp / (n_fp + n_tn) if (n_fp + n_tn) > 0 else 0
            fn_rate = n_fn / (n_fn + n_tp) if (n_fn + n_tp) > 0 else 0
            
            results.append({
                'feature': col,
                'category': cat,
                'total': n_total,
                'positive': n_positive,
                'positive_rate': n_positive / n_total if n_total > 0 else 0,
                'FP': n_fp, 'FN': n_fn, 'TP': n_tp, 'TN': n_tn,
                'FP_rate': fp_rate, 'FN_rate': fn_rate,
            })
    
    results_df = pd.DataFrame(results)
    
    # å…¨ä½“å¹³å‡ã‚¨ãƒ©ãƒ¼ç‡
    overall_fp_rate = fp_mask.sum() / (fp_mask.sum() + tn_mask.sum())
    overall_fn_rate = fn_mask.sum() / (fn_mask.sum() + tp_mask.sum())
    
    results_df['FP_rate_ratio'] = results_df['FP_rate'] / overall_fp_rate
    results_df['FN_rate_ratio'] = results_df['FN_rate'] / overall_fn_rate
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã‚‚ã®ã‚’é™¤å¤–
    results_df = results_df[results_df['total'] >= 100]
    
    high_fp_risk = results_df[results_df['FP_rate_ratio'] > 1.5].sort_values('FP_rate_ratio', ascending=False)
    high_fn_risk = results_df[results_df['FN_rate_ratio'] > 1.5].sort_values('FN_rate_ratio', ascending=False)
    
    print(f"\n   ğŸ”´ é«˜FPãƒªã‚¹ã‚¯ Top 10:")
    for _, row in high_fp_risk.head(10).iterrows():
        print(f"      {row['feature']}={row['category']}: FPç‡ {row['FP_rate']:.3f} ({row['FP_rate_ratio']:.1f}x)")
    
    print(f"\n   ğŸŸ  é«˜FNãƒªã‚¹ã‚¯ Top 10:")
    for _, row in high_fn_risk.head(10).iterrows():
        print(f"      {row['feature']}={row['category']}: FNç‡ {row['FN_rate']:.3f} ({row['FN_rate_ratio']:.1f}x)")
    
    results_df.to_csv(output_dir / "feature_error_analysis.csv", index=False)
    print(f"\n   ä¿å­˜: feature_error_analysis.csv")
    
    return results_df, high_fp_risk, high_fn_risk


def plot_feature_error_rates(results_df, output_dir):
    """ç‰¹å¾´é‡åˆ¥ã‚¨ãƒ©ãƒ¼ç‡ã®æ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
    print("\nï¿½ ç‰¹å¾´é‡åˆ¥ã‚¨ãƒ©ãƒ¼ç‡ã‚°ãƒ©ãƒ•ä½œæˆ...")
    
    # å„ç‰¹å¾´é‡ã®ä¸­ã§æœ€ã‚‚ã‚¨ãƒ©ãƒ¼ç‡æ¯”ãŒé«˜ã„ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
    top_fp_by_feature = results_df.loc[results_df.groupby('feature')['FP_rate_ratio'].idxmax()]
    top_fn_by_feature = results_df.loc[results_df.groupby('feature')['FN_rate_ratio'].idxmax()]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # FPç‡ãŒé«˜ã„ç‰¹å¾´é‡Ã—ã‚«ãƒ†ã‚´ãƒª
    ax1 = axes[0]
    top_fp = top_fp_by_feature.nlargest(10, 'FP_rate_ratio')
    labels = [f"{row['feature']}\n({row['category']})" for _, row in top_fp.iterrows()]
    values = top_fp['FP_rate_ratio'].values
    colors = sns.color_palette("Reds_r", len(labels))
    ax1.barh(labels, values, color=colors)
    ax1.axvline(1.0, color='gray', linestyle='--', label='å…¨ä½“å¹³å‡')
    ax1.set_xlabel('FPç‡æ¯” (å…¨ä½“å¹³å‡=1.0)')
    ax1.set_title('é«˜FPãƒªã‚¹ã‚¯ç‰¹å¾´ Top 10')
    ax1.legend()
    
    # FNç‡ãŒé«˜ã„ç‰¹å¾´é‡Ã—ã‚«ãƒ†ã‚´ãƒª
    ax2 = axes[1]
    top_fn = top_fn_by_feature.nlargest(10, 'FN_rate_ratio')
    labels = [f"{row['feature']}\n({row['category']})" for _, row in top_fn.iterrows()]
    values = top_fn['FN_rate_ratio'].values
    colors = sns.color_palette("Oranges_r", len(labels))
    ax2.barh(labels, values, color=colors)
    ax2.axvline(1.0, color='gray', linestyle='--', label='å…¨ä½“å¹³å‡')
    ax2.set_xlabel('FNç‡æ¯” (å…¨ä½“å¹³å‡=1.0)')
    ax2.set_title('é«˜FNãƒªã‚¹ã‚¯ç‰¹å¾´ Top 10')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig(output_dir / "feature_error_rates.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: feature_error_rates.png")


def save_hard_examples_with_features(df, fp_mask, fn_mask, output_dir):
    """é«˜é›£æ˜“åº¦äº‹ä¾‹ã‚’ç‰¹å¾´é‡ä»˜ãã§CSVã«ä¿å­˜"""
    print("\nğŸ’¾ é«˜é›£æ˜“åº¦äº‹ä¾‹ã®ä¿å­˜...")
    
    save_cols = ANALYSIS_COLS + ['fatal'] + MODEL_COLS + ['pred_ensemble']
    save_cols = [c for c in save_cols if c in df.columns]
    
    fp_df = df.loc[fp_mask, save_cols].copy()
    fp_df['error_type'] = 'FP'
    fp_df = fp_df.sort_values('pred_ensemble', ascending=False)
    
    fn_df = df.loc[fn_mask, save_cols].copy()
    fn_df['error_type'] = 'FN'
    fn_df = fn_df.sort_values('pred_ensemble', ascending=True)
    
    fp_df.to_csv(output_dir / "false_positives_all.csv", index=False)
    fn_df.to_csv(output_dir / "false_negatives_all.csv", index=False)
    print(f"   ä¿å­˜: false_positives_all.csv ({len(fp_df):,} ä»¶)")
    print(f"   ä¿å­˜: false_negatives_all.csv ({len(fn_df):,} ä»¶)")
    
    high_confidence_fp = fp_df[fp_df['pred_ensemble'] > 0.5]
    if len(high_confidence_fp) > 0:
        high_confidence_fp.to_csv(output_dir / "high_confidence_fp.csv", index=False)
        print(f"   ä¿å­˜: high_confidence_fp.csv ({len(high_confidence_fp):,} ä»¶)")
    
    low_confidence_fn = fn_df[fn_df['pred_ensemble'] < 0.1]
    if len(low_confidence_fn) > 0:
        low_confidence_fn.to_csv(output_dir / "low_confidence_fn.csv", index=False)
        print(f"   ä¿å­˜: low_confidence_fn.csv ({len(low_confidence_fn):,} ä»¶)")


def generate_report(df, stats, threshold, best_f1, high_fp_risk, high_fn_risk, 
                    model_disagreement, correlation_results, output_dir):
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
    print("\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    
    total = stats['TP'] + stats['TN'] + stats['FP'] + stats['FN']
    precision = stats['TP'] / (stats['TP'] + stats['FP']) if (stats['TP'] + stats['FP']) > 0 else 0
    recall = stats['TP'] / (stats['TP'] + stats['FN']) if (stats['TP'] + stats['FN']) > 0 else 0
    
    fp_risk_table = ""
    for _, row in high_fp_risk.head(15).iterrows():
        fp_risk_table += f"| {row['feature']} | {row['category']} | {row['total']:,} | {row['FP_rate']:.3f} | {row['FP_rate_ratio']:.1f}x |\n"
    
    fn_risk_table = ""
    for _, row in high_fn_risk.head(15).iterrows():
        fn_risk_table += f"| {row['feature']} | {row['category']} | {row['total']:,} | {row['FN_rate']:.3f} | {row['FN_rate_ratio']:.1f}x |\n"
    
    # ç›¸é–¢åˆ†æã®è§£é‡ˆ
    fp_corr = correlation_results.get('FP (èª¤æ¤œçŸ¥)', {}).get('avg_correlation', 0)
    fn_corr = correlation_results.get('FN (è¦‹é€ƒã—)', {}).get('avg_correlation', 0)
    
    if fp_corr > 0.7:
        fp_interpretation = "å…¨ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜é–“é•ã„ã‚’ã—ã¦ãŠã‚Šã€**ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®äºˆæ¸¬é›£æ˜“åº¦ãŒé«˜ã„**ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚"
    elif fp_corr < 0.4:
        fp_interpretation = "ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã ã‘ãŒé–“é•ãˆã¦ã„ã‚‹å‚¾å‘ãŒã‚ã‚Šã€**ãã®ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ä½™åœ°**ãŒã‚ã‚Šã¾ã™ã€‚"
    else:
        fp_interpretation = "ä¸­ç¨‹åº¦ã®ç›¸é–¢ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«é–“ã§éƒ¨åˆ†çš„ã«å…±é€šã—ãŸé–“é•ã„ã‚’ã—ã¦ã„ã¾ã™ã€‚"
    
    if fn_corr > 0.7:
        fn_interpretation = "å…¨ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜äº‹ä¾‹ã‚’è¦‹é€ƒã—ã¦ãŠã‚Šã€**è¦³æ¸¬ä¸å¯èƒ½ãªè¦å› **ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
    elif fn_corr < 0.4:
        fn_interpretation = "ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã ã‘ãŒè¦‹é€ƒã—ã¦ãŠã‚Šã€**ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®å¤šæ§˜æ€§**ã§æ”¹å–„ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
    else:
        fn_interpretation = "ä¸­ç¨‹åº¦ã®ç›¸é–¢ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«é–“ã§éƒ¨åˆ†çš„ã«å…±é€šã—ãŸè¦‹é€ƒã—ã‚’ã—ã¦ã„ã¾ã™ã€‚"
    
    report = f"""# 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« èª¤å·®åˆ†æãƒ¬ãƒãƒ¼ãƒˆ (v3)

## æ¦‚è¦

ã“ã®åˆ†æã¯ã€LightGBM/CatBoost/MLP/TabNetã®4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ãŠã‘ã‚‹
äºˆæ¸¬èª¤ã‚Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã—ã€ä»Šå¾Œã®ãƒ¢ãƒ‡ãƒ«æ”¹å–„æ–¹é‡ã‚’æ±ºå®šã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

## åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿

- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: honhyo_for_analysis_with_traffic_hospital_no_leakage.csv
- **OOFäºˆæ¸¬æ•°**: {total:,} ä»¶
- **æœ€é©é–¾å€¤**: {threshold:.4f} (F1ã‚¹ã‚³ã‚¢: {best_f1:.4f})

## åˆ†é¡çµæœã‚µãƒãƒªãƒ¼

![æ··åŒè¡Œåˆ—](confusion_matrix.png)

| åˆ†é¡ | ä»¶æ•° | å…¨ä½“æ¯”ç‡ |
|------|------|----------|
| True Positive (æ­£ã—ãæ¤œå‡º) | {stats['TP']:,} | {stats['TP']/total*100:.2f}% |
| True Negative (æ­£ã—ãæ£„å´) | {stats['TN']:,} | {stats['TN']/total*100:.2f}% |
| **False Positive (èª¤æ¤œçŸ¥)** | **{stats['FP']:,}** | **{stats['FP']/total*100:.2f}%** |
| **False Negative (è¦‹é€ƒã—)** | **{stats['FN']:,}** | **{stats['FN']/total*100:.2f}%** |

- **Precision**: {precision:.4f}
- **Recall**: {recall:.4f}
- **F1 Score**: {best_f1:.4f}

## ï¿½ ãƒ¢ãƒ‡ãƒ«é–“ç›¸é–¢åˆ†æ

ã‚¨ãƒ©ãƒ¼äº‹ä¾‹ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å€¤ã®ç›¸é–¢ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ©ãƒ¼ã®åŸå› ã‚’ç‰¹å®šã§ãã¾ã™ã€‚

![ãƒ¢ãƒ‡ãƒ«ç›¸é–¢](model_correlation.png)

### False Positive (èª¤æ¤œçŸ¥) ã®ç›¸é–¢
- **å¹³å‡ç›¸é–¢ä¿‚æ•°**: {fp_corr:.3f}
- **è§£é‡ˆ**: {fp_interpretation}

### False Negative (è¦‹é€ƒã—) ã®ç›¸é–¢
- **å¹³å‡ç›¸é–¢ä¿‚æ•°**: {fn_corr:.3f}
- **è§£é‡ˆ**: {fn_interpretation}

## ğŸ“Š äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ

![äºˆæ¸¬åˆ†å¸ƒ](prediction_distribution.png)

## ï¿½ğŸ”´ é«˜FPãƒªã‚¹ã‚¯ç‰¹å¾´ (èª¤æ¤œçŸ¥ãŒå¤šç™ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³)

![ç‰¹å¾´é‡ã‚¨ãƒ©ãƒ¼ç‡](feature_error_rates.png)

| ç‰¹å¾´é‡ | ã‚«ãƒ†ã‚´ãƒª | ã‚µãƒ³ãƒ—ãƒ«æ•° | FPç‡ | å…¨ä½“æ¯” |
|--------|----------|------------|------|--------|
{fp_risk_table}

## ğŸŸ  é«˜FNãƒªã‚¹ã‚¯ç‰¹å¾´ (è¦‹é€ƒã—ãŒå¤šç™ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³)

| ç‰¹å¾´é‡ | ã‚«ãƒ†ã‚´ãƒª | ã‚µãƒ³ãƒ—ãƒ«æ•° | FNç‡ | å…¨ä½“æ¯” |
|--------|----------|------------|------|--------|
{fn_risk_table}

## ãƒ¢ãƒ‡ãƒ«é–“ã®äºˆæ¸¬ä¸ä¸€è‡´

FP/FNç™ºç”Ÿæ™‚ã«ã€å„ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ãªäºˆæ¸¬ã‚’ã—ã¦ã„ãŸã‹ã‚’åˆ†æã—ã¾ã—ãŸã€‚

![ãƒ¢ãƒ‡ãƒ«ä¸ä¸€è‡´](model_disagreement.png)

| ãƒ¢ãƒ‡ãƒ« | FPæ™‚ã®å¹³å‡äºˆæ¸¬ | FNæ™‚ã®å¹³å‡äºˆæ¸¬ |
|--------|----------------|----------------|
| LightGBM | {model_disagreement['FP'].get('LightGBM', 0):.4f} | {model_disagreement['FN'].get('LightGBM', 0):.4f} |
| CatBoost | {model_disagreement['FP'].get('CatBoost', 0):.4f} | {model_disagreement['FN'].get('CatBoost', 0):.4f} |
| MLP | {model_disagreement['FP'].get('MLP', 0):.4f} | {model_disagreement['FN'].get('MLP', 0):.4f} |
| TabNet | {model_disagreement['FP'].get('TabNet', 0):.4f} | {model_disagreement['FN'].get('TabNet', 0):.4f} |

## ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

- `confusion_matrix.png`: æ··åŒè¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
- `prediction_distribution.png`: FP/FN vs TP/TNã®äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ
- `model_correlation.png`: ã‚¨ãƒ©ãƒ¼äº‹ä¾‹ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«é–“ç›¸é–¢
- `model_disagreement.png`: ãƒ¢ãƒ‡ãƒ«é–“ã®äºˆæ¸¬å€¤æ¯”è¼ƒ
- `feature_error_rates.png`: ç‰¹å¾´é‡åˆ¥ã‚¨ãƒ©ãƒ¼ç‡ã‚°ãƒ©ãƒ•
- `false_positives_all.csv`: å…¨FPäº‹ä¾‹ï¼ˆç‰¹å¾´é‡ä»˜ãï¼‰
- `false_negatives_all.csv`: å…¨FNäº‹ä¾‹ï¼ˆç‰¹å¾´é‡ä»˜ãï¼‰
- `feature_error_analysis.csv`: ç‰¹å¾´é‡åˆ¥ã‚¨ãƒ©ãƒ¼ç‡ã®è©³ç´°

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— (æ¨å¥¨)

1. **é«˜ç›¸é–¢ã‚¨ãƒ©ãƒ¼ã¸ã®å¯¾å‡¦**: ãƒ¢ãƒ‡ãƒ«é–“ç›¸é–¢ãŒé«˜ã„ã‚¨ãƒ©ãƒ¼ã¯ã€æ–°ã—ã„ç‰¹å¾´é‡ã®è¿½åŠ ã‚„å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ã§å¯¾å‡¦
2. **ä½ç›¸é–¢ã‚¨ãƒ©ãƒ¼ã¸ã®å¯¾å‡¦**: ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã®å•é¡Œã¯ã€ãã®ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚„ç‰¹å¾´é‡é¸æŠã§å¯¾å‡¦
3. **é«˜ãƒªã‚¹ã‚¯ç‰¹å¾´ã¸ã® MoE**: è¦‹é€ƒã—ã‚„ã™ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç‰¹åŒ–ã—ãŸExpertãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
4. **é–¾å€¤èª¿æ•´**: ç”¨é€”ã«å¿œã˜ã¦Recall/Precisionã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´
"""
    
    with open(output_dir / "error_analysis_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"   ä¿å­˜: error_analysis_report.md")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 70)
    print(" ğŸ” 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« èª¤å·®åˆ†æ (v3: å¼·åŒ–ç‰ˆ)")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ç´ä»˜ã‘
    df = load_and_align_data()
    
    # æœ€é©é–¾å€¤ã®æ±ºå®š
    y_true = df['fatal'].values
    y_prob = df['pred_ensemble'].values
    
    threshold, best_f1 = find_optimal_threshold(y_true, y_prob)
    print(f"\nğŸ¯ æœ€é©é–¾å€¤ (F1æœ€å¤§åŒ–): {threshold:.4f} (F1: {best_f1:.4f})")
    
    # FP/FNæŠ½å‡º
    tp_mask, tn_mask, fp_mask, fn_mask, stats = extract_hard_examples(df, threshold)
    
    # å¯è¦–åŒ–ãƒ»åˆ†æ
    plot_confusion_matrix(stats, OUTPUT_DIR)
    analyze_prediction_distribution(df, tp_mask, tn_mask, fp_mask, fn_mask, OUTPUT_DIR)
    correlation_results = analyze_model_correlation(df, tp_mask, tn_mask, fp_mask, fn_mask, OUTPUT_DIR)
    model_disagreement = analyze_model_disagreement(df, fp_mask, fn_mask, OUTPUT_DIR)
    feature_results, high_fp_risk, high_fn_risk = analyze_feature_distribution(
        df, tp_mask, tn_mask, fp_mask, fn_mask, OUTPUT_DIR
    )
    plot_feature_error_rates(feature_results, OUTPUT_DIR)
    
    # çµæœä¿å­˜
    save_hard_examples_with_features(df, fp_mask, fn_mask, OUTPUT_DIR)
    generate_report(df, stats, threshold, best_f1, high_fp_risk, high_fn_risk, 
                    model_disagreement, correlation_results, OUTPUT_DIR)
    
    # çµæœã‚µãƒãƒªãƒ¼ä¿å­˜ (numpyå‹ã‚’Pythonå‹ã«å¤‰æ›)
    def convert_to_native(obj):
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_to_native(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_native(item) for item in obj]
        return obj
    
    summary = {
        'threshold': float(threshold),
        'best_f1': float(best_f1),
        'stats': convert_to_native(stats),
        'correlation_results': convert_to_native({k: v for k, v in correlation_results.items()}),
        'high_fp_risk_count': int(len(high_fp_risk)),
        'high_fn_risk_count': int(len(high_fn_risk)),
    }
    with open(OUTPUT_DIR / "analysis_summary.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 70)
    print(" âœ… åˆ†æå®Œäº†ï¼")
    print(f"    å‡ºåŠ›å…ˆ: {OUTPUT_DIR}")
    print("=" * 70)


if __name__ == "__main__":
    main()

"""
Advanced Error Analysis Script (SHAP & Geospatial)
==================================================
ç›®çš„:
1. SHAPå€¤ã‚’ç”¨ã„ã¦ã€èª¤æ¤œçŸ¥ï¼ˆFPï¼‰ã‚„è¦‹é€ƒã—ï¼ˆFNï¼‰ã®è¦å› ã‚’ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã‹ã‚‰è§£æ˜ã™ã‚‹ã€‚
2. é«˜ãƒªã‚¹ã‚¯å¸‚åŒºç”ºæ‘ï¼ˆ483ç­‰ï¼‰ã®å…·ä½“çš„ãªåœ°ç†çš„ä½ç½®ã‚’ç‰¹å®šã™ã‚‹ã€‚
3. å¹´é½¢å±¤Ã—äº‹æ•…é¡å‹ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’è¡Œã„ã€ãƒ‡ãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãªå¼±ç‚¹ã‚’ç‰¹å®šã™ã‚‹ã€‚

ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:
- LightGBM (Stage 1 or Stage 2 model) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ç”¨
- â€»ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å…¨ä½“ã®SHAPã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚ã€ä»£è¡¨ã¨ã—ã¦LightGBMã‚’ä½¿ç”¨

ä½¿ç”¨æ–¹æ³•:
    python scripts/experiment/analyze_error_shap_geo.py
"""

import os
import json
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import lightgbm as lgb
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve

import warnings
warnings.filterwarnings('ignore')

# --- è¨­å®š ---
DATA_PATH = Path("data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv")
STAGE1_OOF_PATH = Path("data/processed/stage1_oof_predictions.csv")
ENSEMBLE_OOF_PATH = Path("results/tabnet_optimized/oof_predictions.csv")
MODEL_PATH = Path("results/tabnet_optimized/lgbm_model_fold0.pkl") # LightGBMãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ (ä»®)
OUTPUT_DIR = Path("results/error_analysis_advanced")
os.makedirs(OUTPUT_DIR, exist_ok=True)

RANDOM_STATE = 42
TEST_SIZE = 0.2
STAGE1_RECALL_TARGET = 0.98

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'MS Gothic'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


def load_and_align_data():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ç´ä»˜ã‘ (v3å†åˆ©ç”¨)"""
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ç´ä»˜ã‘ä¸­...")
    df_full = pd.read_csv(DATA_PATH)
    df_full['fatal'] = df_full['fatal'].astype(int)
    
    stage1_oof = pd.read_csv(STAGE1_OOF_PATH)
    ensemble_oof = pd.read_csv(ENSEMBLE_OOF_PATH)
    
    all_indices = np.arange(len(df_full))
    train_indices, _ = train_test_split(
        all_indices, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_full['fatal']
    )
    
    y_train = df_full.iloc[train_indices]['fatal'].values
    stage1_prob = 0.85 * stage1_oof['prob_catboost'].values + 0.15 * stage1_oof['prob_lgbm'].values
    
    precision, recall, thresholds = precision_recall_curve(y_train, stage1_prob)
    valid_idx = np.where(recall[:-1] >= STAGE1_RECALL_TARGET)[0]
    stage1_threshold = thresholds[valid_idx[-1]] if len(valid_idx) > 0 else 0.0
    
    filter_mask = stage1_prob >= stage1_threshold
    filtered_train_indices = train_indices[filter_mask]
    
    df_aligned = df_full.iloc[filtered_train_indices].reset_index(drop=True).copy()
    for col in ensemble_oof.columns:
        df_aligned[f'pred_{col}'] = ensemble_oof[col].values
        
    return df_aligned, stage1_threshold


def identify_city_location(df, city_codes, output_dir):
    """ç‰¹å®šå¸‚åŒºç”ºæ‘ã®ç·¯åº¦çµŒåº¦å¹³å‡ã‚’ç®—å‡ºã—ã€å ´æ‰€ã‚’ç‰¹å®šã™ã‚‹"""
    print("\nğŸ“ å¸‚åŒºç”ºæ‘ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç‰¹å®š...")
    
    results = []
    
    for code in city_codes:
        mask = df['å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰'] == code
        if mask.sum() == 0:
            continue
            
        lat_mean = df.loc[mask, 'åœ°ç‚¹ã€€ç·¯åº¦ï¼ˆåŒ—ç·¯ï¼‰'].mean()
        lon_mean = df.loc[mask, 'åœ°ç‚¹ã€€çµŒåº¦ï¼ˆæ±çµŒï¼‰'].mean()
        count = mask.sum()
        
        # åº¦åˆ†ç§’è¡¨è¨˜ã®å¯èƒ½æ€§ã‚’è€ƒæ…® (æ•°å€¤ãŒç•°å¸¸ã«å¤§ãã„å ´åˆ)
        # ç·¯åº¦ãŒ 100 ä»¥ä¸Šã®å ´åˆã¯åº¦åˆ†ç§’ã®å¯èƒ½æ€§ãŒé«˜ã„ (æ—¥æœ¬ã¯ç·¯åº¦20-46)
        # ã—ã‹ã—å‰å›ã®ãƒ—ãƒ­ãƒƒãƒˆã§æ—¥æœ¬åœ°å›³ã«ãªã£ã¦ã„ãŸãªã‚‰å¤‰æ›æ¸ˆã¿ã¨æ¨æ¸¬
        # ã“ã“ã§ã¯ãã®ã¾ã¾å‡ºåŠ›
        
        # Google Maps URL
        gmap_url = f"https://www.google.com/maps/search/?api=1&query={lat_mean},{lon_mean}"
        
        print(f"   City {code}: N={count}, Lat={lat_mean:.4f}, Lon={lon_mean:.4f}")
        print(f"   -> {gmap_url}")
        
        results.append({
            'city_code': code,
            'count': count,
            'lat_mean': lat_mean,
            'lon_mean': lon_mean,
            'google_maps_url': gmap_url
        })
        
    df_res = pd.DataFrame(results)
    df_res.to_csv(output_dir / "city_locations.csv", index=False)
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã«ã‚‚å‡ºåŠ›
    with open(output_dir / "city_locations_report.txt", "w", encoding="utf-8") as f:
        for res in results:
            f.write(f"City Code: {res['city_code']}\n")
            f.write(f"Sample Count: {res['count']}\n")
            f.write(f"Centroid: {res['lat_mean']}, {res['lon_mean']}\n")
            f.write(f"Map URL: {res['google_maps_url']}\n")
            f.write("-" * 30 + "\n")


def analyze_demographic_heatmap(df, fp_mask, output_dir):
    """å¹´é½¢å±¤ Ã— äº‹æ•…é¡å‹ã®èª¤æ¤œçŸ¥ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
    print("\nğŸ‘¥ ãƒ‡ãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†æ (Age x Type)...")
    
    # å¹´é½¢å±¤ã‚’ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚° (å…ƒãƒ‡ãƒ¼ã‚¿ã¯ã‚«ãƒ†ã‚´ãƒªã‚³ãƒ¼ãƒ‰ã®å¯èƒ½æ€§)
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã«ã‚ˆã‚‹ã¨: 1:0-24, 25:25-34, ..., 75:75+
    # ã‚ã‹ã‚Šã‚„ã™ã„ãƒ©ãƒ™ãƒ«ã«å¤‰æ›
    age_map = {
        1: '0-24æ­³', 25: '25-34æ­³', 35: '35-44æ­³', 
        45: '45-54æ­³', 55: '55-64æ­³', 65: '65-74æ­³', 
        75: '75æ­³ä»¥ä¸Š', 0: 'ä¸æ˜'
    }
    
    # å¹´é½¢ã‚«ãƒ©ãƒ å
    col_age = 'å¹´é½¢ï¼ˆå½“äº‹è€…Aï¼‰'
    
    # äº‹æ•…é¡å‹ãƒãƒƒãƒ— (ä¸»è¦ãªã‚‚ã®)
    type_map = {
        1: 'äººå¯¾è»Šä¸¡', 21: 'è»Šä¸¡ç›¸äº’', 41: 'è»Šä¸¡å˜ç‹¬', 61: 'åˆ—è»Š'
    }
    
    # äº‹æ•…é¡å‹ãŒãªã„å ´åˆã€å½“äº‹è€…ç¨®åˆ¥ï¼ˆå½“äº‹è€…Aï¼‰ã‚’ä»£ç”¨
    target_col = 'äº‹æ•…é¡å‹'
    title_label = 'äº‹æ•…é¡å‹'
    
    if target_col not in df.columns:
        if 'å½“äº‹è€…ç¨®åˆ¥ï¼ˆå½“äº‹è€…Aï¼‰' in df.columns:
            target_col = 'å½“äº‹è€…ç¨®åˆ¥ï¼ˆå½“äº‹è€…Aï¼‰'
            title_label = 'å½“äº‹è€…ç¨®åˆ¥'
        else:
            print("   âš ï¸ åˆ†æã«å¿…è¦ãªã‚«ãƒ©ãƒ ï¼ˆäº‹æ•…é¡å‹ or å½“äº‹è€…ç¨®åˆ¥ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼
    df_viz = df[[col_age, target_col]].copy()
    
    # ãƒãƒƒãƒ”ãƒ³ã‚°
    df_viz['Age_Group'] = df_viz[col_age].map(age_map)
    
    if title_label == 'äº‹æ•…é¡å‹':
        df_viz['Type_Group'] = df_viz[target_col].map(type_map)
    else:
        # å½“äº‹è€…ç¨®åˆ¥ã®ç°¡æ˜“ãƒãƒƒãƒ”ãƒ³ã‚°
        # 1-5:ä¹—ç”¨è»Š, 11-14:è²¨ç‰©, 31-36:äºŒè¼ª, 51:è‡ªè»¢è»Š, 61:æ­©è¡Œè€…
        def map_party_type_simple(x):
            if 1 <= x <= 10: return 'ä¹—ç”¨è»Š'
            elif 11 <= x <= 20: return 'è²¨ç‰©è»Š'
            elif 31 <= x <= 40: return 'äºŒè¼ªè»Š'
            elif x == 51 or x == 52: return 'è‡ªè»¢è»Š'
            elif x == 61: return 'æ­©è¡Œè€…'
            else: return 'ãã®ä»–'
        
        df_viz['Type_Group'] = df_viz[target_col].apply(map_party_type_simple)

    # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ (FPç‡)
    # fp_mask ã¯ numpy array ãªã®ã§ã€df_vizã¨åŒã˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤Seriesã«å¤‰æ›ã—ã¦ã‹ã‚‰å‚ç…§ã™ã‚‹
    fp_mask_series = pd.Series(fp_mask, index=df.index)
    
    # ãƒ€ãƒŸãƒ¼åˆ—ã‚’è¿½åŠ 
    df_viz['Dummy'] = 1
    
    pivot_fp = df_viz.pivot_table(
        index='Type_Group', columns='Age_Group', 
        values='Dummy', # é‡è¤‡ã—ãªã„ã‚«ãƒ©ãƒ ã‚’æŒ‡å®š
        aggfunc=lambda x: (fp_mask_series.loc[x.index].sum() / len(x)) 
                          if len(x) > 50 else np.nan 
    )
    
    # åˆ—é †åºã‚’æ•´ãˆã‚‹
    age_order = ['0-24æ­³', '25-34æ­³', '35-44æ­³', '45-54æ­³', '55-64æ­³', '65-74æ­³', '75æ­³ä»¥ä¸Š']
    age_order = [c for c in age_order if c in pivot_fp.columns]
    pivot_fp = pivot_fp[age_order]
    
    plt.figure(figsize=(10, 6))
    sns.heatmap(pivot_fp, annot=True, fmt='.1%', cmap='Reds')
    plt.title(f'èª¤æ¤œçŸ¥ç‡ (FP Rate) Heatmap: å¹´é½¢ Ã— {title_label}')
    plt.tight_layout()
    plt.savefig(output_dir / "age_type_fp_heatmap.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: age_type_fp_heatmap.png")


def analyze_shap_feature_importance(df, fp_mask, output_dir):
    """SHAPå€¤ã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ (FPè¦å› )"""
    print("\nğŸŒŸ SHAPå€¤åˆ†æ (LightGBM)...")
    
    # ä¿å­˜æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™ï¼ˆresults/tabnet_optimized, results/lgbm_optuna ãªã©ï¼‰
    model_paths = list(Path("results").glob("**/lgbm*.pkl")) + list(Path("results").glob("**/model*.pkl"))
    lgbm_path = None
    
    # æœ€ã‚‚æ–°ã—ã„pklã‚’æ¢ã™ãªã©ã®ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã§æ¨æ¸¬
    # scripts/experiment/train_tabnet_optimized.py ã¯TabNetç”¨ãªã®ã§ã€
    # ä»¥å‰ã®ä¼šè©±ã§ä½œã£ãŸLightGBMãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ã¯ãšã€‚
    # ãªã‘ã‚Œã°ã€ç°¡æ˜“çš„ã«ã“ã“ã§å­¦ç¿’ã•ã›ã‚‹æ–¹ãŒç¢ºå®Ÿã§æ—©ã„ã€‚
    
    print("   LightGBMãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¦SHAPã‚’è¨ˆç®—ã—ã¾ã™ (ç´ä»˜ã‘ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’ç›´æ¥åæ˜ ã™ã‚‹ãŸã‚)...")
    
    # ç‰¹å¾´é‡é¸å®š (æ•°å€¤ã®ã¿ã€ã‚«ãƒ†ã‚´ãƒªã¯categorical_featureã¨ã—ã¦æ‰±ã†)
    # åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ å®šç¾©ã«åŸºã¥ã„ã¦é¸å®š
    excluded_cols = ['fatal', 'pred_lgbm', 'pred_catboost', 'pred_mlp', 
                     'pred_tabnet_optimized', 'pred_ensemble', 'target']
    feature_cols = [c for c in df.columns if c not in excluded_cols]
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®æŒ‡å®š
    cat_cols_candidates = ['éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹',
                'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ', 'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'æ­©è»Šé“åŒºåˆ†', 
                'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'road_type', 'å¹´é½¢ï¼ˆå½“äº‹è€…Aï¼‰', 'å½“äº‹è€…ç¨®åˆ¥ï¼ˆå½“äº‹è€…Aï¼‰']
    cat_cols = [c for c in cat_cols_candidates if c in feature_cols]
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æº–å‚™ (å…ƒã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½¿ç”¨)
    X = df[feature_cols].copy()
    y = df['fatal']
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’categoryå‹ã«å¤‰æ›
    for c in cat_cols:
        X[c] = X[c].astype('category')
        
    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    lgb_train = lgb.Dataset(X, y)
    
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'seed': 42
    }
    
    # ç°¡æ˜“å­¦ç¿’ (SHAPç”¨)
    model = lgb.train(params, lgb_train, num_boost_round=100)
    
    # SHAPè¨ˆç®— (FPãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
    # èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã¯å…¨ä½“ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    # ã‚«ãƒ†ã‚´ãƒªå‹ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã„ã‚ˆã†æ³¨æ„
    X_sample = X.sample(1000, random_state=42)
    explainer = shap.TreeExplainer(model)
    
    # FPãƒ‡ãƒ¼ã‚¿ (èª¤æ¤œçŸ¥) ã®SHAPå€¤
    fp_indices = np.where(fp_mask)[0]
    if len(fp_indices) > 500:
        fp_indices = np.random.choice(fp_indices, 500, replace=False)
        
    X_fp = X.iloc[fp_indices]
    shap_values_fp = explainer.shap_values(X_fp)
    
    # SHAPå€¤ãŒãƒªã‚¹ãƒˆï¼ˆã‚¯ãƒ©ã‚¹åˆ¥ï¼‰ã®å ´åˆã€ã‚¯ãƒ©ã‚¹1ï¼ˆæ­»äº¡ï¼‰ã®SHAPå€¤ã‚’å–å¾—
    if isinstance(shap_values_fp, list):
        shap_values_fp = shap_values_fp[1]
    
    # Summary Plotç”¨ã«ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªå¯¾å¿œãƒ•ã‚©ãƒ³ãƒˆã§è¡¨ç¤ºã•ã›ã‚‹ãŸã‚ã®å·¥å¤«
    # shapã¯matplotlibã‚’ä½¿ã†ã®ã§è¨­å®šæ¸ˆã¿ãƒ•ã‚©ãƒ³ãƒˆãŒåŠ¹ãã¯ãš
    
    # 1. Global Importance (FPè¦å› )
    plt.figure(figsize=(10, 15)) # ç¸¦é•·ã«
    shap.summary_plot(shap_values_fp, X_fp, show=False, plot_type="dot", max_display=20)
    plt.title("èª¤æ¤œçŸ¥(FP)ãƒ‡ãƒ¼ã‚¿ã®SHAPå€¤ (é«˜ã„ã»ã©èª¤æ¤œçŸ¥è¦å› )")
    plt.tight_layout()
    plt.savefig(output_dir / "shap_summary_fp_dot.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: shap_summary_fp_dot.png")
    
    plt.figure(figsize=(10, 15))
    shap.summary_plot(shap_values_fp, X_fp, show=False, plot_type="bar", max_display=20)
    plt.title("èª¤æ¤œçŸ¥(FP)ã¸ã®å½±éŸ¿åº¦ (çµ¶å¯¾å€¤å¹³å‡)")
    plt.tight_layout()
    plt.savefig(output_dir / "shap_summary_fp_bar.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: shap_summary_fp_bar.png")
    
    # 2. Local Importance (è¸åˆ‡äº‹æ•…ã®èª¤æ¤œçŸ¥ãªã©)
    # 'é“è·¯å½¢çŠ¶'=21 (è¸åˆ‡) ã®ã‚±ãƒ¼ã‚¹ã‚’æ¢ã™
    if 'é“è·¯å½¢çŠ¶' in X_fp.columns:
        railroad_mask = (X_fp['é“è·¯å½¢çŠ¶'] == 21)
        if railroad_mask.sum() > 0:
            # æœ€åˆã®1ä»¶ã‚’å–å¾—
            idx = np.where(railroad_mask)[0][0] # ã“ã‚Œã¯X_fpå†…ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            print(f"\n   ğŸš‚ è¸åˆ‡FPäº‹ä¾‹ã®SHAPåˆ†æ... (Index in sample: {idx})")
            
            # shap.plots.waterfall ã¯ Explanation ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¿…è¦ã¨ã™ã‚‹
            # shap_values_fp[idx] ã¯ array
            
            # æœŸå¾…å€¤ (base_value) ã®å–å¾—
            base_value = explainer.expected_value
            if isinstance(base_value, list):
                base_value = base_value[1]
            
            plt.figure(figsize=(10, 8))
            shap.plots.waterfall(
                shap.Explanation(values=shap_values_fp[idx], 
                                 base_values=base_value, 
                                 data=X_fp.iloc[idx], 
                                 feature_names=X_fp.columns),
                show=False, max_display=10
            )
            plt.title(f"è¸åˆ‡FPäº‹ä¾‹ã®è¦å› åˆ†è§£")
            plt.tight_layout()
            plt.savefig(output_dir / "shap_local_railroad.png", dpi=150)
            plt.close()
            print(f"   ä¿å­˜: shap_local_railroad.png")


def main():
    print("=" * 70)
    print(" ğŸ”¬ Advanced Error Analysis (SHAP & Geo)")
    print("=" * 70)
    
    df, threshold = load_and_align_data()
    
    y_true = df['fatal'].values
    y_prob = df['pred_ensemble'].values
    y_pred = (y_prob >= threshold).astype(int)
    
    # numpy array mask
    fp_mask = (y_true == 0) & (y_pred == 1)
    
    # ç·¯åº¦çµŒåº¦ã®ç°¡æ˜“è£œæ­£ (1/10,000,000)
    # å€¤ãŒ1å„„ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã«é©ç”¨
    for col in ['åœ°ç‚¹ã€€ç·¯åº¦ï¼ˆåŒ—ç·¯ï¼‰', 'åœ°ç‚¹ã€€çµŒåº¦ï¼ˆæ±çµŒï¼‰']:
        if col in df.columns and df[col].mean() > 1000:
            print(f"   âš ï¸ {col} ã®å€¤ãŒå¤§ãã„ãŸã‚ã€1/10,000,000 ã—ã¦è£œæ­£ã—ã¾ã™ã€‚")
            df[col] = df[col] / 10000000.0
    
    # 1. å¸‚åŒºç”ºæ‘ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç‰¹å®š (483, 585, 586ãªã©)
    target_cities = [483, 585, 586, 434, 492, 311] # ãƒ¬ãƒãƒ¼ãƒˆã®ä¸Šä½
    identify_city_location(df, target_cities, OUTPUT_DIR)
    
    # 2. ãƒ‡ãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†æ
    # ã“ã“ã§FPç‡ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€heatmapé–¢æ•°å†…ã§reindexã‚¨ãƒ©ãƒ¼ãŒèµ·ããªã„ã‚ˆã†ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£æ¸ˆã¿ã®ã¯ãšã ãŒã€
    # å¿µã®ãŸã‚é–¢æ•°ãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ã€‚
    analyze_demographic_heatmap(df, fp_mask, OUTPUT_DIR)
    
    # 3. SHAPåˆ†æ
    analyze_shap_feature_importance(df, fp_mask, OUTPUT_DIR)
    
    print("\nâœ… é«˜åº¦åˆ†æå®Œäº†")
    print(f"   å‡ºåŠ›å…ˆ: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()

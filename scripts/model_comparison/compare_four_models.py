"""
4ãƒ¢ãƒ‡ãƒ«çµ±åˆæ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° vs Random Forest vs LightGBM vs XGBoost

4ã¤ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¬å¹³ã«æ¯”è¼ƒ:
- ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰
- Random Forestï¼ˆãƒã‚®ãƒ³ã‚°æ‰‹æ³•ï¼‰
- LightGBMï¼ˆãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»Leaf-wiseï¼‰
- XGBoostï¼ˆãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»Depth-wiseï¼‰

è©•ä¾¡å†…å®¹:
- PR-AUC, ROC-AUC, F1, Accuracy, Precision, Recall
- çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®šï¼ˆFriedmanæ¤œå®šï¼‰
- è¨“ç·´æ™‚é–“ãƒ»äºˆæ¸¬æ™‚é–“ã®æ¯”è¼ƒ
- è©³ç´°ãª4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    average_precision_score,
    roc_auc_score,
    f1_score,
    accuracy_score,
    precision_score,
    recall_score
)
from scipy import stats
import time
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# psutilã¯ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼ˆãƒ¡ãƒ¢ãƒªç›£è¦–ç”¨ï¼‰
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


class FourModelComparator:
    """4ãƒ¢ãƒ‡ãƒ«ã®çµ±åˆæ¯”è¼ƒ"""
    
    def __init__(
        self,
        data_path='data/processed/honhyo_clean_predictable_only.csv',
        target_column='æ­»è€…æ•°',
        n_folds=5,
        random_state=42
    ):
        self.data_path = data_path
        self.target_column = target_column
        self.n_folds = n_folds
        self.random_state = random_state
        
        # LightGBMã®æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆTrial 153ï¼‰
        self.lightgbm_params = {
            'learning_rate': 0.07658346283890378,
            'num_leaves': 125,
            'max_depth': 8,
            'min_child_samples': 278,
            'subsample': 0.6147706754536576,
            'colsample_bytree': 0.6267708320804088,
            'reg_alpha': 0.9961403311275829,
            'reg_lambda': 8.228908331551605,
            'min_child_weight': 0.12646850234127796,
            'min_split_gain': 0.24303906753172422,
            'path_smooth': 2.254892007170922,
            'scale_pos_weight': 61.47728365878301,
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 10000,
            'random_state': random_state,
            'n_jobs': -1,
            'verbose': -1
        }
        
        # Random Forestã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.rf_params = {
            'n_estimators': 500,
            'max_depth': 15,
            'min_samples_split': 20,
            'min_samples_leaf': 10,
            'max_features': 'sqrt',
            'class_weight': 'balanced',
            'random_state': random_state,
            'n_jobs': -1,
            'verbose': 0
        }
        
        # XGBoostã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆLightGBMã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
        self.xgboost_params = {
            'learning_rate': 0.08,
            'n_estimators': 1000,
            'max_depth': 8,
            'min_child_weight': 5,
            'subsample': 0.6,
            'colsample_bytree': 0.6,
            'reg_alpha': 1.0,
            'reg_lambda': 8.0,
            'scale_pos_weight': 61.48,
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'random_state': random_state,
            'n_jobs': -1,
            'verbosity': 0
        }
        
        print("="*80)
        print("4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° vs RF vs LightGBM vs XGBoost")
        print("="*80)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        print(f"\n[ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿] {data_path}")
        start_load = time.time()
        self.df = pd.read_csv(data_path)
        load_time = time.time() - start_load
        print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df):,} ä»¶ ({load_time:.2f}ç§’)")
        if PSUTIL_AVAILABLE:
            print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # å‰å‡¦ç†
        self._preprocess_data()
        
    def _preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        print("\n[å‰å‡¦ç†] ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        # ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
        self.y = self.df[self.target_column]
        self.X = self.df.drop(columns=[self.target_column])
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’è¡¨ç¤º
        positive_count = (self.y == 1).sum()
        negative_count = (self.y == 0).sum()
        positive_ratio = positive_count / len(self.y) * 100
        print(f"\n[ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ]")
        print(f"  é™°æ€§ã‚¯ãƒ©ã‚¹ (æ­»è€…ãªã—): {negative_count:,} ä»¶ ({100-positive_ratio:.2f}%)")
        print(f"  é™½æ€§ã‚¯ãƒ©ã‚¹ (æ­»è€…ã‚ã‚Š): {positive_count:,} ä»¶ ({positive_ratio:.2f}%)")
        print(f"  ä¸å‡è¡¡æ¯”ç‡: 1:{negative_count/positive_count:.1f}")
        
        # ç™ºç”Ÿæ—¥æ™‚ã‚’é™¤å¤–
        if 'ç™ºç”Ÿæ—¥æ™‚' in self.X.columns:
            self.X = self.X.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        # ç‰¹å¾´é‡ã®åˆ†é¡
        self.numeric_cols = self.X.select_dtypes(include=['int64', 'float64']).columns.tolist()
        self.categorical_cols = self.X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        explicit_cat_cols = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'è·¯ç·šã‚³ãƒ¼ãƒ‰', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰',
            'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
            'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
            'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)'
        ]
        
        explicit_cat_cols = [c for c in explicit_cat_cols if c in self.X.columns]
        self.categorical_cols = list(set(self.categorical_cols + explicit_cat_cols))
        self.numeric_cols = [c for c in self.numeric_cols if c not in self.categorical_cols]
        
        print(f"\n[ç‰¹å¾´é‡æƒ…å ±]")
        print(f"  ç·ç‰¹å¾´é‡æ•°: {len(self.X.columns)} å€‹")
        print(f"  - æ•°å€¤å‹: {len(self.numeric_cols)} å€‹")
        print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹: {len(self.categorical_cols)} å€‹")
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ–‡å­—åˆ—ã«çµ±ä¸€
        for col in self.categorical_cols:
            if col in self.X.columns:
                self.X[col] = self.X[col].astype(str)
        
    def _build_logreg_pipeline(self):
        """ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, self.numeric_cols),
                ('cat', categorical_transformer, self.categorical_cols)
            ],
            remainder='drop'
        )
        
        return Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('classifier', LogisticRegression(
                penalty='l2', C=1.0, solver='saga', max_iter=1000,
                class_weight='balanced', random_state=self.random_state,
                n_jobs=-1, verbose=0
            ))
        ])
    
    def _build_rf_pipeline(self):
        """Random Forestã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median'))
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, self.numeric_cols),
                ('cat', categorical_transformer, self.categorical_cols)
            ],
            remainder='drop'
        )
        
        return Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('classifier', RandomForestClassifier(**self.rf_params))
        ])
    
    def _build_lightgbm_model(self):
        """LightGBMãƒ¢ãƒ‡ãƒ«"""
        return lgb.LGBMClassifier(**self.lightgbm_params)
    
    def _build_xgboost_model(self):
        """XGBoostãƒ¢ãƒ‡ãƒ«"""
        return xgb.XGBClassifier(**self.xgboost_params)
    
    def _prepare_data_for_tree_models(self, X):
        """Treeç³»ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæ¬ æå€¤è£œå®Œ+ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰"""
        from sklearn.preprocessing import LabelEncoder
        
        X_prepared = X.copy()
        
        # æ•°å€¤å‹ã®æ¬ æå€¤è£œå®Œ
        for col in self.numeric_cols:
            if col in X_prepared.columns and X_prepared[col].isna().any():
                X_prepared[col].fillna(X_prepared[col].median(), inplace=True)
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹ã®æ¬ æå€¤è£œå®Œã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        for col in self.categorical_cols:
            if col in X_prepared.columns:
                # æ¬ æå€¤è£œå®Œ
                if X_prepared[col].isna().any():
                    mode_result = X_prepared[col].mode()
                    if len(mode_result) > 0:
                        X_prepared[col].fillna(mode_result[0], inplace=True)
                    else:
                        # ã™ã¹ã¦æ¬ æå€¤ã®å ´åˆã¯'unknown'ã§è£œå®Œ
                        X_prepared[col].fillna('unknown', inplace=True)
                
                # LabelEncoderã§æ•°å€¤å¤‰æ›
                le = LabelEncoder()
                X_prepared[col] = le.fit_transform(X_prepared[col].astype(str))
        
        return X_prepared
    
    def compare_with_cv(self):
        """äº¤å·®æ¤œè¨¼ã§4ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ"""
        print(f"\n[é–‹å§‹] {self.n_folds}-fold äº¤å·®æ¤œè¨¼ã§4ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ")
        print("="*80)
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if PSUTIL_AVAILABLE:
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {psutil.virtual_memory().percent:.1f}% ä½¿ç”¨ä¸­")
        cv_start_time = time.time()
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        results = {
            'logreg': [],
            'rf': [],
            'lightgbm': [],
            'xgboost': []
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            fold_start_time = time.time()
            print(f"\n{'='*80}")
            print(f"Fold {fold+1}/{self.n_folds}")
            print(f"{'='*80}")
            print(f"æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
            
            X_train, X_val = self.X.iloc[train_idx].copy(), self.X.iloc[val_idx].copy()
            y_train, y_val = self.y.iloc[train_idx], self.y.iloc[val_idx]
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æƒ…å ±ã‚’è¡¨ç¤º
            print(f"\n[ãƒ‡ãƒ¼ã‚¿åˆ†å‰²]")
            print(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train):,} ä»¶ (é™½æ€§: {(y_train==1).sum():,}, é™°æ€§: {(y_train==0).sum():,})")
            print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,} ä»¶ (é™½æ€§: {(y_val==1).sum():,}, é™°æ€§: {(y_val==0).sum():,})")
            
            # ===== 1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° =====
            print(f"\n[1/4] ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°")
            print(f"  é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
            logreg = self._build_logreg_pipeline()
            start = time.time()
            print(f"  å­¦ç¿’ä¸­...")
            logreg.fit(X_train, y_train)
            train_time = time.time() - start
            
            start = time.time()
            prob = logreg.predict_proba(X_val)[:, 1]
            pred_time = time.time() - start
            pred = (prob >= 0.5).astype(int)
            
            results['logreg'].append(self._calculate_metrics(y_val, pred, prob, train_time, pred_time, fold+1))
            print(f"  âœ… å®Œäº† - PR-AUC: {results['logreg'][-1]['pr_auc']:.4f} | å­¦ç¿’: {train_time:.1f}ç§’ | äºˆæ¸¬: {pred_time:.3f}ç§’")
            print(f"     ãã®ä»–æŒ‡æ¨™ - ROC-AUC: {results['logreg'][-1]['roc_auc']:.4f}, F1: {results['logreg'][-1]['f1']:.4f}")
            
            # ===== 2. Random Forest =====
            print(f"\n[2/4] Random Forest")
            print(f"  é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
            rf = self._build_rf_pipeline()
            start = time.time()
            print(f"  å­¦ç¿’ä¸­ (n_estimators={self.rf_params['n_estimators']})...")
            rf.fit(X_train, y_train)
            train_time = time.time() - start
            
            start = time.time()
            prob = rf.predict_proba(X_val)[:, 1]
            pred_time = time.time() - start
            pred = (prob >= 0.5).astype(int)
            
            results['rf'].append(self._calculate_metrics(y_val, pred, prob, train_time, pred_time, fold+1))
            print(f"  âœ… å®Œäº† - PR-AUC: {results['rf'][-1]['pr_auc']:.4f} | å­¦ç¿’: {train_time:.1f}ç§’ | äºˆæ¸¬: {pred_time:.3f}ç§’")
            print(f"     ãã®ä»–æŒ‡æ¨™ - ROC-AUC: {results['rf'][-1]['roc_auc']:.4f}, F1: {results['rf'][-1]['f1']:.4f}")
            
            # Treeç³»ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            print(f"\n[ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†] Treeç³»ãƒ¢ãƒ‡ãƒ«ç”¨ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­...")
            prep_start = time.time()
            X_train_tree = self._prepare_data_for_tree_models(X_train)
            X_val_tree = self._prepare_data_for_tree_models(X_val)
            prep_time = time.time() - prep_start
            print(f"  âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº† ({prep_time:.2f}ç§’)")
            
            # ===== 3. LightGBM =====
            print(f"\n[3/4] LightGBM")
            print(f"  é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
            lgbm = self._build_lightgbm_model()
            start = time.time()
            print(f"  å­¦ç¿’ä¸­ (max_iter={self.lightgbm_params['n_estimators']}, early_stop=50)...")
            lgbm.fit(X_train_tree, y_train, eval_set=[(X_val_tree, y_val)],
                    callbacks=[lgb.early_stopping(50, verbose=False)])
            train_time = time.time() - start
            
            start = time.time()
            prob = lgbm.predict_proba(X_val_tree)[:, 1]
            pred_time = time.time() - start
            pred = (prob >= 0.5).astype(int)
            
            results['lightgbm'].append(self._calculate_metrics(y_val, pred, prob, train_time, pred_time, fold+1))
            print(f"  âœ… å®Œäº† - PR-AUC: {results['lightgbm'][-1]['pr_auc']:.4f} | å­¦ç¿’: {train_time:.1f}ç§’ | äºˆæ¸¬: {pred_time:.3f}ç§’")
            print(f"     ãã®ä»–æŒ‡æ¨™ - ROC-AUC: {results['lightgbm'][-1]['roc_auc']:.4f}, F1: {results['lightgbm'][-1]['f1']:.4f}")
            print(f"     Best iteration: {lgbm.best_iteration_}")
            
            # ===== 4. XGBoost =====
            print(f"\n[4/4] XGBoost")
            print(f"  é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
            xgb_model = self._build_xgboost_model()
            start = time.time()
            print(f"  å­¦ç¿’ä¸­ (max_iter={self.xgboost_params['n_estimators']})...")
            xgb_model.fit(X_train_tree, y_train, eval_set=[(X_val_tree, y_val)],
                         verbose=False)
            train_time = time.time() - start
            
            start = time.time()
            prob = xgb_model.predict_proba(X_val_tree)[:, 1]
            pred_time = time.time() - start
            pred = (prob >= 0.5).astype(int)
            
            results['xgboost'].append(self._calculate_metrics(y_val, pred, prob, train_time, pred_time, fold+1))
            print(f"  âœ… å®Œäº† - PR-AUC: {results['xgboost'][-1]['pr_auc']:.4f} | å­¦ç¿’: {train_time:.1f}ç§’ | äºˆæ¸¬: {pred_time:.3f}ç§’")
            print(f"     ãã®ä»–æŒ‡æ¨™ - ROC-AUC: {results['xgboost'][-1]['roc_auc']:.4f}, F1: {results['xgboost'][-1]['f1']:.4f}")
            print(f"     Best iteration: {xgb_model.best_iteration}")
            
            # Foldå®Œäº†æ™‚ã®æƒ…å ±
            fold_time = time.time() - fold_start_time
            elapsed_total = time.time() - cv_start_time
            print(f"\n  ğŸ“Š Fold {fold+1} å®Œäº†æ™‚é–“: {fold_time/60:.1f}åˆ†")
            print(f"  ğŸ“Š ç´¯è¨ˆçµŒéæ™‚é–“: {elapsed_total/60:.1f}åˆ† / æ¨å®šæ®‹ã‚Š: {elapsed_total/(fold+1)*(self.n_folds-fold-1)/60:.1f}åˆ†")
            if PSUTIL_AVAILABLE:
                print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {psutil.virtual_memory().percent:.1f}%")
        
        # DataFrameã«å¤‰æ›
        total_cv_time = time.time() - cv_start_time
        print(f"\n{'='*80}")
        print(f"âœ… å…¨{self.n_folds} Foldå®Œäº†!")
        print(f"ç·å®Ÿè¡Œæ™‚é–“: {total_cv_time/60:.1f}åˆ† ({total_cv_time/3600:.2f}æ™‚é–“)")
        print(f"{'='*80}")
        
        self.results_dfs = {name: pd.DataFrame(data) for name, data in results.items()}
        self.results_means = {name: df.mean() for name, df in self.results_dfs.items()}
        self.results_stds = {name: df.std() for name, df in self.results_dfs.items()}
        
        self._print_comparison_summary()
        
        return self.results_dfs
    
    def _calculate_metrics(self, y_true, y_pred, y_prob, train_time, pred_time, fold):
        """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
        return {
            'fold': fold,
            'pr_auc': average_precision_score(y_true, y_prob),
            'roc_auc': roc_auc_score(y_true, y_prob),
            'f1': f1_score(y_true, y_pred),
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred),
            'train_time': train_time,
            'pred_time': pred_time
        }
    
    def _print_comparison_summary(self):
        """æ¯”è¼ƒçµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "="*80)
        print("[çµæœ] 4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚µãƒãƒªãƒ¼")
        print("="*80)
        
        model_names = ['ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°', 'Random Forest', 'LightGBM', 'XGBoost']
        model_keys = ['logreg', 'rf', 'lightgbm', 'xgboost']
        
        metrics = ['pr_auc', 'roc_auc', 'f1']
        metric_names = ['PR-AUC', 'ROC-AUC', 'F1 Score']
        
        for metric, metric_name in zip(metrics, metric_names):
            print(f"\n{metric_name}:")
            values = []
            for key, name in zip(model_keys, model_names):
                val = self.results_means[key][metric]
                std = self.results_stds[key][metric]
                values.append(val)
                print(f"  {name:<20}: {val:.4f} Â± {std:.4f}")
            
            best_idx = np.argmax(values)
            print(f"  â†’ æœ€è‰¯: {model_names[best_idx]}")
    
    def statistical_test(self):
        """Friedmanæ¤œå®š"""
        print("\n" + "="*80)
        print("[çµ±è¨ˆçš„æ¤œå®š] Friedmanæ¤œå®š")
        print("="*80)
        
        metrics = ['pr_auc', 'roc_auc', 'f1']
        test_results = []
        
        for metric in metrics:
            scores = [self.results_dfs[key][metric].values for key in ['logreg', 'rf', 'lightgbm', 'xgboost']]
            stat, p_val = stats.friedmanchisquare(*scores)
            
            test_results.append({
                'metric': metric,
                'statistic': stat,
                'p_value': p_val,
                'significant': p_val < 0.05
            })
            
            print(f"{metric.upper()}: çµ±è¨ˆé‡={stat:.4f}, på€¤={p_val:.4f}, æœ‰æ„å·®={'âœ…' if p_val < 0.05 else 'âŒ'}")
        
        return pd.DataFrame(test_results)
    
    def save_results(self, output_dir='results/model_comparison'):
        """çµæœä¿å­˜"""
        os.makedirs(output_dir, exist_ok=True)
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print(f"\n[ä¿å­˜] çµæœã‚’ä¿å­˜ä¸­: {output_dir}")
        
        for name, df in self.results_dfs.items():
            path = f'{output_dir}/{name}_cv_4models_{ts}.csv'
            df.to_csv(path, index=False, encoding='utf-8-sig')
            print(f"  âœ… {name}: {path}")
        
        # çµ±è¨ˆçš„æ¤œå®šçµæœ
        test_df = self.statistical_test()
        test_path = f'{output_dir}/statistical_test_4models_{ts}.csv'
        test_df.to_csv(test_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… çµ±è¨ˆçš„æ¤œå®š: {test_path}")
        
        print(f"\nâœ… ã™ã¹ã¦ã®çµæœã‚’ä¿å­˜å®Œäº†!")


def main():
    comparator = FourModelComparator()
    comparator.compare_with_cv()
    comparator.save_results()
    
    print("\n" + "="*80)
    print("âœ… 4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("="*80)


if __name__ == '__main__':
    main()

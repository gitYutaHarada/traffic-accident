"""
ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚‹æ®µéšçš„ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“

æ®µéšçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚å®Ÿè¡Œå¯èƒ½ã«æ”¹è‰¯ã€‚
ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ã‚’æŒ‡å®šå¯èƒ½ã€‚

ä½¿ç”¨ä¾‹:
    # 1%ã‚µãƒ³ãƒ—ãƒ«
    python train_logistic_regression_staged.py --sample-rate 0.01
    
    # 10%ã‚µãƒ³ãƒ—ãƒ«
    python train_logistic_regression_staged.py --sample-rate 0.1
    
    # å…¨ãƒ‡ãƒ¼ã‚¿
    python train_logistic_regression_staged.py --sample-rate 1.0
"""

import pandas as pd
import numpy as np
import os
import warnings
import argparse
import time
from datetime import timedelta
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    precision_recall_curve,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl

warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
mpl.rcParams['font.family'] = 'MS Gothic'

def format_time(seconds):
    """ç§’æ•°ã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    return str(timedelta(seconds=int(seconds)))

def main():
    """
    æ®µéšçš„ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã‚ˆã‚‹æ­»äº¡äº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
    """
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
    parser = argparse.ArgumentParser(description='æ®µéšçš„ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°å®Ÿé¨“')
    parser.add_argument('--sample-rate', type=float, default=0.01,
                        help='ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ (0.01=1%%, 0.1=10%%, 1.0=å…¨ãƒ‡ãƒ¼ã‚¿)')
    args = parser.parse_args()
    
    sample_rate = args.sample_rate
    
    # å®Ÿè¡Œé–‹å§‹æ™‚åˆ»
    script_start_time = time.time()
    
    print("=" * 80)
    print(f"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: {sample_rate*100:.1f}%)")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿(LightGBMã¨åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)
    file_path = 'data/processed/honhyo_model_ready.csv'
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    
    try:
        df = pd.read_csv(file_path)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} ä»¶")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ç›®çš„å¤‰æ•°
    target_col = 'æ­»è€…æ•°'
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°(å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã‚¯ãƒ©ã‚¹æ¯”ç‡ã‚’ç¶­æŒ)
    if sample_rate < 1.0:
        print(f"\nğŸ² {sample_rate*100:.1f}% ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸­(å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)...")
        # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰åŒã˜å‰²åˆã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        df_0 = df[df[target_col] == 0].sample(frac=sample_rate, random_state=42)
        df_1 = df[df[target_col] == 1].sample(frac=sample_rate, random_state=42)
        df = pd.concat([df_0, df_1], ignore_index=True)
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)
        print(f"âœ“ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(df):,} ä»¶")
    
    # é™¤å¤–ã™ã‚‹åˆ—(LightGBMã¨åŒã˜äº‹å¾Œæƒ…å ±ã‚’é™¤å¤–)
    drop_cols = [
        'è³‡æ–™åŒºåˆ†', 'æœ¬ç¥¨ç•ªå·',
        'äººèº«æå‚·ç¨‹åº¦(å½“äº‹è€…A)', 'äººèº«æå‚·ç¨‹åº¦(å½“äº‹è€…B)',
        'è»Šä¸¡ã®æå£Šç¨‹åº¦(å½“äº‹è€…A)', 'è»Šä¸¡ã®æå£Šç¨‹åº¦(å½“äº‹è€…B)',
        'è² å‚·è€…æ•°',
        'è»Šä¸¡ã®è¡çªéƒ¨ä½(å½“äº‹è€…A)', 'è»Šä¸¡ã®è¡çªéƒ¨ä½(å½“äº‹è€…B)',
        'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™(å½“äº‹è€…A)', 'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™(å½“äº‹è€…B)',
        'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™(å½“äº‹è€…A)', 'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™(å½“äº‹è€…B)',
        'äº‹æ•…å†…å®¹'
    ]
    
    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–(å…¨è§’æ‹¬å¼§ã‚’åŠè§’ã«çµ±ä¸€)
    df.columns = df.columns.str.replace('(', '(').str.replace(')', ')')
    
    print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­(äº‹å¾Œæƒ…å ±ã®é™¤å¤–)...")
    df_clean = df.drop(columns=drop_cols, errors='ignore')
    
    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°
    X = df_clean.drop(columns=[target_col])
    y = df_clean[target_col]
    
    print(f"âœ“ å‰å‡¦ç†å®Œäº† - ç‰¹å¾´é‡æ•°: {X.shape[1]}")
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã¨æ•°å€¤å¤‰æ•°ã®åˆ†é¡
    # ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ—ã¯æ•°å€¤ã¨ã—ã¦æ‰±ã†
    count_encoding_cols = [col for col in X.columns if col.endswith('_count')]
    
    # æ•°å€¤å‹ã®åˆ—
    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹ã®åˆ—(æ–‡å­—åˆ—å‹ + categoryå‹)
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã¨ã—ã¦æ˜ç¤ºçš„ã«æ‰±ã†ã¹ãåˆ—(æ•°å€¤ã‚³ãƒ¼ãƒ‰ã ãŒã‚«ãƒ†ã‚´ãƒª)
    explicit_cat_cols = [
        'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'è·¯ç·šã‚³ãƒ¼ãƒ‰', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰',
        'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
        'ä¸€æ™‚åœæ­¢è¦åˆ¶ æ¨™è­˜', 'ä¸€æ™‚åœæ­¢è¦åˆ¶ è¡¨ç¤º', 'è»Šé“å¹…å“¡', 'é“è·¯ç·šå½¢',
        'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
        'äº‹æ•…é¡å‹', 'å¹´é½¢', 'å½“äº‹è€…ç¨®åˆ¥', 'ç”¨é€”åˆ¥', 'è»Šä¸¡å½¢çŠ¶',
        'ã‚ªãƒ¼ãƒˆãƒãƒãƒƒã‚¯è»Š', 'ã‚µãƒã‚«ãƒ¼', 'é€Ÿåº¦è¦åˆ¶(æŒ‡å®šã®ã¿)',
        'æ›œæ—¥', 'ç¥æ—¥', 'ç™ºç”Ÿæœˆ', 'ç™ºç”Ÿæ™‚', 'ç™ºç”Ÿå¹´', 'Area_Cluster_ID'
    ]
    
    # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’å¯¾è±¡
    explicit_cat_cols = [c for c in explicit_cat_cols if c in X.columns and c not in count_encoding_cols]
    
    # çµ±åˆã—ãŸã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ãƒªã‚¹ãƒˆ
    final_cat_cols = list(set(categorical_cols + explicit_cat_cols))
    
    # æ•°å€¤å¤‰æ•°ãƒªã‚¹ãƒˆ(ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã§ãªã„ã‚‚ã®)
    final_numeric_cols = [c for c in numeric_cols if c not in final_cat_cols]
    
    print(f"\nğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°: {len(final_cat_cols)} ã‚«ãƒ©ãƒ ")
    print(f"ğŸ”¢ æ•°å€¤å¤‰æ•°: {len(final_numeric_cols)} ã‚«ãƒ©ãƒ ")
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ–‡å­—åˆ—å‹ã«çµ±ä¸€(OneHotEncoderãŒå‹ã®æ··åœ¨ã‚’è¨±ã•ãªã„ãŸã‚)
    print("\nğŸ”„ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›ä¸­...")
    for col in final_cat_cols:
        if col in X.columns:
            X[col] = X[col].astype(str)
    
    # ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒé«˜ã™ãã‚‹å¤‰æ•°ã®å‡¦ç†(ä¸Šä½Nå€‹ä»¥å¤–ã‚’'ãã®ä»–'ã«ã¾ã¨ã‚ã‚‹)
    high_cardinality_threshold = 50  # 100 â†’ 50 ã«å‰Šæ¸›
    for col in final_cat_cols:
        if col in X.columns:
            nunique = X[col].nunique()
            if nunique > high_cardinality_threshold:
                print(f"  âš ï¸ '{col}' ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒé«˜ã„({nunique})ãŸã‚ã€ä¸Šä½{high_cardinality_threshold}å€‹ä»¥å¤–ã‚’'ãã®ä»–'ã«ã¾ã¨ã‚ã¾ã™")
                top_categories = X[col].value_counts().head(high_cardinality_threshold).index
                X[col] = X[col].apply(lambda x: x if x in top_categories else 'ãã®ä»–')
    
    # å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=30))  # 50 â†’ 30
    ])
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, final_numeric_cols),
            ('cat', categorical_transformer, final_cat_cols)
        ],
        remainder='drop'  # ãã®ä»–ã®åˆ—ã¯å‰Šé™¤
    )
    
    # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«
    # class_weight='balanced'ã§ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œ(LightGBMã®scale_pos_weightã«ç›¸å½“)
    model = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(
            penalty='l2',
            C=1.0,
            solver='saga',  # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸã‚½ãƒ«ãƒãƒ¼
            max_iter=500,  # 1000 â†’ 500 ã«å‰Šæ¸›
            class_weight='balanced',
            random_state=42,
            verbose=1,  # é€²æ—è¡¨ç¤ºã‚’æœ‰åŠ¹åŒ–
            n_jobs=-1
        ))
    ])
    
    # ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡æ¯”ã‚’è¡¨ç¤º
    pos_count = y.sum()
    neg_count = len(y) - pos_count
    print(f"\nâš–ï¸ ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”:")
    print(f"  Negative (0): {neg_count:,}")
    print(f"  Positive (1): {pos_count:,}")
    print(f"  æ¯”ç‡: {neg_count/pos_count:.2f}:1")
    
    # 5-foldäº¤å·®æ¤œè¨¼(LightGBMã¨åŒã˜)
    k_folds = 5
    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    print(f"\nğŸ”„ {k_folds}-fold äº¤å·®æ¤œè¨¼ã‚’é–‹å§‹...")
    print(f"ğŸ’¡ é€²æ—è¡¨ç¤º: ã‚½ãƒ«ãƒãƒ¼ã®åæŸçŠ¶æ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™\n")
    
    fold_metrics = []
    y_true_all = []
    y_prob_all = []
    
    cv_start_time = time.time()
    
    for i, (train_index, val_index) in enumerate(skf.split(X, y)):
        fold_start_time = time.time()
        elapsed = fold_start_time - cv_start_time
        
        print("=" * 80)
        print(f"--- Fold {i+1}/{k_folds} ---")
        print(f"ç·çµŒéæ™‚é–“: {format_time(elapsed)}")
        print("=" * 80)
        
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]
        
        # å­¦ç¿’
        print(f"\n  ğŸ“š å­¦ç¿’ä¸­ (è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,} ä»¶)...")
        fit_start = time.time()
        model.fit(X_train, y_train)
        fit_time = time.time() - fit_start
        print(f"  âœ“ å­¦ç¿’å®Œäº† (æ‰€è¦æ™‚é–“: {format_time(fit_time)})")
        
        # äºˆæ¸¬(ç¢ºç‡)
        print(f"  ğŸ”® äºˆæ¸¬ä¸­ (æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val):,} ä»¶)...")
        pred_start = time.time()
        y_prob = model.predict_proba(X_val)[:, 1]
        pred_time = time.time() - pred_start
        print(f"  âœ“ äºˆæ¸¬å®Œäº† (æ‰€è¦æ™‚é–“: {format_time(pred_time)})")
        
        # å…¨ä½“ã®çµæœã«è“„ç©
        y_true_all.extend(y_val)
        y_prob_all.extend(y_prob)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤(0.5)ã§ã®è©•ä¾¡
        y_pred_default = (y_prob >= 0.5).astype(int)
        
        acc = accuracy_score(y_val, y_pred_default)
        prec = precision_score(y_val, y_pred_default, average='binary', zero_division=0)
        rec = recall_score(y_val, y_pred_default, average='binary')
        f1 = f1_score(y_val, y_pred_default, average='binary')
        
        print(f"\n  ğŸ“Š [Threshold 0.5] Acc: {acc:.4f}, Prec: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}")
        
        fold_total_time = time.time() - fold_start_time
        print(f"  â±ï¸  Fold {i+1} åˆè¨ˆæ™‚é–“: {format_time(fold_total_time)}")
        
        fold_metrics.append({
            'Fold': i+1,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1 Score': f1,
            'Fit Time (sec)': fit_time,
            'Predict Time (sec)': pred_time,
            'Total Time (sec)': fold_total_time
        })
    
    cv_total_time = time.time() - cv_start_time
    print("\n" + "=" * 80)
    print(f"âœ… å…¨Foldå®Œäº† (åˆè¨ˆæ™‚é–“: {format_time(cv_total_time)})")
    print("=" * 80)
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    y_true_all = np.array(y_true_all)
    y_prob_all = np.array(y_prob_all)
    
    # AUCã®è¨ˆç®—
    auc_score = roc_auc_score(y_true_all, y_prob_all)
    print(f"\nğŸ“ˆ AUC Score: {auc_score:.4f}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    if sample_rate < 1.0:
        output_dir = f'results/model_comparison/logistic_regression_{int(sample_rate*100)}pct'
    else:
        output_dir = 'results/model_comparison/logistic_regression'
    os.makedirs(output_dir, exist_ok=True)
    
    # AUCã®ä¿å­˜
    with open(f'{output_dir}/auc_score.txt', 'w') as f:
        f.write(f"{auc_score:.4f}")
    
    # PRæ›²ç·šã¨æœ€é©é–¾å€¤ã®æ¢ç´¢
    precisions, recalls, thresholds = precision_recall_curve(y_true_all, y_prob_all)
    
    # F1ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã«ãªã‚‹é–¾å€¤
    f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€é©é–¾å€¤ã®æ¢ç´¢çµæœ")
    print("=" * 80)
    print(f"Best Threshold (Max F1): {best_threshold:.4f}")
    print(f"Max F1 Score: {best_f1:.4f}")
    print(f"Precision at Best: {precisions[best_idx]:.4f}")
    print(f"Recall at Best: {recalls[best_idx]:.4f}")
    
    # Recallé‡è¦–ã®é–¾å€¤è¨­å®š
    target_recall = 0.8
    valid_indices = np.where(recalls >= target_recall)[0]
    if len(valid_indices) > 0:
        best_prec_idx = valid_indices[np.argmax(precisions[valid_indices])]
        recall_threshold = thresholds[best_prec_idx] if best_prec_idx < len(thresholds) else thresholds[-1]
        
        print(f"\n[Recallé‡è¦–è¨­å®š (Target >= {target_recall})]")
        print(f"Threshold: {recall_threshold:.4f}")
        print(f"Precision: {precisions[best_prec_idx]:.4f}")
        print(f"Recall: {recalls[best_prec_idx]:.4f}")
    
    # PRæ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 6))
    plt.plot(recalls, precisions, marker='.', label=f'Logistic Regression ({sample_rate*100:.1f}% sample)')
    plt.xlabel('Recall (å†ç¾ç‡)')
    plt.ylabel('Precision (é©åˆç‡)')
    plt.title(f'Precision-Recall Curve (Logistic Regression, {sample_rate*100:.1f}% sample)')
    plt.legend()
    plt.grid(True)
    
    pr_path = f'{output_dir}/pr_curve.png'
    plt.savefig(pr_path)
    print(f"\nâœ“ PRæ›²ç·šã‚’ä¿å­˜: {pr_path}")
    plt.close()
    
    # æ··åŒè¡Œåˆ—(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ 0.5)
    y_pred_05 = (y_prob_all >= 0.5).astype(int)
    cm = confusion_matrix(y_true_all, y_pred_05)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['éæ­»äº¡', 'æ­»äº¡'], yticklabels=['éæ­»äº¡', 'æ­»äº¡'])
    plt.title(f'Confusion Matrix (Logistic Regression, {sample_rate*100:.1f}% sample, Threshold=0.5)')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    
    cm_path = f'{output_dir}/confusion_matrix.png'
    plt.savefig(cm_path)
    print(f"âœ“ æ··åŒè¡Œåˆ—ã‚’ä¿å­˜: {cm_path}")
    plt.close()
    
    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜
    metrics_df = pd.DataFrame(fold_metrics)
    metrics_df.to_csv(f'{output_dir}/metrics.csv', index=False)
    print(f"âœ“ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜: {output_dir}/metrics.csv")
    
    # å¹³å‡å€¤ã®è¨ˆç®—
    avg_metrics = metrics_df.mean()
    print(f"\nğŸ“Š {k_folds}-fold CV å¹³å‡ã‚¹ã‚³ã‚¢:")
    print(f"  Accuracy:  {avg_metrics['Accuracy']:.4f}")
    print(f"  Precision: {avg_metrics['Precision']:.4f}")
    print(f"  Recall:    {avg_metrics['Recall']:.4f}")
    print(f"  F1 Score:  {avg_metrics['F1 Score']:.4f}")
    
    print(f"\nâ±ï¸  å¹³å‡å®Ÿè¡Œæ™‚é–“ (1 Fold):")
    print(f"  å­¦ç¿’æ™‚é–“:  {format_time(avg_metrics['Fit Time (sec)'])}")
    print(f"  äºˆæ¸¬æ™‚é–“:  {format_time(avg_metrics['Predict Time (sec)'])}")
    print(f"  åˆè¨ˆæ™‚é–“:  {format_time(avg_metrics['Total Time (sec)'])}")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿè¡Œæ™‚é–“æ¨å®š
    script_total_time = time.time() - script_start_time
    if sample_rate < 1.0:
        estimated_full_time = script_total_time / sample_rate
        print(f"\nğŸ”® å…¨ãƒ‡ãƒ¼ã‚¿(100%)ã§ã®æ¨å®šå®Ÿè¡Œæ™‚é–“:")
        print(f"  ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«ç‡: {sample_rate*100:.1f}%")
        print(f"  å®Ÿæ¸¬æ™‚é–“: {format_time(script_total_time)}")
        print(f"  æ¨å®šæ™‚é–“(ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°): {format_time(estimated_full_time)}")
        
        if estimated_full_time > 3600:  # 1æ™‚é–“ä»¥ä¸Š
            print(f"  âš ï¸  æ¨å®šæ™‚é–“ãŒé•·ã„ãŸã‚ã€ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
    summary_lines = [
        f"# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° å®Ÿé¨“çµæœ ({sample_rate*100:.1f}% ã‚µãƒ³ãƒ—ãƒ«)",
        "",
        "**å®Ÿé¨“æ—¥æ™‚:** " + pd.Timestamp.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S'),
        "**ç›®çš„:** LightGBMã¨ã®æ¯”è¼ƒã®ãŸã‚ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«",
        f"**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡:** {sample_rate*100:.1f}%",
        "",
        "---",
        "",
        "## ğŸ“Š å®Ÿé¨“æ¦‚è¦",
        "",
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
        f"- ãƒ•ã‚¡ã‚¤ãƒ«: `{file_path}`",
        f"- å…ƒãƒ‡ãƒ¼ã‚¿æ•°: 1,895,275 ä»¶",
        f"- ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(df):,} ä»¶ ({sample_rate*100:.1f}%)",
        f"- Positive(æ­»äº¡äº‹æ•…): {pos_count:,} ä»¶",
        f"- Negative(éæ­»äº¡): {neg_count:,} ä»¶",
        f"- ä¸å‡è¡¡æ¯”: {neg_count/pos_count:.2f}:1",
        "",
        "### ç‰¹å¾´é‡",
        f"- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°: {len(final_cat_cols)} ã‚«ãƒ©ãƒ ",
        f"- æ•°å€¤å¤‰æ•°: {len(final_numeric_cols)} ã‚«ãƒ©ãƒ ",
        f"- ç·ç‰¹å¾´é‡æ•°: {X.shape[1]} (One-Hot Encodingå¾Œã¯å¢—åŠ )",
        f"- é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å‡¦ç†: ä¸Šä½{high_cardinality_threshold}ã‚«ãƒ†ã‚´ãƒªä»¥å¤–ã‚’'ãã®ä»–'ã«çµ±åˆ",
        "",
        "### ãƒ¢ãƒ‡ãƒ«è¨­å®š",
        "```python",
        "LogisticRegression(",
        "    penalty='l2',",
        "    C=1.0,",
        "    solver='saga',",
        "    max_iter=500,",
        "    class_weight='balanced',  # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–",
        "    verbose=1,",
        "    random_state=42",
        ")",
        "```",
        "",
        "---",
        "",
        "## ğŸ“ˆ è©•ä¾¡çµæœ",
        "",
        "### 5-fold CV å¹³å‡ã‚¹ã‚³ã‚¢ (Threshold 0.5)",
        "| æŒ‡æ¨™ | ã‚¹ã‚³ã‚¢ |",
        "|------|--------|",
        f"| **Accuracy** | {avg_metrics['Accuracy']:.4f} |",
        f"| **Precision** | {avg_metrics['Precision']:.4f} |",
        f"| **Recall** | {avg_metrics['Recall']:.4f} |",
        f"| **F1 Score** | {avg_metrics['F1 Score']:.4f} |",
        f"| **AUC** | **{auc_score:.4f}** |",
        "",
        "### æœ€é©é–¾å€¤ã®æ¢ç´¢çµæœ",
        "| è¨­å®š | é–¾å€¤ | Recall | Precision | F1 Score |",
        "|------|------|--------|-----------|----------|",
        f"| **Max F1** | {best_threshold:.4f} | {recalls[best_idx]:.4f} | {precisions[best_idx]:.4f} | {best_f1:.4f} |",
    ]
    
    if len(valid_indices) > 0:
        summary_lines.append(f"| **Recallâ‰¥0.8** | {recall_threshold:.4f} | {recalls[best_prec_idx]:.4f} | {precisions[best_prec_idx]:.4f} | - |")
    
    summary_lines.extend([
        "",
        "---",
        "",
        "## â±ï¸ å®Ÿè¡Œæ™‚é–“",
        "",
        f"| é …ç›® | æ™‚é–“ |",
        f"|------|------|",
        f"| **åˆè¨ˆå®Ÿè¡Œæ™‚é–“** | {format_time(script_total_time)} |",
        f"| **äº¤å·®æ¤œè¨¼æ™‚é–“** | {format_time(cv_total_time)} |",
        f"| **å¹³å‡å­¦ç¿’æ™‚é–“(1 Fold)** | {format_time(avg_metrics['Fit Time (sec)'])} |",
        f"| **å¹³å‡äºˆæ¸¬æ™‚é–“(1 Fold)** | {format_time(avg_metrics['Predict Time (sec)'])} |",
    ])
    
    if sample_rate < 1.0:
        estimated_full_time = script_total_time / sample_rate
        summary_lines.extend([
            "",
            "### å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®æ¨å®šæ™‚é–“",
            f"- ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«ç‡: **{sample_rate*100:.1f}%**",
            f"- å®Ÿæ¸¬æ™‚é–“: **{format_time(script_total_time)}**",
            f"- æ¨å®šæ™‚é–“(ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°): **{format_time(estimated_full_time)}**",
        ])
    
    summary_lines.extend([
        "",
        "---",
        "",
        "## ğŸ’¡ è€ƒå¯Ÿ",
        "",
        "### å‰å‡¦ç†ã®é•ã„",
        "- **ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°**: One-Hot Encodingã‚’ä½¿ç”¨(LightGBMã¯categoryå‹ã‚’ç›´æ¥æ‰±ãˆã‚‹)",
        "- **æ•°å€¤å¤‰æ•°**: StandardScalerã§æ¨™æº–åŒ–(LightGBMã¯ä¸è¦)",
        "- **æ¬ æå€¤**: SimpleImputerã§è£œå®Œ(LightGBMã¯æ¬ æå€¤ã‚’ãã®ã¾ã¾æ‰±ãˆã‚‹)",
        "",
        "### ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´",
        "- **ç·šå½¢ãƒ¢ãƒ‡ãƒ«**: ç‰¹å¾´é‡é–“ã®è¤‡é›‘ãªç›¸äº’ä½œç”¨ã‚’æ‰ãˆã«ãã„",
        "- **è§£é‡ˆæ€§**: ä¿‚æ•°(Coefficients)ã‹ã‚‰å„ç‰¹å¾´é‡ã®å½±éŸ¿ã‚’ç›´æ¥èª­ã¿å–ã‚Œã‚‹",
        "- **è¨ˆç®—ã‚³ã‚¹ãƒˆ**: LightGBMã‚ˆã‚Šå­¦ç¿’æ™‚é–“ãŒçŸ­ã„(ãŸã ã—One-Hot Encodingã§ç‰¹å¾´é‡æ•°ãŒå¢—åŠ )",
        "",
        "---",
        "",
        "## ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«",
        f"- [PRæ›²ç·š]({pr_path})",
        f"- [æ··åŒè¡Œåˆ—]({cm_path})",
        f"- [è©•ä¾¡æŒ‡æ¨™CSV]({output_dir}/metrics.csv)",
        f"- [AUCã‚¹ã‚³ã‚¢]({output_dir}/auc_score.txt)",
    ])
    
    summary_path = f'{output_dir}/summary_report.md'
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(summary_lines))
    print(f"âœ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {summary_path}")
    
    print("\n" + "=" * 80)
    print("âœ… å®Ÿé¨“å®Œäº†")
    print("=" * 80)
    print(f"ğŸ“‚ çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_dir}")
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {format_time(script_total_time)}")
    
    if sample_rate < 1.0 and estimated_full_time < 28800:  # 8æ™‚é–“ä»¥å†…ãªã‚‰
        next_sample = min(sample_rate * 10, 1.0)
        print(f"\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: {next_sample*100:.0f}%ã‚µãƒ³ãƒ—ãƒ«ã§ã®å®Ÿè¡Œã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        print(f"   ã‚³ãƒãƒ³ãƒ‰: python {__file__} --sample-rate {next_sample}")

if __name__ == "__main__":
    main()

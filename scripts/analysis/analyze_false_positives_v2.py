"""
èª¤æ¤œçŸ¥ï¼ˆFalse Positiveï¼‰åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2.1
===========================================
Implementation Planã«åŸºã¥ãã€FPã®è©³ç´°åˆ†æã‚’è¡Œã†ã€‚

ä¸»ãªæ©Ÿèƒ½:
1. OOF (Out-of-Fold) äºˆæ¸¬ã‚¹ã‚³ã‚¢ã®ç”Ÿæˆãƒ»ä¿å­˜
2. å…ƒãƒ‡ãƒ¼ã‚¿ã¨ã®çµåˆã«ã‚ˆã‚‹é‡å‚·åº¦æƒ…å ±ã®å¾©å…ƒ
3. å³ã—ã„é–¾å€¤ã«ã‚ˆã‚‹ã€Œé ‘å›ºãªFP (Hard FP)ã€ã®æŠ½å‡º
4. é‡å‚·åº¦ãƒ»åœ°ç†çš„åˆ†å¸ƒã®åˆ†æ
5. é«˜ç¢ºä¿¡åº¦FPã«å¯¾ã™ã‚‹SHAPå€‹ç¥¨åˆ†æ

ä¿®æ­£(v2.1):
- SHAP Force Plotã‚¨ãƒ©ãƒ¼ä¿®æ­£
- --skip-cv ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆæ—¢å­˜çµæœã‚’ç”¨ã„ã¦åˆ†æã®ã¿å†å®Ÿè¡Œï¼‰

æ¨å®šå®Ÿè¡Œæ™‚é–“: ç´„30åˆ†ã€œ1æ™‚é–“ï¼ˆ--skip-cvæ™‚ã¯æ•°åˆ†ï¼‰
"""

import pandas as pd
import numpy as np
import os
import gc
import json
import argparse
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb
import shap
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib as mpl
import warnings

warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š (Windows)
mpl.rcParams['font.family'] = 'MS Gothic'
mpl.rcParams['axes.unicode_minus'] = False


class FalsePositiveAnalyzer:
    """
    èª¤æ¤œçŸ¥ï¼ˆFalse Positiveï¼‰ã®è©³ç´°åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(
        self,
        features_path: str = "data/processed/honhyo_clean_with_features.csv",
        raw_path: str = "data/raw/honhyo_all_shishasuu_binary.csv",
        target_col: str = "æ­»è€…æ•°",
        n_folds: int = 5,
        random_state: int = 42
    ):
        self.features_path = features_path
        self.raw_path = raw_path
        self.target_col = target_col
        self.n_folds = n_folds
        self.random_state = random_state
        
        self.output_dir = "results/analysis/fp_analysis_v2"
        self.shap_dir = os.path.join(self.output_dir, "shap_force_plots")
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.shap_dir, exist_ok=True)
        
        self.oof_proba = None
        self.df_merged = None
        self.threshold_strict = None
        self.df_fp_hard = None
        
        print("=" * 70)
        print("èª¤æ¤œçŸ¥ï¼ˆFalse Positiveï¼‰åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2.1")
        print("=" * 70)
        print(f"å‡ºåŠ›å…ˆ: {self.output_dir}")
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        self.df_features = pd.read_csv(self.features_path)
        print(f"   ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿: {self.df_features.shape}")
        
        self.df_raw = pd.read_csv(self.raw_path)
        print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {self.df_raw.shape}")
        
        self.y = self.df_features[self.target_col].values
        self.X = self.df_features.drop(columns=[self.target_col])
        
        if 'ç™ºç”Ÿæ—¥æ™‚' in self.X.columns:
            self.X = self.X.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        known_categoricals = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
            'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
            'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
            'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
            'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
        ]
        
        self.categorical_cols = []
        self.numeric_cols = []
        
        for col in self.X.columns:
            if col in known_categoricals or self.X[col].dtype == 'object':
                self.categorical_cols.append(col)
                self.X[col] = self.X[col].astype(str).fillna('Missing')
            else:
                self.numeric_cols.append(col)
                median_val = self.X[col].median()
                self.X[col] = self.X[col].fillna(median_val).astype(np.float32)
        
        print(f"   æ•°å€¤ç‰¹å¾´é‡: {len(self.numeric_cols)}, ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {len(self.categorical_cols)}")
        gc.collect()
        
        self.ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
        self.ordinal_encoder.fit(self.X[self.categorical_cols])
        self.feature_names = self.numeric_cols + self.categorical_cols

    def train_full_model(self, model_name: str = "RandomForest"):
        """SHAPåˆ†æç”¨ã«å…¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’"""
        print(f"\nğŸ§  ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿, {model_name}ï¼‰...")
        X_cat_enc = self.ordinal_encoder.transform(self.X[self.categorical_cols])
        X_enc = np.hstack([self.X[self.numeric_cols].values, X_cat_enc])
        
        if model_name == "RandomForest":
            self.final_model = RandomForestClassifier(
                n_estimators=100, max_depth=10, min_samples_leaf=20,
                class_weight='balanced', random_state=self.random_state, n_jobs=-1
            )
            self.final_model.fit(X_enc, self.y)
            self.X_encoded = X_enc
        else:
            X_lgb = self.X.copy()
            for c in self.categorical_cols:
                X_lgb[c] = X_lgb[c].astype('category')
            lgb_params = {
                'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': -1,
                'n_estimators': 500, 'learning_rate': 0.05, 'num_leaves': 31,
                'random_state': self.random_state, 'n_jobs': -1
            }
            self.final_model = lgb.LGBMClassifier(**lgb_params)
            self.final_model.fit(X_lgb, self.y)
            self.X_encoded = X_lgb
        print("   å†å­¦ç¿’å®Œäº†")

    def generate_oof_predictions(self, model_name: str = "RandomForest"):
        """Out-of-Fold (OOF) äºˆæ¸¬ã‚’ç”Ÿæˆ"""
        print(f"\nğŸ”® OOFäºˆæ¸¬ç”Ÿæˆä¸­ ({model_name})...")
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba = np.zeros(len(self.y))
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            print(f"   Fold {fold + 1}/{self.n_folds}...")
            X_train = self.X.iloc[train_idx].copy()
            X_val = self.X.iloc[val_idx].copy()
            y_train = self.y[train_idx]
            
            X_train_cat_enc = self.ordinal_encoder.transform(X_train[self.categorical_cols])
            X_val_cat_enc = self.ordinal_encoder.transform(X_val[self.categorical_cols])
            
            X_train_enc = np.hstack([X_train[self.numeric_cols].values, X_train_cat_enc])
            X_val_enc = np.hstack([X_val[self.numeric_cols].values, X_val_cat_enc])
            
            if model_name == "RandomForest":
                model = RandomForestClassifier(
                    n_estimators=100, max_depth=10, min_samples_leaf=20,
                    class_weight='balanced', random_state=self.random_state, n_jobs=-1
                )
                model.fit(X_train_enc, y_train)
                y_prob = model.predict_proba(X_val_enc)[:, 1]
            else:
                pass 
            
            self.oof_proba[val_idx] = y_prob
            del model, X_train, X_val
            gc.collect()
        
        print(f"   OOFäºˆæ¸¬å®Œäº†ã€‚å¹³å‡äºˆæ¸¬ç¢ºç‡: {self.oof_proba.mean():.4f}")
    
    def merge_raw_data(self):
        """å…ƒãƒ‡ãƒ¼ã‚¿ã¨çµåˆ"""
        print("\nğŸ“ å…ƒãƒ‡ãƒ¼ã‚¿ã¨ã®çµåˆ...")
        severity_cols = ['äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰', 'è² å‚·è€…æ•°']
        self.df_merged = self.df_features.copy()
        self.df_merged['oof_proba'] = self.oof_proba
        self.df_merged['y_true'] = self.y
        for col in severity_cols:
            if col in self.df_raw.columns:
                self.df_merged[col] = self.df_raw[col].values
        print(f"   çµåˆå®Œäº†: {self.df_merged.shape}")
    
    def find_precision_threshold(self, target_precision: float = 0.20):
        """Precisionç›®æ¨™é”æˆé–¾å€¤ã®æ¢ç´¢"""
        print(f"\nğŸ“ é–¾å€¤æ¢ç´¢ (ç›®æ¨™Precision = {target_precision:.0%})...")
        thresholds = np.arange(0.1, 0.95, 0.01)
        
        for thresh in reversed(thresholds):
            y_pred = (self.oof_proba >= thresh).astype(int)
            tp = ((y_pred == 1) & (self.y == 1)).sum()
            fp = ((y_pred == 1) & (self.y == 0)).sum()
            if tp + fp > 0:
                precision = tp / (tp + fp)
                if precision >= target_precision:
                    self.threshold_strict = thresh
                    final_precision = precision
                    break
        else:
            self.threshold_strict = 0.5
            final_precision = 0.0 
        
        print(f"   é¸å®šé–¾å€¤: {self.threshold_strict:.2f} â†’ Precision: {final_precision:.2%}")

        threshold_analysis = []
        for thresh in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
            y_pred = (self.oof_proba >= thresh).astype(int)
            tp = ((y_pred == 1) & (self.y == 1)).sum()
            fp = ((y_pred == 1) & (self.y == 0)).sum()
            fn = ((y_pred == 0) & (self.y == 1)).sum()
            precision = tp / (tp + fp) if tp + fp > 0 else 0
            recall = tp / (tp + fn) if tp + fn > 0 else 0
            threshold_analysis.append({
                'Threshold': thresh,
                'TP': tp, 'FP': fp, 'FN': fn, 'Precision': precision, 'Recall': recall
            })
        pd.DataFrame(threshold_analysis).to_csv(os.path.join(self.output_dir, "threshold_analysis.csv"), index=False)

    def analyze_false_positives(self):
        """FPè©³ç´°åˆ†æ"""
        print("\nğŸ”¬ èª¤æ¤œçŸ¥ï¼ˆFPï¼‰åˆ†æé–‹å§‹...")
        y_pred = (self.oof_proba >= self.threshold_strict).astype(int)
        fp_mask = (y_pred == 1) & (self.y == 0)
        self.df_fp_hard = self.df_merged[fp_mask].copy()
        
        print(f"   é–¾å€¤{self.threshold_strict:.2f}ã§ã®Hard FPæ•°: {len(self.df_fp_hard):,}")
        self.df_fp_hard.to_csv(os.path.join(self.output_dir, "fp_high_confidence.csv"), index=False)
        
        print("\nğŸ“Š A. é‡å‚·åº¦åˆ†æ...")
        if 'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰' in self.df_fp_hard.columns:
            severity_dist = self.df_fp_hard['äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰'].value_counts()
            severity_dist.to_csv(os.path.join(self.output_dir, "fp_severity_distribution.csv"))
            print(f"   é‡å‚·åº¦åˆ†å¸ƒ (Hard FP):")
            for val, cnt in severity_dist.head(5).items():
                print(f"      {val}: {cnt:,} ({cnt/len(self.df_fp_hard)*100:.1f}%)")
        
        print("\nğŸ“Š D. åœ°ç†çš„åˆ†æ...")
        if 'area_id' in self.df_fp_hard.columns:
            total_by_area = self.df_merged['area_id'].value_counts()
            fp_by_area = self.df_fp_hard['area_id'].value_counts()
            fp_rate = (fp_by_area / total_by_area * 100).dropna().sort_values(ascending=False)
            fp_rate.head(20).to_csv(os.path.join(self.output_dir, "fp_rate_by_area.csv"))

    def shap_individual_analysis(self, top_n: int = 10):
        """SHAPå€‹ç¥¨åˆ†æï¼ˆä¿®æ­£ç‰ˆ v2.2ï¼‰"""
        print(f"\nğŸ¯ C. SHAPå€‹ç¥¨åˆ†æ (Top {top_n})...")
        df_fp_sorted = self.df_fp_hard.sort_values('oof_proba', ascending=False).head(top_n)
        print(f"   å¯¾è±¡: æœ€ã‚‚ç¢ºä¿¡åº¦ã®é«˜ã„FP {len(df_fp_sorted)} ä»¶")
        
        try:
            # check_additivity=False to avoid minor numerical errors
            if isinstance(self.final_model, RandomForestClassifier):
                explainer = shap.TreeExplainer(self.final_model)
            else:
                explainer = shap.TreeExplainer(self.final_model)
        except Exception as e:
            print(f"   âš ï¸ SHAP Explainerä½œæˆå¤±æ•—: {e}")
            return
            
        for i, (idx, row) in enumerate(df_fp_sorted.iterrows()):
            print(f"   [{i+1}/{len(df_fp_sorted)}] Index={idx}, ç¢ºç‡={row['oof_proba']:.3f}")
            try:
                if isinstance(self.final_model, RandomForestClassifier):
                    X_point = self.X.iloc[[idx]]
                    X_cat_enc = self.ordinal_encoder.transform(X_point[self.categorical_cols])
                    X_point_enc = np.hstack([X_point[self.numeric_cols].values, X_cat_enc])
                    
                    shap_values = explainer.shap_values(X_point_enc, check_additivity=False)
                    
                    # æˆ»ã‚Šå€¤ã®å‹ãƒã‚§ãƒƒã‚¯ã¨é©åˆ‡ãªå–å¾—
                    if isinstance(shap_values, list):
                        # print(f"      DEBUG: list len={len(shap_values)}")
                        if len(shap_values) > 1:
                            sv_target = shap_values[1][0] # Class 1
                        else:
                            sv_target = shap_values[0][0]
                    else:
                        # print(f"      DEBUG: array shape={shap_values.shape}")
                        if len(shap_values.shape) == 3:
                            sv_target = shap_values[0, :, 1]
                        elif len(shap_values.shape) == 2:
                             # (n_samples, n_features) -> binary classification single output
                             sv_target = shap_values[0]
                        else:
                             sv_target = shap_values[0] # Fallback
                    
                    base_value = explainer.expected_value
                    if isinstance(base_value, list):
                        base_value = base_value[1] # Class 1
                    elif isinstance(base_value, np.ndarray) and len(base_value) > 1:
                        base_value = base_value[1]
                        
                    plt.figure(figsize=(20, 6))
                    shap.force_plot(
                        base_value, sv_target, X_point_enc[0],
                        feature_names=self.feature_names, matplotlib=True, show=False
                    )
                else:
                    pass 
                
                plt.title(f"FP Index: {idx}, Prob: {row['oof_proba']:.3f}", fontsize=12)
                plt.tight_layout()
                plt.savefig(os.path.join(self.shap_dir, f"force_plot_{i+1:02d}_idx{idx}.png"), bbox_inches='tight', dpi=100)
                plt.close()
            except Exception as e:
                print(f"      âš ï¸ Force Plotç”Ÿæˆå¤±æ•—: {e}")
                # import traceback
                # traceback.print_exc()

    def generate_report(self):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        y_pred_05 = (self.oof_proba >= 0.5).astype(int)
        tp_05 = ((y_pred_05 == 1) & (self.y == 1)).sum()
        fp_05 = ((y_pred_05 == 1) & (self.y == 0)).sum()
        
        y_pred_strict = (self.oof_proba >= self.threshold_strict).astype(int)
        tp_s = ((y_pred_strict == 1) & (self.y == 1)).sum()
        fp_s = ((y_pred_strict == 1) & (self.y == 0)).sum()
        
        report = f"""# èª¤æ¤œçŸ¥ï¼ˆFalse Positiveï¼‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ v2.1

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ¦‚è¦
æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ã€æ­»äº¡äº‹æ•…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®èª¤æ¤œçŸ¥ï¼ˆFalse Positiveï¼‰ã‚’è©³ç´°ã«åˆ†æã—ãŸçµæœã‚’ã¾ã¨ã‚ã‚‹ã€‚

## 1. å…¨ä½“çµæœï¼ˆé–¾å€¤ = 0.5ï¼‰
- True Positive (TP): {tp_05:,}
- False Positive (FP): {fp_05:,}
- Precision: {tp_05/(tp_05+fp_05)*100:.2f}%

## 2. å³ã—ã„é–¾å€¤ã§ã®åˆ†æï¼ˆé–¾å€¤ = {self.threshold_strict:.2f}ï¼‰
Precision 20%ä»¥ä¸Šã‚’ç›®æ¨™ã¨ã—ãŸé–¾å€¤ã€‚
- Hard False Positive: {fp_s:,}
- Precision: {tp_s/(tp_s+fp_s)*100:.2f}%

## 3. é‡å‚·åº¦åˆ†æ
è©³ç´°: `fp_severity_distribution.csv`

## 4. SHAPå€‹ç¥¨åˆ†æ
ä¿å­˜å…ˆ: `shap_force_plots/`
"""
        with open(os.path.join(self.output_dir, "fp_analysis_v2_report.md"), 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {os.path.join(self.output_dir, 'fp_analysis_v2_report.md')}")

    def run(self, model_name: str = "RandomForest", skip_cv: bool = False):
        self.load_data()
        
        oof_path = os.path.join(self.output_dir, "oof_proba.csv")
        
        if skip_cv and os.path.exists(oof_path):
            print(f"\nâ© æ—¢å­˜ã®OOFäºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã¿ä¸­: {oof_path}")
            self.oof_proba = pd.read_csv(oof_path)['oof_proba'].values
            if len(self.oof_proba) != len(self.y):
                print("âš ï¸ ã‚µã‚¤ã‚ºä¸ä¸€è‡´ã®ãŸã‚å†è¨ˆç®—ã—ã¾ã™")
                self.generate_oof_predictions(model_name=model_name)
        else:
            if skip_cv:
                print("âš ï¸ æ—¢å­˜ã®OOFäºˆæ¸¬çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å†è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚")
            self.generate_oof_predictions(model_name=model_name)
            pd.DataFrame({'oof_proba': self.oof_proba}).to_csv(oof_path, index=False)

        self.train_full_model(model_name=model_name)
        self.merge_raw_data()
        self.find_precision_threshold(target_precision=0.20)
        self.analyze_false_positives()
        self.shap_individual_analysis(top_n=10)
        self.generate_report()

        print("\n" + "=" * 70)
        print("âœ… åˆ†æå®Œäº†!")
        print(f"   çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print("=" * 70)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--skip-cv', action='store_true', help='Skip CV if OOF probabilities are already saved')
    args = parser.parse_args()
    
    analyzer = FalsePositiveAnalyzer()
    analyzer.run(model_name="RandomForest", skip_cv=args.skip_cv)

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š (Windowså‘ã‘)
mpl.rcParams['font.family'] = 'MS Gothic'

def main():
    """
    äº‹å¾Œæƒ…å ±ã‚’é™¤å¤–ã—ã€äº¤å·®æ¤œè¨¼ã‚’ç”¨ã„ã¦ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹
    """
    
    print("=" * 80)
    print("éå­¦ç¿’æ¤œè¨¼: äº‹å¾Œæƒ…å ±é™¤å¤–ãƒ¢ãƒ‡ãƒ«ã®äº¤å·®æ¤œè¨¼")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    file_path = 'data/raw/honhyo_all_shishasuu_binary.csv'
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    
    try:
        df = pd.read_csv(file_path)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} ä»¶")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ç›®çš„å¤‰æ•°
    target_col = 'æ­»è€…æ•°'
    
    # é™¤å¤–ã™ã‚‹åˆ—ï¼ˆIDã€ç®¡ç†ç•ªå·ã€ãŠã‚ˆã³äº‹å¾Œæƒ…å ±ï¼‰
    # ã“ã“ã§ã€Œäº‹æ•…ãŒèµ·ããŸå¾Œã§ãªã„ã¨åˆ†ã‹ã‚‰ãªã„æƒ…å ±ã€ã‚’å¾¹åº•çš„ã«æ’é™¤ã™ã‚‹
    drop_cols = [
        'è³‡æ–™åŒºåˆ†', 'æœ¬ç¥¨ç•ªå·',           # ç®¡ç†æƒ…å ±
        'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰',        # äº‹å¾Œæƒ…å ±
        'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',        # äº‹å¾Œæƒ…å ±
        'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰',      # äº‹å¾Œæƒ…å ±
        'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',      # äº‹å¾Œæƒ…å ±
        'è² å‚·è€…æ•°',                       # äº‹å¾Œæƒ…å ±
        'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Aï¼‰',      # äº‹å¾Œæƒ…å ±ï¼ˆäº‹æ•…æ…‹æ§˜ã«ã‚ˆã‚‹ãŒã€çµæœã«è¿‘ã„ï¼‰
        'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Bï¼‰',      # äº‹å¾Œæƒ…å ±
        'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰',    # ä½œå‹•çŠ¶æ³ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚é™¤å¤–
        'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
        'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰',
        'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
        'äº‹æ•…å†…å®¹'                        # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åŸå› ï¼ˆæ­»äº¡/è² å‚·ã®åŒºåˆ†ãã®ã‚‚ã®ï¼‰
    ]
    
    print("\nğŸš« é™¤å¤–ã™ã‚‹ç‰¹å¾´é‡ï¼ˆäº‹å¾Œæƒ…å ±ãªã©ï¼‰:")
    for col in drop_cols:
        print(f"  - {col}")
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­...")
    df_clean = df.drop(columns=drop_cols, errors='ignore')
    
    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°
    X = df_clean.drop(columns=[target_col])
    y = df_clean[target_col]
    
    # æ¬ æå€¤å‡¦ç†
    num_cols = X.select_dtypes(include=[np.number]).columns
    X[num_cols] = X[num_cols].fillna(X[num_cols].median())
    
    cat_cols = X.select_dtypes(include=['object']).columns
    for col in cat_cols:
        X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown')
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    le = LabelEncoder()
    for col in cat_cols:
        X[col] = le.fit_transform(X[col].astype(str))
        
    print(f"âœ“ å‰å‡¦ç†å®Œäº† - ç‰¹å¾´é‡æ•°: {X.shape[1]}")
    
    # äº¤å·®æ¤œè¨¼ (5-fold)
    k_folds = 5
    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    print(f"\nğŸ”„ {k_folds}-fold äº¤å·®æ¤œè¨¼ã‚’é–‹å§‹...")
    
    fold_metrics = []
    
    for i, (train_index, val_index) in enumerate(skf.split(X, y)):
        print(f"\n--- Fold {i+1}/{k_folds} ---")
        
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        # å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’ç‰¹å®š
        X_train_minority = X_train[y_train == 1]
        y_train_minority = y_train[y_train == 1]
        
        X_train_majority = X_train[y_train == 0]
        y_train_majority = y_train[y_train == 0]
        
        # ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
        X_minority_upsampled, y_minority_upsampled = resample(
            X_train_minority, y_train_minority,
            replace=True,
            n_samples=len(X_train_majority),
            random_state=42
        )
        
        # çµåˆ
        X_train_res = pd.concat([X_train_majority, X_minority_upsampled])
        y_train_res = pd.concat([y_train_majority, y_minority_upsampled])
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1) # æ™‚é–“çŸ­ç¸®ã®ãŸã‚50æœ¨
        rf.fit(X_train_res, y_train_res)
        
        # è©•ä¾¡
        y_pred = rf.predict(X_val)
        
        acc = accuracy_score(y_val, y_pred)
        prec = precision_score(y_val, y_pred, average='binary', zero_division=0)
        rec = recall_score(y_val, y_pred, average='binary')
        f1 = f1_score(y_val, y_pred, average='binary')
        
        print(f"  Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}")
        
        fold_metrics.append({
            'Fold': i+1,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1 Score': f1
        })
    
    # å¹³å‡ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    metrics_df = pd.DataFrame(fold_metrics)
    mean_metrics = metrics_df.mean()
    
    print("\n" + "=" * 80)
    print("ğŸ“Š äº¤å·®æ¤œè¨¼çµæœ (å¹³å‡)")
    print("=" * 80)
    print(f"Accuracy: {mean_metrics['Accuracy']:.4f}")
    print(f"Precision: {mean_metrics['Precision']:.4f}")
    print(f"Recall:    {mean_metrics['Recall']:.4f}")
    print(f"F1 Score:  {mean_metrics['F1 Score']:.4f}")
    
    # çµæœã®ä¿å­˜
    output_csv = 'results/analysis/refined_model_cv_metrics.csv'
    metrics_df.to_csv(output_csv, index=False)
    print(f"\nâœ“ è©³ç´°çµæœã‚’ä¿å­˜: {output_csv}")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®å†å­¦ç¿’ã¨ç‰¹å¾´é‡é‡è¦åº¦ã®ç¢ºèªï¼ˆå‚è€ƒç”¨ï¼‰
    print("\nğŸ” å…¨ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’ã—ã¦ç‰¹å¾´é‡é‡è¦åº¦ã‚’ç¢ºèªä¸­...")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
    X_minority = X[y == 1]
    y_minority = y[y == 1]
    X_majority = X[y == 0]
    y_majority = y[y == 0]
    
    X_min_up, y_min_up = resample(X_minority, y_minority, replace=True, n_samples=len(X_majority), random_state=42)
    X_full = pd.concat([X_majority, X_min_up])
    y_full = pd.concat([y_majority, y_min_up])
    
    rf_full = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_full.fit(X_full, y_full)
    
    feature_importances = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_full.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # é‡è¦åº¦ã®å¯è¦–åŒ–
    plt.figure(figsize=(12, 10))
    sns.barplot(
        x='importance', 
        y='feature', 
        data=feature_importances.head(20),
        palette='viridis'
    )
    plt.title('ç‰¹å¾´é‡é‡è¦åº¦ Top 20 (äº‹å¾Œæƒ…å ±é™¤å¤–ãƒ¢ãƒ‡ãƒ«)', fontsize=16, pad=20)
    plt.xlabel('é‡è¦åº¦', fontsize=12)
    plt.ylabel('ç‰¹å¾´é‡', fontsize=12)
    plt.tight_layout()
    
    fi_path = 'results/visualizations/feature_importance_refined.png'
    plt.savefig(fi_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {fi_path}")

    print("\nç‰¹å¾´é‡é‡è¦åº¦ (Top 20):")
    print("-" * 80)
    for idx, row in feature_importances.head(20).iterrows():
        print(f"{row['feature']:45s}: {row['importance']:.6f}")
    
    print("\nâœ… æ¤œè¨¼å®Œäº†")

if __name__ == "__main__":
    main()

"""
ã€Œåˆ¤å®šãŒé›£ã—ã„äº‹æ•… (Hard Samples)ã€åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
================================================
ç›®çš„:
1. Stage 2 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦ã€Œé›£ã—ã„ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®šã™ã‚‹ã€‚
   - Boundary Samples: äºˆæ¸¬ç¢ºç‡ãŒ 0.5 ä»˜è¿‘ (0.3 - 0.7) ã§è¿·ã£ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿
   - High Variance Samples: ãƒ¢ãƒ‡ãƒ«é–“ (LGBM, CatBoost, TabNet) ã§æ„è¦‹ãŒå‰²ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿
   - Error Samples: False Positive (èª¤æ¤œçŸ¥) / False Negative (è¦‹é€ƒã—)

2. ã“ã‚Œã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´ï¼ˆæ˜¼å¤œã€åœ°å½¢ã€é“è·¯å½¢çŠ¶ãªã©ï¼‰ã‚’é›†è¨ˆã—ã€
   ã€Œå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ« (Mixture of Experts)ã€ã®åˆ‡ã‚Šå£ã‚’ææ¡ˆã™ã‚‹ã€‚

å®Ÿè¡Œæ–¹æ³•:
    python scripts/analysis/analyze_hard_samples.py
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import recall_score

class HardSampleAnalyzer:
    def __init__(
        self,
        data_path="data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv",
        target_col="fatal",
        ckpt_dir="results/ensemble_stage2/checkpoints",
        output_dir="results/ensemble_stage2",
        random_state=42,
        n_folds=5,
        n_seeds=3
    ):
        self.data_path = data_path
        self.target_col = target_col
        self.ckpt_dir = ckpt_dir
        self.output_dir = output_dir
        self.random_state = random_state
        self.n_folds = n_folds
        self.n_seeds = n_seeds

    def load_data_and_predictions(self):
        """ãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬å€¤ã®èª­ã¿è¾¼ã¿ (OOFå†æ§‹ç¯‰)"""
        print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        df = pd.read_csv(self.data_path)
        y_all = df[self.target_col].values
        X_all = df.drop(columns=[self.target_col])
        if 'ç™ºç”Ÿæ—¥æ™‚' in X_all.columns:
            X_all = X_all.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])

        # Data Split (Train/Test)
        X_train, X_test, y_train, y_test = train_test_split(
            X_all, y_all, test_size=0.2, random_state=self.random_state, stratify=y_all
        )
        X_train = X_train.reset_index(drop=True)
        self.X_train = X_train
        self.y_train = y_train

        # --- Stage 1 OOF & Mask ---
        print("   Stage 1 OOF å†æ§‹ç¯‰...")
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        oof_stage1 = np.zeros(len(y_train))
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
            fold_dir = os.path.join(self.ckpt_dir, f"stage1_fold{fold}")
            fold_pred = np.zeros(len(val_idx))
            for seed in range(self.n_seeds):
                pred_path = os.path.join(fold_dir, f"seed{seed}_pred.npy")
                if os.path.exists(pred_path):
                    fold_pred += np.load(pred_path)
            oof_stage1[val_idx] = fold_pred / self.n_seeds

        # é–¾å€¤åˆ¤å®š (Recall 99%)
        threshold_stage1 = 0.0400 # å‰å›ã®å®Ÿé¨“å€¤ã‚’ä½¿ç”¨
        stage2_mask = oof_stage1 >= threshold_stage1
        
        self.X_s2 = X_train[stage2_mask].reset_index(drop=True)
        self.y_s2 = y_train[stage2_mask]
        print(f"   Stage 2 Target Data: {len(self.y_s2):,}")

        # --- Stage 2 OOF (All Models) ---
        print("   Stage 2 OOF å†æ§‹ç¯‰ (LGBM, CatBoost, TabNet)...")
        skf_s2 = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        self.oof_lgb = np.zeros(len(self.y_s2))
        self.oof_cat = np.zeros(len(self.y_s2))
        self.oof_tab = np.zeros(len(self.y_s2))
        
        for fold, (train_idx, val_idx) in enumerate(skf_s2.split(self.X_s2, self.y_s2)):
            fold_dir = os.path.join(self.ckpt_dir, f"stage2_fold{fold}")
            
            p_lgb = os.path.join(fold_dir, "lgb_pred.npy")
            p_cat = os.path.join(fold_dir, "cat_pred.npy")
            p_tab = os.path.join(fold_dir, "tab_pred.npy")
            
            if os.path.exists(p_lgb): self.oof_lgb[val_idx] = np.load(p_lgb)
            if os.path.exists(p_cat): self.oof_cat[val_idx] = np.load(p_cat)
            if os.path.exists(p_tab): self.oof_tab[val_idx] = np.load(p_tab)

        # Ensemble Prediction (Even weights for analysis simplicity or optimized)
        # Optimized weights from previous run: 0.333 each
        w = [1/3, 1/3, 1/3]
        self.oof_ens = w[0]*self.oof_lgb + w[1]*self.oof_cat + w[2]*self.oof_tab

    def analyze_hard_samples(self):
        """é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å®šã¨åˆ†æ"""
        print("\nğŸ” åˆ¤å®šé›£æ˜“åº¦åˆ†æã‚’å®Ÿè¡Œä¸­...")
        
        df_res = self.X_s2.copy()
        df_res['target'] = self.y_s2
        df_res['pred_ens'] = self.oof_ens
        df_res['pred_lgb'] = self.oof_lgb
        df_res['pred_cat'] = self.oof_cat
        df_res['pred_tab'] = self.oof_tab
        
        # 1. Uncertainty (è¿·ã„) : 0.4 < prob < 0.6
        df_res['is_uncertain'] = (df_res['pred_ens'] > 0.4) & (df_res['pred_ens'] < 0.6)
        
        # 2. Disagreement (æ„è¦‹å‰²ã‚Œ) : æ¨™æº–åå·®ãŒå¤§ãã„ä¸Šä½10%
        # 3ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã®åˆ†æ•£ã‚’è¨ˆç®—
        preds_stack = np.vstack([self.oof_lgb, self.oof_cat, self.oof_tab])
        df_res['model_std'] = np.std(preds_stack, axis=0)
        high_var_thresh = np.percentile(df_res['model_std'], 90)
        df_res['is_disagreement'] = df_res['model_std'] > high_var_thresh
        
        # 3. Error (é–“é•ã£ãŸã‚‚ã®) with standard threshold 0.5
        df_res['pred_binary'] = (df_res['pred_ens'] >= 0.5).astype(int)
        df_res['is_fp'] = (df_res['target'] == 0) & (df_res['pred_binary'] == 1)
        df_res['is_fn'] = (df_res['target'] == 1) & (df_res['pred_binary'] == 0)
        
        # ã‚«ãƒ†ã‚´ãƒªä½œæˆ: "Hard Sample"
        # å®šç¾©: ã€Œè¿·ã£ã¦ã„ã‚‹ã€ ã¾ãŸã¯ ã€Œæ„è¦‹ãŒå‰²ã‚Œã¦ã„ã‚‹ã€ ã¾ãŸã¯ ã€Œé–“é•ãˆãŸ(FP)ã€
        # FNã¯ã€Œè¦‹é€ƒã—ã€ãªã®ã§å°‘ã—æ€§è³ªãŒé•ã†ãŒã€ä»Šå›ã¯å«ã‚ã‚‹
        df_res['is_hard'] = df_res['is_uncertain'] | df_res['is_disagreement'] | df_res['is_fp'] | df_res['is_fn']

        n_hard = df_res['is_hard'].sum()
        print(f"   Hard Samples Identified: {n_hard:,} / {len(df_res):,} ({n_hard/len(df_res):.1%})")
        
        # åˆ†æ: Hard Sampleã«ç‰¹å¾´çš„ãªã‚«ãƒ†ã‚´ãƒªã¯ä½•ã‹ï¼Ÿ
        self._analyze_categorical_bias(df_res, 'is_hard', "Hard Sample")
        self._analyze_categorical_bias(df_res, 'is_fp', "False Positive (èª¤æ¤œçŸ¥)")
        self._analyze_categorical_bias(df_res, 'is_disagreement', "Disagreement (æ„è¦‹å‰²ã‚Œ)")

    def _analyze_categorical_bias(self, df, flag_col, title):
        """ç‰¹å®šã®ãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã«åã£ã¦ã„ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’æ¢ã™"""
        print(f"\nğŸ“Š --- {title} ã®ç‰¹å¾´åˆ†æ ---")
        target_cols = ['æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ', 'äº‹æ•…é¡å‹']
        
        report_lines = []
        report_lines.append(f"### {title} ã®ç‰¹å¾´çš„ãƒ‘ã‚¿ãƒ¼ãƒ³\n")
        
        for col in target_cols:
            if col not in df.columns: continue
            
            # å…¨ä½“åˆ†å¸ƒ
            overall_dist = df[col].value_counts(normalize=True)
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ
            target_dist = df[df[flag_col] == True][col].value_counts(normalize=True)
            
            # å·®åˆ†ãŒå¤§ãã„ã‚«ãƒ†ã‚´ãƒªã‚’æ¢ã™
            diff = target_dist - overall_dist
            
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢ (å·®åˆ†ã®çµ¶å¯¾å€¤ã®åˆè¨ˆ)
            importance = diff.abs().sum()
            
            if importance > 0.05: # ã‚ã‚‹ç¨‹åº¦å·®ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤º
                print(f"   category: {col}")
                report_lines.append(f"#### {col}")
                # ç‰¹å¾´çš„ãªå€¤ãƒˆãƒƒãƒ—3
                top_diffs = diff.abs().sort_values(ascending=False).head(3)
                for val in top_diffs.index:
                    d = diff[val]
                    if abs(d) > 0.02: # 2%ä»¥ä¸Šã®ä¹–é›¢
                        direction = "å¤šã„ (Over-represented)" if d > 0 else "å°‘ãªã„ (Under-represented)"
                        msg = f"      - **{val}**: {target_dist.get(val, 0):.1%} (å…¨ä½“æ¯” {d:+.1%}) -> {direction}"
                        print(msg)
                        report_lines.append(msg)
                report_lines.append("")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        file_name = f"hard_sample_analysis_{flag_col}.md"
        with open(os.path.join(self.output_dir, file_name), 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))

if __name__ == "__main__":
    analyzer = HardSampleAnalyzer()
    analyzer.load_data_and_predictions()
    analyzer.analyze_hard_samples()

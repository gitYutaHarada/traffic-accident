import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.metrics import (
    classification_report, 
    accuracy_score, 
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š (Windowså‘ã‘)
mpl.rcParams['font.family'] = 'MS Gothic'

def main():
    """
    ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ»è©•ä¾¡ã™ã‚‹
    æ³¨æ„: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«å¯¾ã—ã¦ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†
    """
    
    print("=" * 80)
    print("ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†æï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    print("=" * 80)
    
    # å…ƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
    file_path = 'data/raw/honhyo_all_shishasuu_binary.csv'
    print(f"\nğŸ“‚ å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    
    try:
        df = pd.read_csv(file_path)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} ä»¶")
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - {file_path}")
        return
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ç›®çš„å¤‰æ•°ã¨ä¸è¦ãªåˆ—ã®å®šç¾©
    target_col = 'æ­»è€…æ•°'
    drop_cols = ['è³‡æ–™åŒºåˆ†', 'æœ¬ç¥¨ç•ªå·']
    
    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
    print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­...")
    df = df.drop(columns=drop_cols, errors='ignore')
    
    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã«åˆ†é›¢
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # æ¬ æå€¤ã®å‡¦ç†
    print("  ãƒ»æ¬ æå€¤ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™...")
    num_cols = X.select_dtypes(include=[np.number]).columns
    X[num_cols] = X[num_cols].fillna(X[num_cols].median())
    
    cat_cols = X.select_dtypes(include=['object']).columns
    for col in cat_cols:
        X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown')
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    print("  ãƒ»ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã„ã¾ã™...")
    le = LabelEncoder()
    for col in cat_cols:
        X[col] = le.fit_transform(X[col].astype(str))
    
    # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰² (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: 80%, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 20%)
    # stratify=y ã‚’æŒ‡å®šã—ã¦ã€åˆ†å‰²å¾Œã®ã‚¯ãƒ©ã‚¹æ¯”ç‡ã‚’ç¶­æŒã™ã‚‹
    print("\nğŸ”€ ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ä¸­...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train):,} ä»¶")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test):,} ä»¶")
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\nğŸ”„ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦ä¸€æ™‚çš„ãªDataFrameã‚’ä½œæˆ
    train_df = pd.concat([X_train, y_train], axis=1)
    
    # ã‚¯ãƒ©ã‚¹ã”ã¨ã«åˆ†é›¢
    train_majority = train_df[train_df[target_col] == 0]
    train_minority = train_df[train_df[target_col] == 1]
    
    print(f"  ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‰ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰:")
    print(f"    å¤šæ•°æ´¾ï¼ˆ0ï¼‰: {len(train_majority):,} ä»¶")
    print(f"    å°‘æ•°æ´¾ï¼ˆ1ï¼‰: {len(train_minority):,} ä»¶")
    
    # å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    train_minority_upsampled = resample(
        train_minority,
        replace=True,
        n_samples=len(train_majority), # å¤šæ•°æ´¾ã¨åŒæ•°ã«
        random_state=42
    )
    
    # ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    train_upsampled = pd.concat([train_majority, train_minority_upsampled])
    
    # X_train, y_train ã‚’æ›´æ–°
    X_train_res = train_upsampled.drop(columns=[target_col])
    y_train_res = train_upsampled[target_col]
    
    print(f"  ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰: {len(X_train_res):,} ä»¶")
    print(f"    å¤šæ•°æ´¾ï¼ˆ0ï¼‰: {sum(y_train_res==0):,} ä»¶")
    print(f"    å°‘æ•°æ´¾ï¼ˆ1ï¼‰: {sum(y_train_res==1):,} ä»¶")
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’
    print("\nğŸŒ² ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
    rf = RandomForestClassifier(
        n_estimators=100, 
        random_state=42, 
        n_jobs=-1,
        verbose=0
    )
    rf.fit(X_train_res, y_train_res)
    
    print("âœ“ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")
    
    # äºˆæ¸¬ã¨è©•ä¾¡ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯å…ƒã®åˆ†å¸ƒã®ã¾ã¾è©•ä¾¡ï¼‰
    print("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...")
    y_pred = rf.predict(X_test)
    
    # è©•ä¾¡æŒ‡æ¨™ã®ç®—å‡º
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')
    
    print("\n" + "=" * 80)
    print("ğŸ“Š è©•ä¾¡çµæœ (ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿)")
    print("=" * 80)
    print(f"Accuracy (ç²¾åº¦):    {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision (é©åˆç‡): {precision:.4f} ({precision*100:.2f}%)")
    print(f"Recall (å†ç¾ç‡):    {recall:.4f} ({recall*100:.2f}%)")
    print(f"F1 Score:           {f1:.4f}")
    
    print("\n" + "-" * 80)
    print("è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print("-" * 80)
    print(classification_report(y_test, y_pred, target_names=['éæ­»äº¡äº‹æ•…', 'æ­»äº¡äº‹æ•…']))
    
    # æ··åŒè¡Œåˆ—ã®ä½œæˆã¨ä¿å­˜
    print("\nğŸ“‰ æ··åŒè¡Œåˆ—ã‚’ä½œæˆä¸­...")
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=['éæ­»äº¡äº‹æ•… (0)', 'æ­»äº¡äº‹æ•… (1)'],
        yticklabels=['éæ­»äº¡äº‹æ•… (0)', 'æ­»äº¡äº‹æ•… (1)'],
        cbar_kws={'label': 'ä»¶æ•°'}
    )
    plt.title('æ··åŒè¡Œåˆ— (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)', fontsize=16, pad=20)
    plt.ylabel('å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹', fontsize=12)
    plt.xlabel('äºˆæ¸¬ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹', fontsize=12)
    plt.tight_layout()
    
    cm_path = 'results/visualizations/confusion_matrix_upsampled.png'
    os.makedirs(os.path.dirname(cm_path), exist_ok=True)
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ æ··åŒè¡Œåˆ—ã‚’ä¿å­˜: {cm_path}")
    plt.close()
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®è¡¨ç¤ºã¨ä¿å­˜
    print("\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æä¸­...")
    feature_importances = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nç‰¹å¾´é‡é‡è¦åº¦ (Top 20):")
    print("-" * 80)
    for idx, row in feature_importances.head(20).iterrows():
        print(f"{row['feature']:45s}: {row['importance']:.6f}")
    
    # é‡è¦åº¦ã®å¯è¦–åŒ–
    plt.figure(figsize=(12, 10))
    sns.barplot(
        x='importance', 
        y='feature', 
        data=feature_importances.head(20),
        palette='viridis'
    )
    plt.title('ç‰¹å¾´é‡é‡è¦åº¦ Top 20', fontsize=16, pad=20)
    plt.xlabel('é‡è¦åº¦', fontsize=12)
    plt.ylabel('ç‰¹å¾´é‡', fontsize=12)
    plt.tight_layout()
    
    fi_path = 'results/visualizations/feature_importance_upsampled.png'
    plt.savefig(fi_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {fi_path}")
    plt.close()
    
    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’CSVã«ä¿å­˜
    metrics_df = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
        'Value': [accuracy, precision, recall, f1]
    })
    
    metrics_path = 'results/analysis/upsampled_model_metrics.csv'
    metrics_df.to_csv(metrics_path, index=False, encoding='utf-8-sig')
    print(f"âœ“ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜: {metrics_path}")
    
    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œäº†")
    print("=" * 80)

if __name__ == "__main__":
    main()

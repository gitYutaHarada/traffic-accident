import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
# è­¦å‘ŠæŠ‘åˆ¶ï¼ˆLightGBMã®categoryå‹å¯¾å¿œã§è­¦å‘ŠãŒå‡ºã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ï¼‰
import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    precision_recall_curve,
    roc_auc_score
)
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š (Windowså‘ã‘)
mpl.rcParams['font.family'] = 'MS Gothic'

def main():
    """
    scale_pos_weightï¼ˆé‡ã¿ä»˜ã‘ï¼‰ã‚’ä½¿ç”¨ã—ã€Recallæ”¹å–„ã‚’ç›®æŒ‡ã™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    SMOTEã¯ä½¿ç”¨ã›ãšã€ç´”ç²‹ãªé‡ã¿ä»˜ã‘ã®åŠ¹æœã‚’æ¤œè¨¼ã™ã‚‹
    """
    
    print("=" * 80)
    print("ãƒ¢ãƒ‡ãƒ«æ”¹å–„å®Ÿé¨“: LightGBM + scale_pos_weight (é‡ã¿ä»˜ã‘)")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå‰å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
    file_path = 'data/processed/honhyo_model_ready.csv'
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    
    try:
        df = pd.read_csv(file_path)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} ä»¶")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # ç›®çš„å¤‰æ•°
    target_col = 'æ­»è€…æ•°'
    
    # é™¤å¤–ã™ã‚‹åˆ—ï¼ˆäº‹å¾Œæƒ…å ±ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åŸå› ã‚’å¾¹åº•æ’é™¤ï¼‰
    drop_cols = [
        'è³‡æ–™åŒºåˆ†', 'æœ¬ç¥¨ç•ªå·',
        'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',
        'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',
        'è² å‚·è€…æ•°',
        'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Aï¼‰', 'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Bï¼‰',
        'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰', 'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
        'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰', 'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
        'äº‹æ•…å†…å®¹'  # ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯åŸå› 
    ]
    
    print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­ï¼ˆäº‹å¾Œæƒ…å ±ã®é™¤å¤–ï¼‰...")
    df_clean = df.drop(columns=drop_cols, errors='ignore')
    
    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°
    X = df_clean.drop(columns=[target_col])
    y = df_clean[target_col]
    
    # æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†
    # æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†
    # LightGBMã¯æ¬ æå€¤ã‚’ãã®ã¾ã¾æ‰±ãˆã‚‹ãŸã‚ã€åŸ‹ã‚ãšã«ãã®ã¾ã¾ã«ã™ã‚‹ (NaNç¶­æŒ)
    print("\nâš ï¸ æ¬ æå€¤ã®ç©´åŸ‹ã‚(Imputation)ã¯è¡Œã‚ãšã€NaNã¨ã—ã¦æ‰±ã„ã¾ã™")

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã¨ã—ã¦æ‰±ã†åˆ—ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
    categorical_candidates = [
        'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'è·¯ç·šã‚³ãƒ¼ãƒ‰', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰',
        'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
        'ä¸€æ™‚åœæ­¢è¦åˆ¶ æ¨™è­˜', 'ä¸€æ™‚åœæ­¢è¦åˆ¶ è¡¨ç¤º', 'è»Šé“å¹…å“¡', 'é“è·¯ç·šå½¢',
        'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
        'äº‹æ•…é¡å‹', 'å¹´é½¢', 'å½“äº‹è€…ç¨®åˆ¥', 'ç”¨é€”åˆ¥', 'è»Šä¸¡å½¢çŠ¶',
        'ã‚ªãƒ¼ãƒˆãƒãƒãƒƒã‚¯è»Š', 'ã‚µãƒã‚«ãƒ¼', 'é€Ÿåº¦è¦åˆ¶ï¼ˆæŒ‡å®šã®ã¿ï¼‰',
        'æ›œæ—¥', 'ç¥æ—¥', 'ç™ºç”Ÿæœˆ', 'ç™ºç”Ÿæ™‚', 'ç™ºç”Ÿå¹´', 'Area_Cluster_ID'
    ]
    
    # å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
    explicit_cat_cols = [c for c in categorical_candidates if c in X.columns]
    
    # æ–‡å­—åˆ—å‹ã®åˆ—ã‚‚ã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦æ‰±ã†
    object_cols = X.select_dtypes(include=['object']).columns.tolist()
    
    # çµ±åˆã—ãŸã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ãƒªã‚¹ãƒˆ
    final_cat_cols = list(set(explicit_cat_cols + object_cols))
    
    print(f"\nğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®å¤‰æ›: {len(final_cat_cols)} ã‚«ãƒ©ãƒ ")
    
    for col in final_cat_cols:
        # categoryå‹ã«å¤‰æ› (NaNã¯NaNã¨ã—ã¦ç¶­æŒã•ã‚Œã‚‹)
        X[col] = X[col].astype('category')

    # LabelEncoderã¯ä¸è¦ã«ãªã£ãŸãŸã‚å‰Šé™¤
    # LightGBMã¯ category å‹ã‚’ç›´æ¥æ‰±ãˆã‚‹
        
    print(f"âœ“ å‰å‡¦ç†å®Œäº† - ç‰¹å¾´é‡æ•°: {X.shape[1]}")
    
    # ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡æ¯”ã‚’è¨ˆç®—ã—ã€scale_pos_weightã«è¨­å®š
    # scale_pos_weight = (negative samples) / (positive samples)
    pos_count = y.sum()
    neg_count = len(y) - pos_count
    scale_pos_weight = neg_count / pos_count
    
    print(f"\nâš–ï¸ ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”ã®è¨ˆç®—:")
    print(f"  Negative (0): {neg_count:,}")
    print(f"  Positive (1): {pos_count:,}")
    print(f"  Calculated scale_pos_weight: {scale_pos_weight:.2f}")
    
    # LightGBMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lgbm_params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'n_estimators': 1000,
        'learning_rate': 0.05,
        'num_leaves': 31,
        'random_state': 42,
        'n_jobs': -1,
        'scale_pos_weight': scale_pos_weight  # â˜…ã“ã“ãŒå¤‰æ›´ç‚¹
    }
    
    # äº¤å·®æ¤œè¨¼ (5-fold)
    k_folds = 5
    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    print(f"\nğŸ”„ {k_folds}-fold äº¤å·®æ¤œè¨¼ã‚’é–‹å§‹ (Weighted)...")
    
    fold_metrics = []
    
    # å…¨ä½“ã®äºˆæ¸¬çµæœã‚’æ ¼ç´ã™ã‚‹é…åˆ—
    y_true_all = []
    y_prob_all = []
    feature_importances = pd.DataFrame()
    
    for i, (train_index, val_index) in enumerate(skf.split(X, y)):
        print(f"\n--- Fold {i+1}/{k_folds} ---")
        
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆPipelineä¸è¦ã€ç›´æ¥LGBMClassifierï¼‰
        model = lgb.LGBMClassifier(**lgbm_params)
        
        # å­¦ç¿’
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬ï¼ˆç¢ºç‡ï¼‰
        y_prob = model.predict_proba(X_val)[:, 1]
        
        # å…¨ä½“ã®çµæœã«è“„ç©
        y_true_all.extend(y_val)
        y_prob_all.extend(y_prob)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤(0.5)ã§ã®è©•ä¾¡
        y_pred_default = (y_prob >= 0.5).astype(int)
        
        acc = accuracy_score(y_val, y_pred_default)
        prec = precision_score(y_val, y_pred_default, average='binary', zero_division=0)
        rec = recall_score(y_val, y_pred_default, average='binary')
        f1 = f1_score(y_val, y_pred_default, average='binary')
        
        print(f"  [Threshold 0.5] Acc: {acc:.4f}, Prec: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}")
        
        fold_metrics.append({
            'Fold': i+1,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1 Score': f1
        })

        # ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—
        fi = pd.DataFrame()
        fi['feature'] = X.columns
        fi['importance'] = model.feature_importances_
        fi['fold'] = i + 1
        feature_importances = pd.concat([feature_importances, fi], axis=0)

    # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    y_true_all = np.array(y_true_all)
    y_prob_all = np.array(y_prob_all)
    
    # AUCã®è¨ˆç®—
    auc_score = roc_auc_score(y_true_all, y_prob_all)
    print(f"\nğŸ“ˆ AUC Score: {auc_score:.4f}")
    
    with open('results/analysis/weighted_auc_score.txt', 'w') as f:
        f.write(str(auc_score))

    # PRæ›²ç·šã¨æœ€é©é–¾å€¤ã®æ¢ç´¢
    precisions, recalls, thresholds = precision_recall_curve(y_true_all, y_prob_all)
    
    # F1ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã«ãªã‚‹é–¾å€¤
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€é©é–¾å€¤ã®æ¢ç´¢çµæœ")
    print("=" * 80)
    print(f"Best Threshold (Max F1): {best_threshold:.4f}")
    print(f"Max F1 Score: {best_f1:.4f}")
    print(f"Precision at Best: {precisions[best_idx]:.4f}")
    print(f"Recall at Best: {recalls[best_idx]:.4f}")
    
    # Recallé‡è¦–ã®é–¾å€¤è¨­å®šï¼ˆä¾‹: Recall >= 0.8 ã‚’æº€ãŸã™ä¸­ã§æœ€å¤§ã®Precisionï¼‰
    # é‡ã¿ä»˜ã‘ãƒ¢ãƒ‡ãƒ«ãªã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚‚Recallã¯é«˜ããªã‚‹ã¯ãšã ãŒã€ã•ã‚‰ã«æ¢ç´¢ã™ã‚‹
    target_recall = 0.8
    valid_indices = np.where(recalls >= target_recall)[0]
    if len(valid_indices) > 0:
        best_prec_idx = valid_indices[np.argmax(precisions[valid_indices])]
        recall_threshold = thresholds[best_prec_idx] if best_prec_idx < len(thresholds) else thresholds[-1]
        
        print(f"\n[Recallé‡è¦–è¨­å®š (Target >= {target_recall})]")
        print(f"Threshold: {recall_threshold:.4f}")
        print(f"Precision: {precisions[best_prec_idx]:.4f}")
        print(f"Recall: {recalls[best_prec_idx]:.4f}")
    
    # PRæ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 6))
    plt.plot(recalls, precisions, marker='.', label='LightGBM + Weighted')
    plt.xlabel('Recall (å†ç¾ç‡)')
    plt.ylabel('Precision (é©åˆç‡)')
    plt.title('Precision-Recall Curve (Weighted Model)')
    plt.legend()
    plt.grid(True)
    
    pr_path = 'results/visualizations/pr_curve_weighted.png'
    plt.savefig(pr_path)
    print(f"\nâœ“ PRæ›²ç·šã‚’ä¿å­˜: {pr_path}")
    
    # æ··åŒè¡Œåˆ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ 0.5 ã§ã®è©•ä¾¡ãŒé‡è¦ï¼‰
    # é‡ã¿ä»˜ã‘ã‚’è¡Œã£ãŸå ´åˆã€é–¾å€¤0.5ã§ã‚‚RecallãŒé«˜ããªã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹
    y_pred_05 = (y_prob_all >= 0.5).astype(int)
    cm = confusion_matrix(y_true_all, y_pred_05)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',
                xticklabels=['éæ­»äº¡', 'æ­»äº¡'], yticklabels=['éæ­»äº¡', 'æ­»äº¡'])
    plt.title(f'Confusion Matrix (Weighted, Threshold=0.5)')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    
    cm_path = 'results/visualizations/confusion_matrix_weighted.png'
    plt.savefig(cm_path)
    print(f"âœ“ æ··åŒè¡Œåˆ—ã‚’ä¿å­˜: {cm_path}")
    
    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜
    metrics_df = pd.DataFrame(fold_metrics)
    metrics_df.to_csv('results/analysis/weighted_model_metrics.csv', index=False)
    print("âœ“ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜: results/analysis/weighted_model_metrics.csv")
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã®é›†è¨ˆã¨ä¿å­˜
    feat_imp_mean = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)
    feat_imp_mean.to_csv('results/analysis/feature_importance.csv')
    print("âœ“ ç‰¹å¾´é‡é‡è¦åº¦ã‚’ä¿å­˜: results/analysis/feature_importance.csv")

    # ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–ï¼ˆTop 20ï¼‰
    plt.figure(figsize=(10, 8))
    sns.barplot(x=feat_imp_mean.head(20).values, y=feat_imp_mean.head(20).index, palette='viridis')
    plt.title('LightGBM Feature Importance (Top 20)')
    plt.xlabel('Importance (Split)')
    plt.tight_layout()
    plt.savefig('results/visualizations/feature_importance.png')
    print("âœ“ ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: results/visualizations/feature_importance.png")
    
    print("\nâœ… å®Ÿé¨“å®Œäº†")

if __name__ == "__main__":
    main()

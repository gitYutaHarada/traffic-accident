"""
Two-Stage Spatio-Temporal 4-Model Ensemble Training Script
==========================================================
Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° + æ™‚ç©ºé–“ç‰¹å¾´é‡ã‚’çµ„ã¿åˆã‚ã›ãŸæœ€å¼·ã®ãƒ¢ãƒ‡ãƒ«ã€‚

ç‰¹å¾´:
- Stage 1 OOFäºˆæ¸¬ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé«˜é›£æ˜“åº¦ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ï¼‰
- æ™‚ç©ºé–“ç‰¹å¾´é‡ï¼ˆGeohashå±¥æ­´ + æ™‚é–“ã‚µã‚¤ã‚¯ãƒ«ï¼‰
- LightGBM, CatBoost, MLP, TabNet ã®4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
- å …ç‰¢ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆFoldå˜ä½ãƒ»ãƒ¢ãƒ‡ãƒ«å˜ä½ã§å†é–‹å¯èƒ½ï¼‰
- Intel Core Ultra 9 285K / 64GB RAM æœ€å¤§æ´»ç”¨

ä½¿ç”¨æ³•:
    python scripts/modeling/train_stage2_4models_spatiotemporal_twostage.py
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹
    python scripts/modeling/train_stage2_4models_spatiotemporal_twostage.py --resume
    
    # å¼·åˆ¶çš„ã«æœ€åˆã‹ã‚‰å†å­¦ç¿’
    python scripts/modeling/train_stage2_4models_spatiotemporal_twostage.py --force-retrain
"""

import os
import sys
import json
import argparse
import warnings
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score,
    precision_recall_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.optimize import minimize
import joblib
import gc

warnings.filterwarnings('ignore')

# ========================================
# ç’°å¢ƒæœ€é©åŒ–è¨­å®š (Intel Core Ultra 9 285K)
# ========================================
# Arrow Lake: 8 P-cores + 16 E-cores = 24 cores / 24 threads
# P-coreã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§åŒ–
# E-coreã¯OSã‚„ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã«ä»»ã›ã‚‹
N_CORES = 24  # Intel Core Ultra 9 285K (å…¨ã‚³ã‚¢)
N_JOBS = 8    # P-coreã®ã¿ä½¿ç”¨ï¼ˆE-coreã¸ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²æ­¢ï¼‰

# ã‚¹ãƒ¬ãƒƒãƒ‰è¨­å®šã‚’æœ€åˆã«è¡Œã†ï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰ã«å¿…é ˆï¼‰
os.environ['OMP_NUM_THREADS'] = str(N_JOBS)
os.environ['MKL_NUM_THREADS'] = str(N_JOBS)
os.environ['OPENBLAS_NUM_THREADS'] = str(N_JOBS)
os.environ['VECLIB_MAXIMUM_THREADS'] = str(N_JOBS)
os.environ['NUMEXPR_NUM_THREADS'] = str(N_JOBS)
os.environ['NUMBA_NUM_THREADS'] = str(N_JOBS)

# PyTorchè¨­å®š
import torch
torch.set_num_threads(N_JOBS)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ========================================
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)
# ========================================
try:
    import lightgbm as lgb
    LGBM_AVAILABLE = True
except ImportError:
    LGBM_AVAILABLE = False
    print("âš ï¸ LightGBM not available")

try:
    from catboost import CatBoostClassifier, Pool
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("âš ï¸ CatBoost not available")

try:
    from pytorch_tabnet.tab_model import TabNetClassifier
    TABNET_AVAILABLE = True
except ImportError:
    TABNET_AVAILABLE = False
    print("âš ï¸ TabNet not available")

print(f"ğŸš€ Device: {DEVICE}")
print(f"ğŸ§µ Threads: {N_JOBS} (P-cores only)")
print(f"ğŸ’¾ Available RAM: 64GB")


class TwoStageSpatioTemporalEnsemble:
    """Two-Stageæ§‹æˆ + æ™‚ç©ºé–“ç‰¹å¾´é‡ã‚’ç”¨ã„ãŸ4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    
    def __init__(
        self,
        spatio_temporal_dir: str = "data/spatio_temporal",
        stage1_oof_path: str = "data/processed/stage1_oof_predictions.csv",
        stage1_test_path: str = "data/processed/stage1_test_predictions.csv",
        output_dir: str = "results/twostage_spatiotemporal_ensemble",
        stage1_recall_target: float = 0.98,
        stage1_weights: Tuple[float, float] = (0.85, 0.15),  # (catboost, lgbm)
        n_folds: int = 5,
        random_state: int = RANDOM_SEED,
        force_retrain: bool = False,
    ):
        self.spatio_temporal_dir = Path(spatio_temporal_dir)
        self.stage1_oof_path = Path(stage1_oof_path)
        self.stage1_test_path = Path(stage1_test_path)
        self.output_dir = Path(output_dir)
        self.checkpoint_dir = self.output_dir / "checkpoints"
        
        self.stage1_recall_target = stage1_recall_target
        self.stage1_weights = stage1_weights
        self.n_folds = n_folds
        self.random_state = random_state
        self.force_retrain = force_retrain
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´
        self.train_df = None
        self.val_df = None
        self.test_df = None
        self.feature_cols = None
        self.cat_cols = None
        self.num_cols = None
        self.target_col = "fatal"
        
        # Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨
        self.stage1_threshold = None
        self.train_mask = None
        self.test_mask = None
        
        # äºˆæ¸¬æ ¼ç´
        self.oof_predictions = {}
        self.test_predictions = {}
        self.model_aucs = {}
        
        print("=" * 70)
        print("ğŸš€ Two-Stage Spatio-Temporal 4-Model Ensemble")
        print(f"   Spatio-Temporal Data: {self.spatio_temporal_dir}")
        print(f"   Stage 1 OOF: {self.stage1_oof_path}")
        print(f"   Output: {self.output_dir}")
        print(f"   Checkpoints: {self.checkpoint_dir}")
        print(f"   Stage 1 Recall Target: {stage1_recall_target}")
        print(f"   Folds: {n_folds}, Seed: {random_state}")
        print("=" * 70)
    
    # ========================================
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ï¼ˆå …ç‰¢ç‰ˆï¼‰
    # ========================================
    def _ckpt_path(self, name: str) -> Path:
        return self.checkpoint_dir / f"{name}.npy"
    
    def _model_ckpt_path(self, model_name: str, fold: int) -> Path:
        return self.checkpoint_dir / f"{model_name}_fold{fold}.joblib"
    
    def _state_path(self) -> Path:
        return self.checkpoint_dir / "training_state.json"
    
    def _save_state(self, state: Dict):
        """å­¦ç¿’çŠ¶æ…‹ã®ä¿å­˜"""
        with open(self._state_path(), 'w') as f:
            json.dump(state, f, indent=2)
    
    def _load_state(self) -> Optional[Dict]:
        """å­¦ç¿’çŠ¶æ…‹ã®èª­ã¿è¾¼ã¿"""
        if self.force_retrain:
            return None
        path = self._state_path()
        if path.exists():
            with open(path, 'r') as f:
                return json.load(f)
        return None
    
    def _load_oof_checkpoint(self, model_name: str) -> Optional[np.ndarray]:
        """OOFäºˆæ¸¬ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        if self.force_retrain:
            return None
        path = self._ckpt_path(f"{model_name}_oof")
        if path.exists():
            return np.load(path)
        return None
    
    def _load_test_checkpoint(self, model_name: str) -> Optional[np.ndarray]:
        """ãƒ†ã‚¹ãƒˆäºˆæ¸¬ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        if self.force_retrain:
            return None
        path = self._ckpt_path(f"{model_name}_test")
        if path.exists():
            return np.load(path)
        return None
    
    def _save_checkpoint(self, model_name: str, oof: np.ndarray, test: np.ndarray):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        np.save(self._ckpt_path(f"{model_name}_oof"), oof)
        np.save(self._ckpt_path(f"{model_name}_test"), test)
        print(f"   ğŸ’¾ {model_name} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†")
    
    def _load_fold_checkpoint(self, model_name: str, fold: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """Foldå˜ä½ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        if self.force_retrain:
            return None, None
        oof_path = self.checkpoint_dir / f"{model_name}_fold{fold}_oof.npy"
        test_path = self.checkpoint_dir / f"{model_name}_fold{fold}_test.npy"
        if oof_path.exists() and test_path.exists():
            return np.load(oof_path), np.load(test_path)
        return None, None
    
    def _save_fold_checkpoint(self, model_name: str, fold: int, oof: np.ndarray, test: np.ndarray):
        """Foldå˜ä½ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        np.save(self.checkpoint_dir / f"{model_name}_fold{fold}_oof.npy", oof)
        np.save(self.checkpoint_dir / f"{model_name}_fold{fold}_test.npy", test)
    
    # ========================================
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    # ========================================
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨Stage 1ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        # Spatio-Temporal ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆGBDTç”¨ï¼‰ã‚’èª­ã¿è¾¼ã¿
        train_path = self.spatio_temporal_dir / "raw_train.parquet"
        val_path = self.spatio_temporal_dir / "raw_val.parquet"
        test_path = self.spatio_temporal_dir / "raw_test.parquet"
        
        if not train_path.exists():
            raise FileNotFoundError(
                f"ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_path}\n"
                "å…ˆã« preprocess_spatio_temporal.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )
        
        self.train_df = pd.read_parquet(train_path)
        self.val_df = pd.read_parquet(val_path)
        self.test_df = pd.read_parquet(test_path)
        
        # ã€ID Propagationã€‘å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ
        self.train_df['original_index'] = self.train_df.index
        self.val_df['original_index'] = self.val_df.index
        self.test_df['original_index'] = self.test_df.index
        
        # Train + Val ã‚’å­¦ç¿’ç”¨ã«çµ±åˆï¼ˆignore_index=Trueã§ã‚‚ã€original_indexã‚«ãƒ©ãƒ ã¯ä¿æŒã•ã‚Œã‚‹ï¼‰
        self.full_train_df = pd.concat([self.train_df, self.val_df], ignore_index=True)
        
        print(f"   Train: {len(self.train_df):,} è¡Œ")
        print(f"   Val:   {len(self.val_df):,} è¡Œ")
        print(f"   Test:  {len(self.test_df):,} è¡Œ")
        print(f"   Train+Val (å­¦ç¿’ç”¨): {len(self.full_train_df):,} è¡Œ")
        
        # ç‰¹å¾´é‡åˆ—ã®ç‰¹å®š
        self._identify_columns()
        
        # Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®é©ç”¨
        self._apply_stage1_filtering()
    
    def _identify_columns(self):
        """ç‰¹å¾´é‡åˆ—ã®è­˜åˆ¥"""
        # é™¤å¤–ã™ã‚‹åˆ—
        exclude_cols = [
            self.target_col, 'æ­»è€…æ•°', 'è² å‚·è€…æ•°', 'é‡å‚·è€…æ•°', 'è»½å‚·è€…æ•°',
            'å½“äº‹è€…A_æ­»å‚·çŠ¶æ³', 'å½“äº‹è€…B_æ­»å‚·çŠ¶æ³', 'æœ¬ç¥¨ç•ªå·', 'ç™ºç”Ÿæ—¥æ™‚',
            'lat', 'lon', 'geohash', 'geohash_fine', 'date', 'year',
            'accident_id', 'original_index'
        ]
        
        # åˆ©ç”¨å¯èƒ½ãªåˆ—ã‚’æŠ½å‡º
        available_cols = [c for c in self.full_train_df.columns if c not in exclude_cols]
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—ã¨æ•°å€¤åˆ—ã‚’è­˜åˆ¥
        self.cat_cols = []
        self.num_cols = []
        
        for col in available_cols:
            if self.full_train_df[col].dtype == 'object':
                self.cat_cols.append(col)
            elif self.full_train_df[col].nunique() < 50 and self.full_train_df[col].dtype in ['int64', 'int32']:
                self.cat_cols.append(col)
            else:
                self.num_cols.append(col)
        
        self.feature_cols = self.num_cols + self.cat_cols
        
        print(f"\nğŸ“Š ç‰¹å¾´é‡:")
        print(f"   æ•°å€¤: {len(self.num_cols)} åˆ—")
        print(f"   ã‚«ãƒ†ã‚´ãƒª: {len(self.cat_cols)} åˆ—")
        print(f"   åˆè¨ˆ: {len(self.feature_cols)} åˆ—")
    
    def _apply_stage1_filtering(self):
        """Stage 1 OOFäºˆæ¸¬ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        print("\nğŸ” Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨ä¸­...")
        
        # Stage 1 OOFäºˆæ¸¬ã‚’èª­ã¿è¾¼ã¿
        if not self.stage1_oof_path.exists():
            raise FileNotFoundError(
                f"Stage 1 OOFäºˆæ¸¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.stage1_oof_path}\n"
                "å…ˆã« save_stage1_oof.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )
        
        df_oof = pd.read_csv(self.stage1_oof_path)
        df_test_pred = pd.read_csv(self.stage1_test_path)
        
        # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡
        cat_w, lgb_w = self.stage1_weights
        oof_prob = cat_w * df_oof['prob_catboost'].values + lgb_w * df_oof['prob_lgbm'].values
        test_prob = cat_w * df_test_pred['prob_catboost'].values + lgb_w * df_test_pred['prob_lgbm'].values
        
        # Recall target ã®é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹
        y_train_oof = df_oof['target'].values if 'target' in df_oof.columns else None
        
        if y_train_oof is not None:
            precision, recall, thresholds = precision_recall_curve(y_train_oof, oof_prob)
            valid_idx = np.where(recall[:-1] >= self.stage1_recall_target)[0]
            if len(valid_idx) > 0:
                best_idx = valid_idx[-1]
                self.stage1_threshold = thresholds[best_idx]
            else:
                self.stage1_threshold = 0.0
        else:
            # æ—¢çŸ¥ã®é–¾å€¤ã‚’ä½¿ç”¨
            self.stage1_threshold = 0.0645
        
        print(f"   Stage 1 é–¾å€¤: {self.stage1_threshold:.4f}")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®original_indexã‚’å–å¾—
        train_original_indices = df_oof[oof_prob >= self.stage1_threshold]['original_index'].values
        test_original_indices = df_test_pred[test_prob >= self.stage1_threshold]['original_index'].values
        
        print(f"   Train OOF: {len(oof_prob):,} â†’ {len(train_original_indices):,} (é€šéç‡: {len(train_original_indices)/len(oof_prob)*100:.1f}%)")
        print(f"   Test:      {len(test_prob):,} â†’ {len(test_original_indices):,} (é€šéç‡: {len(test_original_indices)/len(test_prob)*100:.1f}%)")
        
        # ã€Fix #4ã€‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        # Stage 1 OOFä½œæˆæ™‚ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³ãŒåŒä¸€ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        train_idx_set = set(self.full_train_df.index)
        test_idx_set = set(self.test_df.index)
        train_match = len(train_idx_set.intersection(train_original_indices))
        test_match = len(test_idx_set.intersection(test_original_indices))
        print(f"   ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è‡´ç¢ºèª: Train {train_match:,}, Test {test_match:,}")
        
        if train_match == 0:
            print("   âš ï¸ è­¦å‘Š: Trainã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        if test_match == 0:
            print("   âš ï¸ è­¦å‘Š: Testã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        # ã€è¿½åŠ Fixã€‘å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ï¼ˆå¾Œã§å…¨ä»¶å¾©å…ƒã«ä½¿ç”¨ï¼‰
        self.original_test_indices = self.test_df.index.tolist()
        self.filtered_test_indices = test_original_indices.tolist()
        # ã€ID Propagationã€‘ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿Trainã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚ä¿å­˜
        self.filtered_train_indices = train_original_indices.tolist()
        
        # Spatio-Temporal ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # full_train_dfã¨test_dfã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§çµã‚Šè¾¼ã¿
        self.full_train_df = self.full_train_df[self.full_train_df.index.isin(train_original_indices)].reset_index(drop=True)
        self.test_df = self.test_df[self.test_df.index.isin(test_original_indices)].reset_index(drop=True)
        
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿å¾Œ Train: {len(self.full_train_df):,} è¡Œ")
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿å¾Œ Test:  {len(self.test_df):,} è¡Œ")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¢ºèª
        if self.target_col not in self.full_train_df.columns:
            raise ValueError(f"Target column '{self.target_col}' not found in data")
        
        train_fatal = self.full_train_df[self.target_col].sum()
        test_fatal = self.test_df[self.target_col].sum()
        print(f"   Train Fatal: {train_fatal:,} ({train_fatal/len(self.full_train_df)*100:.2f}%)")
        print(f"   Test Fatal:  {test_fatal:,} ({test_fatal/len(self.test_df)*100:.2f}%)")
    
    # ========================================
    # LightGBM
    # ========================================
    def train_lgbm(self) -> Tuple[np.ndarray, np.ndarray]:
        """LightGBMå­¦ç¿’"""
        print("\nğŸ“ˆ lightgbm å­¦ç¿’ä¸­...")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        oof_ckpt = self._load_oof_checkpoint("lgbm")
        test_ckpt = self._load_test_checkpoint("lgbm")
        if oof_ckpt is not None and test_ckpt is not None:
            print("   ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
            self.oof_predictions["lgbm"] = oof_ckpt
            self.test_predictions["lgbm"] = test_ckpt
            auc = roc_auc_score(self.full_train_df[self.target_col].values, oof_ckpt)
            self.model_aucs["lgbm"] = auc
            print(f"   lgbm OOF AUC: {auc:.4f}")
            return oof_ckpt, test_ckpt
        
        X = self.full_train_df[self.feature_cols].copy()
        y = self.full_train_df[self.target_col].values
        X_test = self.test_df[self.feature_cols].copy()
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—ã®å‡¦ç†
        for col in self.cat_cols:
            X[col] = X[col].astype('category')
            X_test[col] = X_test[col].astype('category')
        
        oof_preds = np.zeros(len(X))
        test_preds = np.zeros(len(X_test))
        
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'num_leaves': 63,
            'max_depth': 8,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 100,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'verbose': -1,
            'n_jobs': N_JOBS,
            'seed': self.random_state,
        }
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
            fold_oof, fold_test = self._load_fold_checkpoint("lgbm", fold)
            if fold_oof is not None and fold_test is not None:
                oof_preds[val_idx] = fold_oof
                test_preds += fold_test / self.n_folds
                print(f"      Fold {fold+1} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
                continue
            
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=self.cat_cols)
            val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=self.cat_cols, reference=train_data)
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=2000,
                valid_sets=[val_data],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
            )
            
            val_pred = model.predict(X_val)
            test_pred = model.predict(X_test)
            
            oof_preds[val_idx] = val_pred
            test_preds += test_pred / self.n_folds
            
            fold_auc = roc_auc_score(y_val, val_pred)
            print(f"      Fold {fold+1} AUC: {fold_auc:.4f}")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            self._save_fold_checkpoint("lgbm", fold, val_pred, test_pred)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            joblib.dump(model, self._model_ckpt_path("lgbm", fold))
            
            del model, train_data, val_data
            gc.collect()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        self._save_checkpoint("lgbm", oof_preds, test_preds)
        
        auc = roc_auc_score(y, oof_preds)
        self.oof_predictions["lgbm"] = oof_preds
        self.test_predictions["lgbm"] = test_preds
        self.model_aucs["lgbm"] = auc
        print(f"   lgbm OOF AUC: {auc:.4f}")
        
        return oof_preds, test_preds
    
    # ========================================
    # CatBoost
    # ========================================
    def train_catboost(self) -> Tuple[np.ndarray, np.ndarray]:
        """CatBoostå­¦ç¿’"""
        print("\nğŸ± catboost å­¦ç¿’ä¸­...")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        oof_ckpt = self._load_oof_checkpoint("catboost")
        test_ckpt = self._load_test_checkpoint("catboost")
        if oof_ckpt is not None and test_ckpt is not None:
            print("   ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
            self.oof_predictions["catboost"] = oof_ckpt
            self.test_predictions["catboost"] = test_ckpt
            auc = roc_auc_score(self.full_train_df[self.target_col].values, oof_ckpt)
            self.model_aucs["catboost"] = auc
            print(f"   catboost OOF AUC: {auc:.4f}")
            return oof_ckpt, test_ckpt
        
        X = self.full_train_df[self.feature_cols].copy()
        y = self.full_train_df[self.target_col].values
        X_test = self.test_df[self.feature_cols].copy()
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        cat_features = [self.feature_cols.index(c) for c in self.cat_cols if c in self.feature_cols]
        
        # æ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼ˆCatBoostç”¨ï¼‰
        for col in self.cat_cols:
            X[col] = X[col].astype(str).fillna('missing')
            X_test[col] = X_test[col].astype(str).fillna('missing')
        
        oof_preds = np.zeros(len(X))
        test_preds = np.zeros(len(X_test))
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
            fold_oof, fold_test = self._load_fold_checkpoint("catboost", fold)
            if fold_oof is not None and fold_test is not None:
                oof_preds[val_idx] = fold_oof
                test_preds += fold_test / self.n_folds
                print(f"      Fold {fold+1} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
                continue
            
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            model = CatBoostClassifier(
                iterations=2000,
                learning_rate=0.05,
                depth=8,
                l2_leaf_reg=3,
                loss_function='Logloss',
                eval_metric='AUC',
                cat_features=cat_features,
                random_seed=self.random_state,
                task_type='CPU',  # CPUãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´ï¼ˆGPUã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ³ã‚°ã™ã‚‹å ´åˆã‚ã‚Šï¼‰
                thread_count=N_JOBS,
                early_stopping_rounds=50,
                verbose=100,
            )
            
            model.fit(
                X_train, y_train,
                eval_set=(X_val, y_val),
                use_best_model=True,
            )
            
            val_pred = model.predict_proba(X_val)[:, 1]
            test_pred = model.predict_proba(X_test)[:, 1]
            
            oof_preds[val_idx] = val_pred
            test_preds += test_pred / self.n_folds
            
            fold_auc = roc_auc_score(y_val, val_pred)
            print(f"      Fold {fold+1} AUC: {fold_auc:.4f}")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            self._save_fold_checkpoint("catboost", fold, val_pred, test_pred)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model.save_model(str(self._model_ckpt_path("catboost", fold)))
            
            del model
            gc.collect()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        self._save_checkpoint("catboost", oof_preds, test_preds)
        
        auc = roc_auc_score(y, oof_preds)
        self.oof_predictions["catboost"] = oof_preds
        self.test_predictions["catboost"] = test_preds
        self.model_aucs["catboost"] = auc
        print(f"   catboost OOF AUC: {auc:.4f}")
        
        return oof_preds, test_preds
    
    # ========================================
    # MLP (PyTorch)
    # ========================================
    def train_mlp(self) -> Tuple[np.ndarray, np.ndarray]:
        """MLPå­¦ç¿’ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å¯¾ç­–æ¸ˆã¿ï¼‰"""
        print("\nğŸ§  mlp å­¦ç¿’ä¸­...")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        oof_ckpt = self._load_oof_checkpoint("mlp")
        test_ckpt = self._load_test_checkpoint("mlp")
        if oof_ckpt is not None and test_ckpt is not None:
            print("   ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
            self.oof_predictions["mlp"] = oof_ckpt
            self.test_predictions["mlp"] = test_ckpt
            auc = roc_auc_score(self.full_train_df[self.target_col].values, oof_ckpt)
            self.model_aucs["mlp"] = auc
            print(f"   mlp OOF AUC: {auc:.4f}")
            return oof_ckpt, test_ckpt
        
        X = self.full_train_df[self.feature_cols].copy()
        y = self.full_train_df[self.target_col].values
        X_test = self.test_df[self.feature_cols].copy()
        
        oof_preds = np.zeros(len(X))
        test_preds = np.zeros(len(X_test))
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
            fold_oof, fold_test = self._load_fold_checkpoint("mlp", fold)
            if fold_oof is not None and fold_test is not None:
                oof_preds[val_idx] = fold_oof
                test_preds += fold_test / self.n_folds
                print(f"      Fold {fold+1} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
                continue
            
            X_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()
            y_train, y_val = y[train_idx], y[val_idx]
            X_test_fold = X_test.copy()
            
            # ===ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å¯¾ç­–ã€‘Foldå†…ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§fit===
            from sklearn.impute import SimpleImputer
            
            scaler = StandardScaler()
            imputer = SimpleImputer(strategy='mean')
            label_encoders = {}
            
            # æ•°å€¤åˆ—ã®æ¬ æåŸ‹ã‚ (å¹³å‡å€¤) & ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            X_train_num = scaler.fit_transform(imputer.fit_transform(X_train[self.num_cols]))
            X_val_num = scaler.transform(imputer.transform(X_val[self.num_cols]))
            X_test_num = scaler.transform(imputer.transform(X_test_fold[self.num_cols]))
            
            # ã€è¿½åŠ Fix #4ã€‘ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆè¾æ›¸ãƒãƒƒãƒ—ã§é«˜é€ŸåŒ–ãƒ»å®‰å…¨æ€§å‘ä¸Šï¼‰
            X_train_cat = np.zeros((len(X_train), len(self.cat_cols)), dtype=np.int64)
            X_val_cat = np.zeros((len(X_val), len(self.cat_cols)), dtype=np.int64)
            X_test_cat = np.zeros((len(X_test_fold), len(self.cat_cols)), dtype=np.int64)
            
            cat_mappers = {}
            for i, col in enumerate(self.cat_cols):
                train_vals = X_train[col].astype(str).fillna('missing')
                # è¾æ›¸ãƒãƒƒãƒ—ä½œæˆï¼ˆ+1ã‚ªãƒ•ã‚»ãƒƒãƒˆ: 0=æœªçŸ¥, 1ä»¥ä¸Š=æ—¢çŸ¥ï¼‰
                unique_vals = sorted(set(train_vals))
                mapper = {v: idx + 1 for idx, v in enumerate(unique_vals)}
                cat_mappers[col] = mapper
                
                # é«˜é€Ÿå¤‰æ›: mapé–¢æ•°ã§ä¸€æ‹¬å¤‰æ›ï¼ˆå­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã¯NaNâ†’fillna(0)ï¼‰
                X_train_cat[:, i] = train_vals.map(mapper).fillna(0).astype(np.int64).values
                
                val_vals = X_val[col].astype(str).fillna('missing')
                test_vals = X_test_fold[col].astype(str).fillna('missing')
                
                X_val_cat[:, i] = val_vals.map(mapper).fillna(0).astype(np.int64).values
                X_test_cat[:, i] = test_vals.map(mapper).fillna(0).astype(np.int64).values
            
            # PyTorchãƒ†ãƒ³ã‚½ãƒ«
            # ã€Fixã€‘Trainç”¨ãƒ†ãƒ³ã‚½ãƒ«ã¯CPUã«ä¿æŒï¼ˆDataLoader + pin_memoryç”¨ï¼‰
            X_train_t = torch.FloatTensor(np.hstack([X_train_num, X_train_cat]))  # CPU
            y_train_t = torch.FloatTensor(y_train)  # CPU
            # Val/Testã¯GPUã«ç›´æ¥é…ç½®
            X_val_t = torch.FloatTensor(np.hstack([X_val_num, X_val_cat])).to(DEVICE)
            X_test_t = torch.FloatTensor(np.hstack([X_test_num, X_test_cat])).to(DEVICE)
            y_val_t = torch.FloatTensor(y_val).to(DEVICE)
            
            # ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆSigmoidå±¤ãªã— - BCEWithLogitsLossä½¿ç”¨ï¼‰
            input_dim = X_train_t.shape[1]
            model = torch.nn.Sequential(
                torch.nn.Linear(input_dim, 512),
                torch.nn.BatchNorm1d(512),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.3),
                torch.nn.Linear(512, 256),
                torch.nn.BatchNorm1d(256),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.2),
                torch.nn.Linear(256, 64),
                torch.nn.BatchNorm1d(64),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.1),
                torch.nn.Linear(64, 1),
                # NO Sigmoid here - BCEWithLogitsLoss applies it internally
            ).to(DEVICE)
            
            # æå¤±é–¢æ•°ã¨æœ€é©åŒ–
            pos_weight = torch.tensor([(len(y_train) - sum(y_train)) / sum(y_train)]).to(DEVICE)
            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
            # ã€Fix #3ã€‘mode='max' ã§AUCã‚’ãã®ã¾ã¾æ¸¡ã™ï¼ˆç›´æ„Ÿçš„ã§å®‰å…¨ï¼‰
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
            # ã€Fixã€‘num_workers=0ã§ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å•é¡Œã‚’å›é¿ã€pin_memory=Trueã§é«˜é€ŸGPUè»¢é€
            train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)
            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0, pin_memory=True)
            
            # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
            best_val_auc = 0
            patience = 10
            no_improve = 0
            
            for epoch in range(100):
                model.train()
                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)  # CPU -> GPU
                    optimizer.zero_grad()
                    outputs = model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                
                # æ¤œè¨¼
                model.eval()
                with torch.no_grad():
                    val_logits = model(X_val_t).squeeze()
                    val_prob = torch.sigmoid(val_logits).cpu().numpy()  # Explicit sigmoid for inference
                    val_auc = roc_auc_score(y_val, val_prob)
                
                scheduler.step(val_auc)  # ã€Fix #3ã€‘AUCã‚’ãã®ã¾ã¾æ¸¡ã™
                
                if val_auc > best_val_auc:
                    best_val_auc = val_auc
                    best_model_state = model.state_dict().copy()
                    no_improve = 0
                else:
                    no_improve += 1
                    if no_improve >= patience:
                        break
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
            model.load_state_dict(best_model_state)
            model.eval()
            with torch.no_grad():
                val_logits = model(X_val_t).squeeze()
                test_logits = model(X_test_t).squeeze()
                val_pred = torch.sigmoid(val_logits).cpu().numpy()  # Explicit sigmoid
                test_pred = torch.sigmoid(test_logits).cpu().numpy()  # Explicit sigmoid
            
            oof_preds[val_idx] = val_pred
            test_preds += test_pred / self.n_folds
            
            fold_auc = roc_auc_score(y_val, val_pred)
            print(f"      Fold {fold+1} AUC: {fold_auc:.4f}")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            self._save_fold_checkpoint("mlp", fold, val_pred, test_pred)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            torch.save(model.state_dict(), self._model_ckpt_path("mlp", fold))
            
            del model, X_train_t, X_val_t, X_test_t
            torch.cuda.empty_cache() if DEVICE == 'cuda' else None
            gc.collect()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        self._save_checkpoint("mlp", oof_preds, test_preds)
        
        auc = roc_auc_score(y, oof_preds)
        self.oof_predictions["mlp"] = oof_preds
        self.test_predictions["mlp"] = test_preds
        self.model_aucs["mlp"] = auc
        print(f"   mlp OOF AUC: {auc:.4f}")
        
        return oof_preds, test_preds
    
    # ========================================
    # TabNet
    # ========================================
    def train_tabnet(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """TabNetå­¦ç¿’ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å¯¾ç­–æ¸ˆã¿ï¼‰"""
        print("\nğŸ“Š tabnet å­¦ç¿’ä¸­...")
        
        # ã€Fix #1ã€‘TabNetæœªåˆ©ç”¨æ™‚ã¯Noneã‚’è¿”ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‹ã‚‰é™¤å¤–
        if not TABNET_AVAILABLE:
            print("   âš ï¸ TabNet not available, skipping...")
            return None, None
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        oof_ckpt = self._load_oof_checkpoint("tabnet")
        test_ckpt = self._load_test_checkpoint("tabnet")
        if oof_ckpt is not None and test_ckpt is not None:
            print("   ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
            self.oof_predictions["tabnet"] = oof_ckpt
            self.test_predictions["tabnet"] = test_ckpt
            auc = roc_auc_score(self.full_train_df[self.target_col].values, oof_ckpt)
            self.model_aucs["tabnet"] = auc
            print(f"   tabnet OOF AUC: {auc:.4f}")
            return oof_ckpt, test_ckpt
        
        X = self.full_train_df[self.feature_cols].copy()
        y = self.full_train_df[self.target_col].values
        X_test = self.test_df[self.feature_cols].copy()
        
        oof_preds = np.zeros(len(X))
        test_preds = np.zeros(len(X_test))
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
            fold_oof, fold_test = self._load_fold_checkpoint("tabnet", fold)
            if fold_oof is not None and fold_test is not None:
                oof_preds[val_idx] = fold_oof
                test_preds += fold_test / self.n_folds
                print(f"      Fold {fold+1} ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
                continue
            
            X_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()
            y_train, y_val = y[train_idx], y[val_idx]
            X_test_fold = X_test.copy()
            
            # ===ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å¯¾ç­–ã€‘Foldå†…ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§fit===
            scaler = StandardScaler()
            label_encoders = {}
            
            # æ•°å€¤åˆ—ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            X_train[self.num_cols] = scaler.fit_transform(X_train[self.num_cols].fillna(0))
            X_val[self.num_cols] = scaler.transform(X_val[self.num_cols].fillna(0))
            X_test_fold[self.num_cols] = scaler.transform(X_test_fold[self.num_cols].fillna(0))
            
            # ã€Fix #2ã€‘ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ+1ã‚ªãƒ•ã‚»ãƒƒãƒˆã§æœªçŸ¥ã‚«ãƒ†ã‚´ãƒª=0ã¨æ—¢å­˜ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é›¢ï¼‰
            # ã€é«˜é€ŸåŒ–ã€‘è¾æ›¸ãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            cat_idxs = []
            cat_dims = []
            
            for i, col in enumerate(self.feature_cols):
                if col in self.cat_cols:
                    # æ–‡å­—åˆ—å¤‰æ›
                    train_vals = X_train[col].astype(str).fillna('missing')
                    val_vals = X_val[col].astype(str).fillna('missing')
                    test_vals = X_test_fold[col].astype(str).fillna('missing')
                    
                    # ãƒãƒƒãƒ‘ãƒ¼ä½œæˆ (1-based index, 0=unknown)
                    unique_vals = sorted(set(train_vals))
                    mapper = {v: idx + 1 for idx, v in enumerate(unique_vals)}
                    
                    # mapã§ä¸€æ‹¬å¤‰æ› (å­˜åœ¨ã—ãªã„ã‚­ãƒ¼ã¯NaNã«ãªã‚‹â†’0ã§åŸ‹ã‚ã‚‹)
                    X_train[col] = train_vals.map(mapper).fillna(0).astype(int)
                    X_val[col] = val_vals.map(mapper).fillna(0).astype(int)
                    X_test_fold[col] = test_vals.map(mapper).fillna(0).astype(int)
                    
                    cat_idxs.append(i)
                    cat_dims.append(len(unique_vals) + 1)  # +1 for unknown category (0)
            
            # NumPyé…åˆ—ã«å¤‰æ›
            X_train_np = X_train[self.feature_cols].values.astype(np.float32)
            X_val_np = X_val[self.feature_cols].values.astype(np.float32)
            X_test_np = X_test_fold[self.feature_cols].values.astype(np.float32)
            
            # TabNet
            model = TabNetClassifier(
                n_d=32,
                n_a=32,
                n_steps=5,
                gamma=1.5,
                lambda_sparse=1e-4,
                cat_idxs=cat_idxs,
                cat_dims=cat_dims,
                cat_emb_dim=8,
                optimizer_fn=torch.optim.Adam,
                optimizer_params=dict(lr=0.02),
                scheduler_fn=torch.optim.lr_scheduler.StepLR,
                scheduler_params=dict(step_size=10, gamma=0.9),
                seed=self.random_state,
                verbose=0,
                device_name=DEVICE,
            )
            
            model.fit(
                X_train_np, y_train,
                eval_set=[(X_val_np, y_val)],
                eval_metric=['auc'],
                max_epochs=100,
                patience=20,
                batch_size=512,  # ç¸®å°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
                virtual_batch_size=128,
            )
            
            val_pred = model.predict_proba(X_val_np)[:, 1]
            test_pred = model.predict_proba(X_test_np)[:, 1]
            
            # ã€å®‰å®šæ€§å¯¾ç­–ã€‘NaNãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if np.isnan(val_pred).any():
                print(f"      âš ï¸ Warning: TabNet produced NaN in Fold {fold+1}. Fallback to mean.")
                val_pred = np.nan_to_num(val_pred, nan=np.mean(y_train))
            if np.isnan(test_pred).any():
                test_pred = np.nan_to_num(test_pred, nan=np.mean(y_train))
            
            oof_preds[val_idx] = val_pred
            test_preds += test_pred / self.n_folds
            
            fold_auc = roc_auc_score(y_val, val_pred)
            print(f"      Fold {fold+1} AUC: {fold_auc:.4f}")
            
            # Foldå˜ä½ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            self._save_fold_checkpoint("tabnet", fold, val_pred, test_pred)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model.save_model(str(self._model_ckpt_path("tabnet", fold)))
            
            del model
            gc.collect()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        self._save_checkpoint("tabnet", oof_preds, test_preds)
        
        auc = roc_auc_score(y, oof_preds)
        # ã€Fix #1ã€‘TabNetãŒæ­£å¸¸ã«å­¦ç¿’ã—ãŸå ´åˆã®ã¿è¾æ›¸ã«è¿½åŠ 
        self.oof_predictions["tabnet"] = oof_preds
        self.test_predictions["tabnet"] = test_preds
        self.model_aucs["tabnet"] = auc
        print(f"   tabnet OOF AUC: {auc:.4f}")
        
        return oof_preds, test_preds
    
    # ========================================
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–
    # ========================================
    def optimize_weights(self) -> Dict[str, float]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã®æœ€é©åŒ–ï¼ˆã€Fix #1, #5ã€‘æ”¹è‰¯ç‰ˆï¼‰"""
        print("\nâš–ï¸ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–ä¸­...")
        
        y = self.full_train_df[self.target_col].values
        
        # ã€Fix #1ã€‘Noneã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’å¤±æ•—ã‚„ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã‚’é™¤å¤–
        # ã€è¿½åŠ Fix #5ã€‘AUCãŒä½ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’å¤±æ•—ï¼‰ã‚‚é™¤å¤–
        valid_models = {}
        for k, v in self.oof_predictions.items():
            if v is None or k == "ensemble":
                continue
            auc = roc_auc_score(y, v)
            if auc < 0.55:
                print(f"   âš ï¸ {k} ã‚’é™¤å¤– (AUCãŒä½ã™ãã‚‹: {auc:.4f})")
                continue
            valid_models[k] = v
        model_names = list(valid_models.keys())
        
        if len(model_names) == 0:
            print("   âš ï¸ æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        
        if len(model_names) < 2:
            print("   ãƒ¢ãƒ‡ãƒ«ãŒ1ã¤ã—ã‹ãªã„ãŸã‚æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            self.oof_predictions["ensemble"] = valid_models[model_names[0]]
            self.test_predictions["ensemble"] = self.test_predictions[model_names[0]]
            return {model_names[0]: 1.0}
        
        oof_matrix = np.column_stack([valid_models[name] for name in model_names])
        
        def objective(weights):
            ensemble_pred = np.dot(oof_matrix, weights)
            return -roc_auc_score(y, ensemble_pred)
        
        init_weights = np.ones(len(model_names)) / len(model_names)
        
        # ã€Fix #5ã€‘SLSQP: åˆ¶ç´„ä»˜ãæœ€é©åŒ–ï¼ˆé‡ã¿ã¯0ã€œ1ã€åˆè¨ˆ1ï¼‰
        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0})
        bounds = [(0.0, 1.0) for _ in range(len(model_names))]
        result = minimize(objective, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)
        
        optimal_weights = result.x
        weights_dict = {name: float(w) for name, w in zip(model_names, optimal_weights)}
        
        print("   æœ€é©åŒ–ã•ã‚ŒãŸé‡ã¿:")
        for name, w in weights_dict.items():
            print(f"     {name}: {w:.4f}")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        ensemble_oof = np.dot(oof_matrix, optimal_weights)
        ensemble_test = np.dot(
            np.column_stack([self.test_predictions[name] for name in model_names]),
            optimal_weights
        )
        
        ensemble_auc = roc_auc_score(y, ensemble_oof)
        print(f"   Ensemble OOF AUC: {ensemble_auc:.4f}")
        
        self.oof_predictions["ensemble"] = ensemble_oof
        self.test_predictions["ensemble"] = ensemble_test
        self.model_aucs["ensemble"] = ensemble_auc
        
        # é‡ã¿ä¿å­˜
        with open(self.output_dir / "ensemble_weights.json", 'w') as f:
            json.dump(weights_dict, f, indent=2)
        
        return weights_dict
    
    # ========================================
    # çµæœä¿å­˜
    # ========================================
    def save_results(self):
        """çµæœã®ä¿å­˜"""
        print("\nğŸ“ˆ çµæœä¿å­˜ä¸­...")
        
        y_train = self.full_train_df[self.target_col].values
        y_test = self.test_df[self.target_col].values
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = []
        for model_name in self.oof_predictions.keys():
            oof_auc = roc_auc_score(y_train, self.oof_predictions[model_name])
            oof_prauc = average_precision_score(y_train, self.oof_predictions[model_name])
            test_auc = roc_auc_score(y_test, self.test_predictions[model_name])
            test_prauc = average_precision_score(y_test, self.test_predictions[model_name])
            
            scores.append({
                'model': model_name,
                'oof_auc': oof_auc,
                'oof_prauc': oof_prauc,
                'test_auc': test_auc,
                'test_prauc': test_prauc,
            })
            
            print(f"   {model_name}: OOF AUC={oof_auc:.4f}, Test AUC={test_auc:.4f}")
        
        # ã‚¹ã‚³ã‚¢ä¿å­˜
        scores_df = pd.DataFrame(scores)
        scores_df.to_csv(self.output_dir / "final_scores.csv", index=False)
        
        # OOFäºˆæ¸¬ä¿å­˜ï¼ˆoriginal_indexã‚’å«ã‚€ï¼‰
        oof_df = pd.DataFrame(self.oof_predictions)
        # ã€ä¿®æ­£ã€‘ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®full_train_dfã‹ã‚‰original_indexã‚’å–å¾—ï¼ˆæ­£ã—ã„é•·ã•ï¼‰
        oof_df['original_index'] = self.full_train_df['original_index'].values
        oof_df['target'] = y_train
        oof_df.to_csv(self.output_dir / "oof_predictions.csv", index=False)
        
        # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ä¿å­˜ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ã¿ã€original_indexã‚’å«ã‚€ï¼‰
        test_df = pd.DataFrame(self.test_predictions)
        # ã€ä¿®æ­£ã€‘ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®test_dfã‹ã‚‰original_indexã‚’å–å¾—ï¼ˆæ­£ã—ã„é•·ã•ï¼‰
        test_df['original_index'] = self.test_df['original_index'].values
        test_df['target'] = y_test
        test_df.to_csv(self.output_dir / "test_predictions.csv", index=False)
        
        # ã€è¿½åŠ Fix #1ã€‘å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¸ã®å¾©å…ƒå‡¦ç†
        print("\n   ğŸ”„ å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¸ã®äºˆæ¸¬å¾©å…ƒä¸­...")
        try:
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            raw_test_df = pd.read_parquet(self.spatio_temporal_dir / "raw_test.parquet")
            
            # å…¨è¡Œã‚’å«ã‚€DataFrameã‚’ä½œæˆ
            final_submission = pd.DataFrame({
                'original_index': raw_test_df.index
            })
            
            # Stage 2 äºˆæ¸¬çµæœï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰ã‚’ãƒãƒ¼ã‚¸ç”¨ã«æº–å‚™
            # ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨
            if hasattr(self, 'filtered_test_indices') and 'ensemble' in self.test_predictions:
                stage2_preds = pd.DataFrame({
                    'original_index': self.filtered_test_indices[:len(self.test_predictions['ensemble'])],
                    'prob_ensemble': self.test_predictions['ensemble']
                })
                
                # å…¨ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ï¼ˆStage 2ã«ãªã„ãƒ‡ãƒ¼ã‚¿ã¯æ¬ æã«ãªã‚‹ï¼‰
                final_submission = final_submission.merge(stage2_preds, on='original_index', how='left')
                
                # Stage 2ã«å«ã¾ã‚Œãªã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆEasy sampleï¼‰ã¯0.0ã§åŸ‹ã‚ã‚‹
                # ï¼ˆé–¾å€¤ä»¥ä¸‹ã®ä½ç¢ºç‡ã¨ã—ã¦æ‰±ã†ï¼‰
                final_submission['prob_ensemble'] = final_submission['prob_ensemble'].fillna(0.0)
                
                # ä¿å­˜
                final_submission.to_csv(self.output_dir / "final_submission_full.csv", index=False)
                print(f"   ğŸ’¾ å…¨ä»¶å¾©å…ƒæ¸ˆã¿äºˆæ¸¬ã‚’ä¿å­˜: final_submission_full.csv ({len(final_submission):,} è¡Œ)")
                print(f"      Stage 2 å¯¾è±¡: {(final_submission['prob_ensemble'] > 0).sum():,} è¡Œ")
                print(f"      ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤–: {(final_submission['prob_ensemble'] == 0).sum():,} è¡Œ")
            else:
                print("   âš ï¸ å…¨ä»¶å¾©å…ƒã«å¿…è¦ãªæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        except Exception as e:
            print(f"   âš ï¸ å…¨ä»¶å¾©å…ƒã«å¤±æ•—: {e}")
        
        print(f"   âœ… å®Œäº†: {self.output_dir}")
    
    # ========================================
    # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
    # ========================================
    def run(self):
        """å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_data()
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        if LGBM_AVAILABLE:
            self.train_lgbm()
        
        if CATBOOST_AVAILABLE:
            self.train_catboost()
        
        self.train_mlp()
        
        if TABNET_AVAILABLE:
            self.train_tabnet()
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        self.optimize_weights()
        
        # çµæœä¿å­˜
        self.save_results()
        
        elapsed = (datetime.now() - start_time).total_seconds() / 60
        
        print("\n" + "=" * 70)
        print(f"âœ… å…¨å·¥ç¨‹å®Œäº†ï¼ å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}åˆ†")
        print("=" * 70)


def main():
    parser = argparse.ArgumentParser(description="Two-Stage Spatio-Temporal 4-Model Ensemble")
    parser.add_argument('--spatio-temporal-dir', type=str, default="data/spatio_temporal",
                        help="Spatio-Temporalå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument('--stage1-oof', type=str, default="data/processed/stage1_oof_predictions.csv",
                        help="Stage 1 OOFäºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument('--stage1-test', type=str, default="data/processed/stage1_test_predictions.csv",
                        help="Stage 1 ãƒ†ã‚¹ãƒˆäºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument('--output-dir', type=str, default="results/twostage_spatiotemporal_ensemble",
                        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument('--recall-target', type=float, default=0.98,
                        help="Stage 1 Recall Target (default: 0.98)")
    parser.add_argument('--n-folds', type=int, default=5, help="äº¤å·®æ¤œè¨¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°")
    parser.add_argument('--force-retrain', action='store_true',
                        help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç„¡è¦–ã—ã¦æœ€åˆã‹ã‚‰å­¦ç¿’")
    parser.add_argument('--resume', action='store_true',
                        help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‹•ä½œï¼‰")
    
    args = parser.parse_args()
    
    ensemble = TwoStageSpatioTemporalEnsemble(
        spatio_temporal_dir=args.spatio_temporal_dir,
        stage1_oof_path=args.stage1_oof,
        stage1_test_path=args.stage1_test,
        output_dir=args.output_dir,
        stage1_recall_target=args.recall_target,
        n_folds=args.n_folds,
        force_retrain=args.force_retrain,
    )
    
    ensemble.run()


if __name__ == "__main__":
    main()

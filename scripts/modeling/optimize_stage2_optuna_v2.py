"""
Stage 2 Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– v2 (ä¿®æ­£ç‰ˆ)
=====================================================
boost_from_average=True ã«æˆ»ã—ã€æŽ¢ç´¢ç¯„å›²ã‚’çµžã£ãŸå†æŒ‘æˆ¦

- Focal Loss: Alpha, Gamma ã‚’ç‹­ã„ç¯„å›²ã§æŽ¢ç´¢ (æ‰‹å‹•æˆåŠŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜è¿‘)
- è©•ä¾¡æŒ‡æ¨™: Recall 98.5%æ™‚ã®Precision
- LGBMClassifier (sklearn API) ä½¿ç”¨
"""

import pandas as pd
import numpy as np
import os
import sys
import gc
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_recall_curve, average_precision_score
import lightgbm as lgb
import optuna
from scipy.special import expit
import warnings

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from scripts.modeling.train_two_stage_final import TwoStageFinalPipeline

warnings.filterwarnings('ignore')


# ============================================================
# Focal Loss å®Ÿè£… (sklearn APIç”¨: y_true, preds ã®é †åº)
# ============================================================
def get_focal_loss_sklearn(alpha, gamma):
    """
    Focal Lossã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ (sklearn APIç”¨)
    
    Args:
        alpha: æ­£ä¾‹ã®é‡ã¿ (0.0~1.0)
        gamma: é›£æ˜“åº¦ã®é‡ã¿ (0.0~5.0)
    
    Returns:
        focal_loss_fn: LGBMClassifierã®objectiveå¼•æ•°ã«æ¸¡ã™é–¢æ•°
    """
    def focal_loss_fn(y_true, preds):
        # sklearn APIã§ã¯ (y_true, preds) ã®é †åº
        p = expit(preds)
        p = np.clip(p, 1e-15, 1 - 1e-15)
        
        p_t = y_true * p + (1 - y_true) * (1 - p)
        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
        focal_weight = (1 - p_t) ** gamma
        
        grad = alpha_t * focal_weight * (p - y_true)
        hess = alpha_t * focal_weight * p * (1 - p)
        hess = np.maximum(hess, 1e-7)
        
        # ã€è¿½åŠ ã€‘ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•° (Factor)
        # å‹¾é…ãŒå°ã•ã™ãŽã¦å­¦ç¿’ãŒé€²ã¾ãªã„ã®ã‚’é˜²ããŸã‚ã€å€¤ã‚’å¤§ããã™ã‚‹
        factor = 10.0
        return grad * factor, hess * factor
    
    return focal_loss_fn


# ============================================================
# Optuna Objectiveé–¢æ•° (ä¿®æ­£ç‰ˆ)
# ============================================================
class Stage2ObjectiveV2:
    """Stage 2ã®Focal Loss + ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (ä¿®æ­£ç‰ˆ)"""
    
    def __init__(self, X, y, n_folds=5, random_state=42):
        self.X = X
        self.y = y
        self.n_folds = n_folds
        self.random_state = random_state
        self.trial_count = 0
        self.best_score = 0.0
        self.start_time = datetime.now()
    
    def __call__(self, trial):
        self.trial_count += 1
        
        # Focal Lossãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŽ¢ç´¢ (ç‹­ã„ç¯„å›²ã«çµžã‚‹)
        # æ‰‹å‹•è¨­å®š (Alpha=0.75, Gamma=1.0) ã®æˆåŠŸä»˜è¿‘
        focal_alpha = trial.suggest_float('focal_alpha', 0.60, 0.90)
        focal_gamma = trial.suggest_float('focal_gamma', 0.5, 2.0)
        
        # Focal Lossé–¢æ•°ã‚’ç”Ÿæˆ
        fobj = get_focal_loss_sklearn(focal_alpha, focal_gamma)
        
        # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (æŽ¢ç´¢ç¯„å›²ã‚’çµžã‚‹)
        # objective ã¯ params ã‹ã‚‰å‰Šé™¤ã—ã€LGBMClassifier ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æ™‚ã«æ¸¡ã™
        params = {
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'n_jobs': -1,
            'random_state': self.random_state,
            
            # boost_from_average=True (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ) ã‚’ç¶­æŒï¼
            # is_unbalance, scale_pos_weight ã¯ä½¿ã‚ãªã„ (Focal Lossã®Alphaã§åˆ¶å¾¡)
            'is_unbalance': False,
            
            # æŽ¢ç´¢å¯¾è±¡ (ç¯„å›²ã‚’çµžã‚‹)
            'num_leaves': trial.suggest_int('num_leaves', 64, 192),
            'max_depth': trial.suggest_int('max_depth', 6, 12),
            'min_child_samples': trial.suggest_int('min_child_samples', 20, 50),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
            'subsample': trial.suggest_float('subsample', 0.5, 0.8),
            'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1, log=True),
            'n_estimators': 1000,
        }
        
        # Cross-Validation
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            X_train = self.X.iloc[train_idx]
            y_train = self.y[train_idx]
            X_val = self.X.iloc[val_idx]
            y_val = self.y[val_idx]
            
            try:
                # objective ã¯ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æ™‚ã«æŒ‡å®š (sklearn API ã®æŽ¨å¥¨æ–¹æ³•)
                model = lgb.LGBMClassifier(objective=fobj, **params)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(50, verbose=False)]
                )
                
                # è©•ä¾¡ (Logits -> Probability)
                y_pred_raw = model.predict(X_val, raw_score=True)
                y_pred_prob = expit(y_pred_raw)
                
                # PR-AUC (Average Precision) ã‚’è¨ˆç®—
                # é–¾å€¤ã«ä¾å­˜ã›ãšã€ãƒ¢ãƒ‡ãƒ«ã®åˆ†é›¢èƒ½åŠ›å…¨ä½“ã‚’è©•ä¾¡ã™ã‚‹æœ€ã‚‚ãƒ­ãƒã‚¹ãƒˆãªæŒ‡æ¨™
                from sklearn.metrics import average_precision_score
                fold_score = average_precision_score(y_val, y_pred_prob)
                
                scores.append(fold_score)
                
            except Exception as e:
                print(f"      [Fold {fold+1}] Error: {e}")
                scores.append(0.0)
            
            del model
            gc.collect()
        
        mean_score = np.mean(scores)
        
        # é€²æ—è¡¨ç¤º
        elapsed = (datetime.now() - self.start_time).total_seconds()
        if mean_score > self.best_score:
            self.best_score = mean_score
            print(f"   ðŸ† Trial {self.trial_count}: PR-AUC={mean_score:.4f} "
                  f"(Î±={focal_alpha:.2f}, Î³={focal_gamma:.2f}) [NEW BEST!] [{elapsed/60:.1f}min]")
        else:
            print(f"   Trial {self.trial_count}: PR-AUC={mean_score:.4f} "
                  f"(Î±={focal_alpha:.2f}, Î³={focal_gamma:.2f}) [{elapsed/60:.1f}min]")
        
        return mean_score


# ============================================================
# ãƒ¡ã‚¤ãƒ³
# ============================================================
def run_optuna_optimization_v2(
    n_trials: int = 50,
    n_folds: int = 5,
    random_state: int = 42,
    output_dir: str = "results/two_stage_model/optuna_focal_loss_v2_results"
):
    """Optunaæœ€é©åŒ–ã‚’å®Ÿè¡Œ (ä¿®æ­£ç‰ˆ: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ)"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print("=" * 70)
    print("Stage 2 Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– v2 (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆç‰ˆ)")
    print("è©•ä¾¡æŒ‡æ¨™: PR-AUC (Average Precision)")
    print("è¨­å®š: boost_from_average=True, æŽ¢ç´¢ç¯„å›²çµžã‚Šè¾¼ã¿")
    print(f"è©¦è¡Œå›žæ•°: {n_trials}")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ)
    print("\nðŸš€ Pipelineã‚’å®Ÿè¡Œã—ã¦æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    pipeline = TwoStageFinalPipeline()
    X_s2, y_s2 = pipeline.get_stage2_data()
    
    n_pos = y_s2.sum()
    n_neg = len(y_s2) - n_pos
    print(f"   æ¯”çŽ‡ (Neg:Pos) = 1:{n_neg//n_pos}")
    
    # Optuna Studyä½œæˆ
    print("\nðŸ” æœ€é©åŒ–é–‹å§‹...")
    print("-" * 70)
    
    study = optuna.create_study(
        direction='maximize',
        study_name='stage2_focal_loss_v2_optimization',
        sampler=optuna.samplers.TPESampler(seed=random_state)
    )
    
    objective = Stage2ObjectiveV2(X_s2, y_s2, n_folds=n_folds, random_state=random_state)
    
    study.optimize(
        objective,
        n_trials=n_trials,
        show_progress_bar=True,
        gc_after_trial=True
    )
    
    # çµæžœè¡¨ç¤º
    print("\n" + "=" * 70)
    print("âœ… æœ€é©åŒ–å®Œäº†ï¼")
    print("=" * 70)
    
    best = study.best_trial
    print(f"\nðŸ† ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢: Precision@Recall98.5% = {best.value:.4f}")
    print(f"\nðŸ“‹ ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    for key, value in best.params.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    # çµæžœä¿å­˜
    results_df = study.trials_dataframe()
    results_df.to_csv(os.path.join(output_dir, "optuna_trials.csv"), index=False)
    
    best_params_df = pd.DataFrame([best.params])
    best_params_df['prec_at_rec99'] = best.value
    best_params_df.to_csv(os.path.join(output_dir, "best_params.csv"), index=False)
    
    print(f"\nðŸ’¾ çµæžœä¿å­˜:")
    print(f"   - {output_dir}/optuna_trials.csv")
    print(f"   - {output_dir}/best_params.csv")
    
    return study


if __name__ == "__main__":
    import sys
    n_trials = int(sys.argv[1]) if len(sys.argv) > 1 else 50
    
    run_optuna_optimization_v2(n_trials=n_trials)

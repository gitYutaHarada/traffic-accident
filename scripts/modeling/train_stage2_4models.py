"""
Stage 2: 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« (Intel Core Ultra 9 æœ€é©åŒ–ç‰ˆ v2)
LightGBM, CatBoost, TabNet, MLP ã‚’ä½¿ç”¨ã—ã¦ Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã€‚
æœ€å¾Œã«é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’å®Ÿè¡Œã€‚

ä¿®æ­£ç‰ˆ (v2):
- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: OOFäºˆæ¸¬ã‚’ãƒãƒ¼ã‚¸ã—ã¦ã‹ã‚‰ train_test_split
- MLP: Embeddingå±¤ã§ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’é©åˆ‡ã«å‡¦ç†
- TabNet: cat_idxs, cat_dims ã‚’æ­£ã—ãè¨­å®š
- Intelæœ€é©åŒ–: OMP/MKL ã‚¹ãƒ¬ãƒƒãƒ‰è¨­å®š
- PyTorchå†ç¾æ€§: ã‚·ãƒ¼ãƒ‰å›ºå®š

ä½¿ç”¨æ³•:
    python scripts/modeling/train_stage2_4models.py
"""
import os
import gc
import json
import warnings
warnings.filterwarnings('ignore')

# ====== Intel ã‚¹ãƒ¬ãƒƒãƒ‰è¨­å®š (æœ€åˆã«è¨­å®š) ======
N_JOBS = 8  # Intel Core Ultra 9 285K: 8 P-cores
os.environ["OMP_NUM_THREADS"] = str(N_JOBS)
os.environ["MKL_NUM_THREADS"] = str(N_JOBS)
os.environ["OPENBLAS_NUM_THREADS"] = str(N_JOBS)
os.environ["VECLIB_MAXIMUM_THREADS"] = str(N_JOBS)
os.environ["NUMEXPR_NUM_THREADS"] = str(N_JOBS)

import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, average_precision_score,
    precision_score, recall_score, f1_score
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.optimize import minimize

# Intel Extension for Scikit-learn (é«˜é€ŸåŒ–)
try:
    from sklearnex import patch_sklearn
    patch_sklearn()
    print("âœ… Intel Extension for Scikit-learn enabled")
except ImportError:
    print("âš ï¸ sklearnex not available. Install with: pip install scikit-learn-intelex")

import lightgbm as lgb
from catboost import CatBoostClassifier

# TabNet
try:
    from pytorch_tabnet.tab_model import TabNetClassifier
    TABNET_AVAILABLE = True
except ImportError:
    TABNET_AVAILABLE = False
    print("âš ï¸ TabNet not available. Install with: pip install pytorch-tabnet")

# MLP (PyTorch)
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
    # Intelæ‹¡å¼µ (PyTorch)
    try:
        import intel_extension_for_pytorch as ipex
        IPEX_AVAILABLE = True
        print("âœ… Intel Extension for PyTorch enabled")
    except ImportError:
        IPEX_AVAILABLE = False
except ImportError:
    TORCH_AVAILABLE = False
    IPEX_AVAILABLE = False
    print("âš ï¸ PyTorch not available.")


# ====== ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–è¨­å®š ======
BATCH_SIZE_LARGE = 8192  # 64GB RAMã‚’æ´»ç”¨
BATCH_SIZE_MLP = 4096
NUM_WORKERS = 0  # Windowsäº’æ›æ€§ã®ãŸã‚0ã«è¨­å®š


def set_seed(seed: int):
    """å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š"""
    np.random.seed(seed)
    if TORCH_AVAILABLE:
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


class EmbeddingMLP(nn.Module):
    """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’Embeddingã§å‡¦ç†ã™ã‚‹MLP"""
    
    def __init__(self, num_numerical: int, cat_dims: list, embed_dims: list, hidden_dims: list = [512, 256, 64]):
        super().__init__()
        
        # Embedding layers for categorical features
        self.embeddings = nn.ModuleList([
            nn.Embedding(num_classes, embed_dim) 
            for num_classes, embed_dim in zip(cat_dims, embed_dims)
        ])
        
        # Total input dimension
        total_embed_dim = sum(embed_dims)
        input_dim = num_numerical + total_embed_dim
        
        # Build MLP layers: Linear -> BatchNorm -> ReLU -> Dropout
        layers = []
        prev_dim = input_dim
        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            dropout_rate = 0.3 if i == 0 else 0.2
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, 1))
        layers.append(nn.Sigmoid())
        
        self.mlp = nn.Sequential(*layers)
        self.num_numerical = num_numerical
        
    def forward(self, x_numerical, x_categorical):
        # Process categorical features through embeddings
        embedded = [emb(x_categorical[:, i]) for i, emb in enumerate(self.embeddings)]
        embedded = torch.cat(embedded, dim=1) if embedded else torch.zeros(x_numerical.size(0), 0, device=x_numerical.device)
        
        # Concatenate numerical and embedded categorical
        x = torch.cat([x_numerical, embedded], dim=1)
        
        return self.mlp(x)


class Stage2EnsemblePipeline:
    """Stage 2: 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (ä¿®æ­£ç‰ˆ)"""
    
    def __init__(
        self,
        data_path: str = "data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv",
        oof_predictions_path: str = "data/processed/stage1_oof_predictions.csv",
        test_predictions_path: str = "data/processed/stage1_test_predictions.csv",
        target_col: str = "æ­»è€…æ•°",
        stage1_recall_target: float = 0.98,
        n_folds: int = 5,
        random_state: int = 42,
        test_size: float = 0.2,
        output_dir: str = "results/stage2_4model_ensemble"
    ):
        self.data_path = data_path
        self.oof_predictions_path = oof_predictions_path
        self.test_predictions_path = test_predictions_path
        self.target_col = target_col
        self.stage1_recall_target = stage1_recall_target
        self.n_folds = n_folds
        self.random_state = random_state
        self.test_size = test_size
        self.output_dir = output_dir
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.checkpoint_dir = os.path.join(output_dir, "checkpoints")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(output_dir, exist_ok=True)
        
        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        set_seed(random_state)
        
        # çµæœæ ¼ç´ç”¨
        self.oof_predictions = {}
        self.test_predictions = {}
        self.model_aucs = {}
        
    def _checkpoint_path(self, model_name: str) -> str:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"""
        return os.path.join(self.checkpoint_dir, f"{model_name}_checkpoint.npz")
    
    def _save_checkpoint(self, model_name: str, oof_proba: np.ndarray, test_proba: np.ndarray, auc: float):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        path = self._checkpoint_path(model_name)
        np.savez(path, oof_proba=oof_proba, test_proba=test_proba, auc=auc)
        print(f"   ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {model_name}")
    
    def _load_checkpoint(self, model_name: str) -> bool:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ (æˆåŠŸæ™‚ True)"""
        path = self._checkpoint_path(model_name)
        if os.path.exists(path):
            data = np.load(path)
            self.oof_predictions[model_name] = data['oof_proba']
            self.test_predictions[model_name] = data['test_proba']
            self.model_aucs[model_name] = float(data['auc'])
            print(f"   ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ: {model_name} (AUC: {self.model_aucs[model_name]:.4f})")
            return True
        return False
        
    def load_and_filter_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨Stage 1ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ä¿è¨¼)"""
        print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
        
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
        df = pd.read_csv(self.data_path)
        print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,} è¡Œ")
        
        # Stage 1 OOFäºˆæ¸¬ã‚’ãƒ­ãƒ¼ãƒ‰
        df_oof = pd.read_csv(self.oof_predictions_path)
        df_test_pred = pd.read_csv(self.test_predictions_path)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
        if self.target_col in df.columns:
            df['fatal'] = (df[self.target_col] > 0).astype(int)
        elif 'fatal' in df.columns:
            pass
        else:
            raise ValueError(f"Target column not found: {self.target_col}")
        
        # === ğŸ”´ é‡è¦: Stage 1äºˆæ¸¬ã‚’ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ã—ã¦ã‹ã‚‰åˆ†å‰² ===
        # ã“ã®æ®µéšã§df_oofã¨dfã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        # save_stage1_oof.py ã¯ train_test_split ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€
        # åŒã˜random_stateã‚’ä½¿ãˆã°ã€åŒã˜åˆ†å‰²ã«ãªã‚‹ã¯ãš
        
        # ã¾ãš train_test_split ã®ã€Œå‰ã€ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§è€ƒãˆã‚‹
        # df_oof ã¯ã€Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®OOFäºˆæ¸¬ã€ãªã®ã§ã€è¡Œæ•°=è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°
        # df_test_pred ã¯ã€Œãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã€ãªã®ã§ã€è¡Œæ•°=ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°
        
        # é™¤å¤–ã‚«ãƒ©ãƒ 
        exclude_cols = [
            self.target_col, 'fatal', 'è² å‚·è€…æ•°', 'é‡å‚·è€…æ•°', 'è»½å‚·è€…æ•°',
            'å½“äº‹è€…A_æ­»å‚·çŠ¶æ³', 'å½“äº‹è€…B_æ­»å‚·çŠ¶æ³', 'æœ¬ç¥¨ç•ªå·', 'ç™ºç”Ÿæ—¥æ™‚'
        ]
        feature_cols = [c for c in df.columns if c not in exclude_cols]
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ—ã®è­˜åˆ¥
        self.categorical_cols = []
        self.numerical_cols = []
        for col in feature_cols:
            if df[col].dtype == 'object' or df[col].nunique() < 50:
                self.categorical_cols.append(col)
            else:
                self.numerical_cols.append(col)
        
        # Train/Test Split (Stage 1 ã¨åŒã˜ã‚·ãƒ¼ãƒ‰ãƒ»åŒã˜æ‰‹é †)
        X_all = df[feature_cols].copy()
        y_all = df['fatal'].values
        
        # åˆ†å‰²ã‚’å®Ÿè¡Œ (save_stage1_oof.py ã¨åŒä¸€ã®ãƒ­ã‚¸ãƒƒã‚¯)
        X_train_full, X_test, y_train_full, y_test, train_indices, test_indices = train_test_split(
            X_all, y_all, np.arange(len(df)),
            test_size=self.test_size,
            random_state=self.random_state, 
            stratify=y_all
        )
        
        # === Stage 1 äºˆæ¸¬ã‚’æ­£ã—ãç´ä»˜ã‘ ===
        # save_stage1_oof.pyã§ä¿å­˜ã•ã‚ŒãŸOOFã¯ã€åˆ†å‰²å¾Œã®Trainãƒ‡ãƒ¼ã‚¿ã®OOFäºˆæ¸¬
        # è¡Œé †ã¯reset_indexå¾Œã®é †åºã‹ã€Foldé †ã‹ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # æœ€ã‚‚å®‰å…¨ãªæ–¹æ³•: ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ‰±ã†
        
        # OOFãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ãƒã‚§ãƒƒã‚¯
        expected_train_size = len(y_train_full)
        expected_test_size = len(y_test)
        
        if len(df_oof) != expected_train_size:
            print(f"âš ï¸ è­¦å‘Š: OOFã‚µã‚¤ã‚ºä¸ä¸€è‡´ (Expected: {expected_train_size}, Got: {len(df_oof)})")
            print("   â†’ è¡Œé †ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        if len(df_test_pred) != expected_test_size:
            print(f"âš ï¸ è­¦å‘Š: Testäºˆæ¸¬ã‚µã‚¤ã‚ºä¸ä¸€è‡´ (Expected: {expected_test_size}, Got: {len(df_test_pred)})")
        
        # reset_indexã—ã¦æ•´åˆæ€§ã‚’å–ã‚‹
        X_train_full = X_train_full.reset_index(drop=True)
        X_test = X_test.reset_index(drop=True)
        self.y_train_full = y_train_full
        self.y_test = y_test
        
        # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç¢ºç‡
        oof_prob = 0.85 * df_oof['prob_catboost'].values + 0.15 * df_oof['prob_lgbm'].values
        test_prob = 0.85 * df_test_pred['prob_catboost'].values + 0.15 * df_test_pred['prob_lgbm'].values
        
        print(f"\n   Train (Full): {len(self.y_train_full):,} (Fatal: {self.y_train_full.sum():,})")
        print(f"   Test:         {len(self.y_test):,} (Fatal: {self.y_test.sum():,})")
        
        # Recall target ã®é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹
        precision, recall, thresholds = precision_recall_curve(self.y_train_full, oof_prob)
        valid_idx = np.where(recall[:-1] >= self.stage1_recall_target)[0]
        if len(valid_idx) > 0:
            best_idx = valid_idx[-1]
            self.stage1_threshold = thresholds[best_idx]
        else:
            self.stage1_threshold = 0.0
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
        train_mask = oof_prob >= self.stage1_threshold
        test_mask = test_prob >= self.stage1_threshold
        
        self.X_train_full = X_train_full
        self.X_test = X_test
        self.X_train = X_train_full[train_mask].reset_index(drop=True)
        self.y_train = self.y_train_full[train_mask]
        self.X_test_filtered = X_test[test_mask].reset_index(drop=True)
        self.y_test_filtered = self.y_test[test_mask]
        
        self.test_mask = test_mask
        
        train_recall = self.y_train.sum() / self.y_train_full.sum()
        test_recall_stage1 = self.y_test_filtered.sum() / self.y_test.sum()
        
        print(f"\nğŸ¯ Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Recall Target: {self.stage1_recall_target:.1%})")
        print(f"   é–¾å€¤: {self.stage1_threshold:.4f}")
        print(f"   Train: {len(self.y_train):,} / {len(self.y_train_full):,} ({len(self.y_train)/len(self.y_train_full):.1%} é€šé)")
        print(f"   Train Recall: {train_recall:.2%}")
        print(f"   Test:  {len(self.y_test_filtered):,} / {len(self.y_test):,} ({len(self.y_test_filtered)/len(self.y_test):.1%} é€šé)")
        print(f"   Test Recall (Stage1): {test_recall_stage1:.2%}")
        
        self.feature_cols = feature_cols
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’äº‹å‰è¨ˆç®—
        self._prepare_categorical_encoders()
        
        gc.collect()
    
    def _prepare_categorical_encoders(self):
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’äº‹å‰æº–å‚™"""
        self.cat_encoders = {}
        self.cat_dims = []  # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚¯ãƒ©ã‚¹æ•°
        self.cat_idxs = []  # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        all_data = pd.concat([self.X_train, self.X_test_filtered], axis=0)
        
        for i, col in enumerate(self.feature_cols):
            if col in self.categorical_cols:
                le = LabelEncoder()
                le.fit(all_data[col].astype(str).fillna('__MISSING__'))
                self.cat_encoders[col] = le
                self.cat_dims.append(len(le.classes_))
                self.cat_idxs.append(i)
        
        print(f"\n   ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: {len(self.categorical_cols)} åˆ—")
        print(f"   æ•°å€¤å¤‰æ•°: {len(self.numerical_cols)} åˆ—")
    
    def _encode_categorical(self, X: pd.DataFrame) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        X_encoded = X.copy()
        for col, le in self.cat_encoders.items():
            if col in X_encoded.columns:
                X_encoded[col] = le.transform(X_encoded[col].astype(str).fillna('__MISSING__'))
        return X_encoded
    
    def _prepare_nn_data(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame):
        """NNç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ (æ•°å€¤ã¨ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é›¢)"""
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        X_train_enc = self._encode_categorical(X_train)
        X_val_enc = self._encode_categorical(X_val)
        X_test_enc = self._encode_categorical(X_test)
        
        # æ•°å€¤å¤‰æ•°ã‚’æŠ½å‡ºãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        num_cols = [c for c in self.numerical_cols if c in X_train.columns]
        
        scaler = StandardScaler()
        X_train_num = scaler.fit_transform(X_train_enc[num_cols].fillna(X_train_enc[num_cols].median()))
        X_val_num = scaler.transform(X_val_enc[num_cols].fillna(X_train_enc[num_cols].median()))
        X_test_num = scaler.transform(X_test_enc[num_cols].fillna(X_train_enc[num_cols].median()))
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æŠ½å‡º
        cat_cols = [c for c in self.categorical_cols if c in X_train.columns]
        X_train_cat = X_train_enc[cat_cols].values.astype(np.int64)
        X_val_cat = X_val_enc[cat_cols].values.astype(np.int64)
        X_test_cat = X_test_enc[cat_cols].values.astype(np.int64)
        
        return (
            X_train_num.astype(np.float32), X_train_cat,
            X_val_num.astype(np.float32), X_val_cat,
            X_test_num.astype(np.float32), X_test_cat
        )
    
    def train_lightgbm(self):
        """LightGBM ã®å­¦ç¿’"""
        if self._load_checkpoint('lgbm'):
            return
        
        print("\nğŸŒ² LightGBM å­¦ç¿’ä¸­...")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        oof_proba = np.zeros(len(self.y_train))
        test_proba = np.zeros(len(self.y_test_filtered))
        
        X = self.X_train.copy()
        for col in self.categorical_cols:
            if col in X.columns:
                X[col] = X[col].astype('category')
        
        X_test = self.X_test_filtered.copy()
        for col in self.categorical_cols:
            if col in X_test.columns:
                X_test[col] = X_test[col].astype('category')
        
        params = {
            'objective': 'binary', 
            'metric': 'auc', 
            'boosting_type': 'gbdt',
            'verbosity': -1, 
            'num_leaves': 31, 
            'max_depth': 8,
            'learning_rate': 0.05, 
            'n_estimators': 1000, 
            'n_jobs': N_JOBS,
            'force_row_wise': True
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, self.y_train)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_tr, y_val = self.y_train[train_idx], self.y_train[val_idx]
            
            model = lgb.LGBMClassifier(**params, random_state=self.random_state + fold)
            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],
                      callbacks=[lgb.early_stopping(50, verbose=False)])
            
            oof_proba[val_idx] = model.predict_proba(X_val)[:, 1]
            test_proba += model.predict_proba(X_test)[:, 1] / self.n_folds
            gc.collect()
        
        auc = roc_auc_score(self.y_train, oof_proba)
        print(f"   LightGBM OOF AUC: {auc:.4f}")
        
        self.oof_predictions['lgbm'] = oof_proba
        self.test_predictions['lgbm'] = test_proba
        self.model_aucs['lgbm'] = auc
        
        self._save_checkpoint('lgbm', oof_proba, test_proba, auc)
    
    def train_catboost(self):
        """CatBoost ã®å­¦ç¿’"""
        if self._load_checkpoint('catboost'):
            return
        
        print("\nğŸ± CatBoost å­¦ç¿’ä¸­...")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        oof_proba = np.zeros(len(self.y_train))
        test_proba = np.zeros(len(self.y_test_filtered))
        
        X = self.X_train.copy()
        for col in self.categorical_cols:
            if col in X.columns:
                X[col] = X[col].astype(str)
        
        X_test = self.X_test_filtered.copy()
        for col in self.categorical_cols:
            if col in X_test.columns:
                X_test[col] = X_test[col].astype(str)
        
        cat_features = [i for i, c in enumerate(X.columns) if c in self.categorical_cols]
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, self.y_train)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_tr, y_val = self.y_train[train_idx], self.y_train[val_idx]
            
            model = CatBoostClassifier(
                iterations=1000, 
                learning_rate=0.05, 
                depth=8,
                cat_features=cat_features, 
                verbose=0, 
                thread_count=N_JOBS,
                random_state=self.random_state + fold, 
                early_stopping_rounds=50,
                task_type='CPU'
            )
            model.fit(X_tr, y_tr, eval_set=(X_val, y_val))
            
            oof_proba[val_idx] = model.predict_proba(X_val)[:, 1]
            test_proba += model.predict_proba(X_test)[:, 1] / self.n_folds
            gc.collect()
        
        auc = roc_auc_score(self.y_train, oof_proba)
        print(f"   CatBoost OOF AUC: {auc:.4f}")
        
        self.oof_predictions['catboost'] = oof_proba
        self.test_predictions['catboost'] = test_proba
        self.model_aucs['catboost'] = auc
        
        self._save_checkpoint('catboost', oof_proba, test_proba, auc)
    
    def train_tabnet(self):
        """TabNet ã®å­¦ç¿’ (cat_idxs, cat_dims è¨­å®š)"""
        if not TABNET_AVAILABLE:
            print("\nâš ï¸ TabNet ã‚¹ã‚­ãƒƒãƒ— (ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)")
            return
        
        if self._load_checkpoint('tabnet'):
            return
        
        print("\nğŸ“Š TabNet å­¦ç¿’ä¸­...")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        oof_proba = np.zeros(len(self.y_train))
        test_proba = np.zeros(len(self.y_test_filtered))
        
        # TabNetç”¨ã«ã‚«ãƒ†ã‚´ãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
        cat_idxs = [i for i, c in enumerate(self.feature_cols) if c in self.categorical_cols]
        cat_dims = [self.cat_dims[self.categorical_cols.index(c)] for c in self.feature_cols if c in self.categorical_cols]
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X_train, self.y_train)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_tr, X_val = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]
            y_tr, y_val = self.y_train[train_idx], self.y_train[val_idx]
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            X_tr_enc = self._encode_categorical(X_tr)
            X_val_enc = self._encode_categorical(X_val)
            X_test_enc = self._encode_categorical(self.X_test_filtered)
            
            # æ•°å€¤åˆ—ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            num_cols = [c for c in self.numerical_cols if c in X_tr.columns]
            scaler = StandardScaler()
            X_tr_enc[num_cols] = scaler.fit_transform(X_tr_enc[num_cols].fillna(0))
            X_val_enc[num_cols] = scaler.transform(X_val_enc[num_cols].fillna(0))
            X_test_enc[num_cols] = scaler.transform(X_test_enc[num_cols].fillna(0))
            
            X_tr_np = X_tr_enc.values.astype(np.float32)
            X_val_np = X_val_enc.values.astype(np.float32)
            X_test_np = X_test_enc.values.astype(np.float32)
            
            model = TabNetClassifier(
                n_d=32, n_a=32, n_steps=5,
                gamma=1.5, n_independent=2, n_shared=2,
                cat_idxs=cat_idxs,
                cat_dims=cat_dims,
                cat_emb_dim=1,  # Embedding dimension
                seed=self.random_state + fold, 
                verbose=0,
                device_name='cpu'
            )
            model.fit(
                X_tr_np, y_tr,
                eval_set=[(X_val_np, y_val)],
                eval_metric=['auc'],
                max_epochs=100,
                patience=10,
                batch_size=BATCH_SIZE_LARGE,
                virtual_batch_size=1024,
                num_workers=NUM_WORKERS,
                drop_last=False
            )
            
            oof_proba[val_idx] = model.predict_proba(X_val_np)[:, 1]
            test_proba += model.predict_proba(X_test_np)[:, 1] / self.n_folds
            gc.collect()
        
        auc = roc_auc_score(self.y_train, oof_proba)
        print(f"   TabNet OOF AUC: {auc:.4f}")
        
        self.oof_predictions['tabnet'] = oof_proba
        self.test_predictions['tabnet'] = test_proba
        self.model_aucs['tabnet'] = auc
        
        self._save_checkpoint('tabnet', oof_proba, test_proba, auc)
    
    def train_mlp(self):
        """MLP ã®å­¦ç¿’ (Embeddingå±¤ã§ã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†)"""
        if not TORCH_AVAILABLE:
            print("\nâš ï¸ MLP ã‚¹ã‚­ãƒƒãƒ— (PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)")
            return
        
        if self._load_checkpoint('mlp'):
            return
        
        print("\nğŸ§  MLP å­¦ç¿’ä¸­...")
        
        device = torch.device('cpu')
        torch.set_num_threads(N_JOBS)
        print(f"   Device: {device}, Threads: {N_JOBS}")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        oof_proba = np.zeros(len(self.y_train))
        test_proba = np.zeros(len(self.y_test_filtered))
        
        # Embeddingæ¬¡å…ƒã‚’è¨ˆç®— (ã‚¯ãƒ©ã‚¹æ•°ã®å¹³æ–¹æ ¹ã®2å€ã€æœ€å¤§50)
        embed_dims = [min(50, max(4, int(np.sqrt(d) * 2))) for d in self.cat_dims]
        num_numerical = len(self.numerical_cols)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X_train, self.y_train)):
            set_seed(self.random_state + fold)
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_tr, X_val = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]
            y_tr, y_val = self.y_train[train_idx], self.y_train[val_idx]
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            X_tr_num, X_tr_cat, X_val_num, X_val_cat, X_test_num, X_test_cat = self._prepare_nn_data(
                X_tr, X_val, self.X_test_filtered
            )
            
            # DataLoaderä½œæˆ
            train_ds = TensorDataset(
                torch.tensor(X_tr_num),
                torch.tensor(X_tr_cat),
                torch.tensor(y_tr, dtype=torch.float32)
            )
            val_ds = TensorDataset(
                torch.tensor(X_val_num),
                torch.tensor(X_val_cat),
                torch.tensor(y_val, dtype=torch.float32)
            )
            
            train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE_MLP, shuffle=True, num_workers=NUM_WORKERS)
            val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE_MLP, shuffle=False, num_workers=NUM_WORKERS)
            
            # ãƒ¢ãƒ‡ãƒ«å®šç¾©
            model = EmbeddingMLP(
                num_numerical=num_numerical,
                cat_dims=self.cat_dims,
                embed_dims=embed_dims,
                hidden_dims=[1024, 512, 128]
            ).to(device)
            
            if IPEX_AVAILABLE:
                model = ipex.optimize(model)
            
            criterion = nn.BCELoss()
            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
            
            best_auc = 0
            best_model_state = None
            patience_counter = 0
            max_patience = 10
            
            for epoch in range(100):
                model.train()
                for X_num, X_cat, y_batch in train_loader:
                    X_num, X_cat, y_batch = X_num.to(device), X_cat.to(device), y_batch.to(device)
                    optimizer.zero_grad()
                    output = model(X_num, X_cat).squeeze()
                    loss = criterion(output, y_batch)
                    loss.backward()
                    optimizer.step()
                
                # Validation
                model.eval()
                val_preds = []
                val_targets = []
                with torch.no_grad():
                    for X_num, X_cat, y_batch in val_loader:
                        X_num, X_cat = X_num.to(device), X_cat.to(device)
                        output = model(X_num, X_cat).squeeze().cpu().numpy()
                        val_preds.extend(output)
                        val_targets.extend(y_batch.numpy())
                
                val_auc = roc_auc_score(val_targets, val_preds)
                scheduler.step(-val_auc)
                
                if val_auc > best_auc:
                    best_auc = val_auc
                    best_model_state = {k: v.clone() for k, v in model.state_dict().items()}
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= max_patience:
                    break
            
            # Best model ã§äºˆæ¸¬
            model.load_state_dict(best_model_state)
            model.eval()
            
            with torch.no_grad():
                X_val_num_t = torch.tensor(X_val_num).to(device)
                X_val_cat_t = torch.tensor(X_val_cat).to(device)
                oof_proba[val_idx] = model(X_val_num_t, X_val_cat_t).squeeze().cpu().numpy()
                
                X_test_num_t = torch.tensor(X_test_num).to(device)
                X_test_cat_t = torch.tensor(X_test_cat).to(device)
                test_proba += model(X_test_num_t, X_test_cat_t).squeeze().cpu().numpy() / self.n_folds
            
            gc.collect()
        
        auc = roc_auc_score(self.y_train, oof_proba)
        print(f"   MLP OOF AUC: {auc:.4f}")
        
        self.oof_predictions['mlp'] = oof_proba
        self.test_predictions['mlp'] = test_proba
        self.model_aucs['mlp'] = auc
        
        self._save_checkpoint('mlp', oof_proba, test_proba, auc)
    
    def optimize_ensemble_weights(self):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã®æœ€é©åŒ–"""
        print("\nâš–ï¸ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–...")
        
        available_models = list(self.oof_predictions.keys())
        n_models = len(available_models)
        
        if n_models < 2:
            print("   ãƒ¢ãƒ‡ãƒ«ãŒ1ã¤ä»¥ä¸‹ã®ãŸã‚ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸å¯")
            return
        
        oof_matrix = np.column_stack([self.oof_predictions[m] for m in available_models])
        
        def neg_auc(weights):
            weights = np.array(weights)
            weights = weights / weights.sum()
            ensemble_pred = oof_matrix @ weights
            return -roc_auc_score(self.y_train, ensemble_pred)
        
        init_weights = np.ones(n_models) / n_models
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0, 1) for _ in range(n_models)]
        
        result = minimize(neg_auc, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)
        
        self.ensemble_weights = {m: w for m, w in zip(available_models, result.x)}
        
        print("   æœ€é©åŒ–ã•ã‚ŒãŸé‡ã¿:")
        for model, weight in self.ensemble_weights.items():
            print(f"      {model}: {weight:.4f}")
        
        self.oof_predictions['ensemble'] = oof_matrix @ result.x
        
        test_matrix = np.column_stack([self.test_predictions[m] for m in available_models])
        self.test_predictions['ensemble'] = test_matrix @ result.x
        
        ensemble_auc = roc_auc_score(self.y_train, self.oof_predictions['ensemble'])
        print(f"   Ensemble OOF AUC: {ensemble_auc:.4f}")
        self.model_aucs['ensemble'] = ensemble_auc
    
    def evaluate(self):
        """æœ€çµ‚è©•ä¾¡"""
        print("\nğŸ“ˆ æœ€çµ‚è©•ä¾¡...")
        
        results = []
        for model_name in self.oof_predictions.keys():
            oof_pred = self.oof_predictions[model_name]
            test_pred = self.test_predictions[model_name]
            
            oof_auc = roc_auc_score(self.y_train, oof_pred)
            test_auc = roc_auc_score(self.y_test_filtered, test_pred)
            oof_pr_auc = average_precision_score(self.y_train, oof_pred)
            test_pr_auc = average_precision_score(self.y_test_filtered, test_pred)
            
            results.append({
                'model': model_name,
                'oof_roc_auc': oof_auc,
                'test_roc_auc': test_auc,
                'oof_pr_auc': oof_pr_auc,
                'test_pr_auc': test_pr_auc
            })
            
            print(f"   {model_name:12s} | OOF AUC: {oof_auc:.4f} | Test AUC: {test_auc:.4f} | OOF PR-AUC: {oof_pr_auc:.4f}")
        
        self.results_df = pd.DataFrame(results)
        self.results_df.to_csv(os.path.join(self.output_dir, "model_comparison.csv"), index=False)
        
        oof_df = pd.DataFrame(self.oof_predictions)
        oof_df['target'] = self.y_train
        oof_df.to_csv(os.path.join(self.output_dir, "oof_predictions.csv"), index=False)
        
        test_df = pd.DataFrame(self.test_predictions)
        test_df['target'] = self.y_test_filtered
        test_df.to_csv(os.path.join(self.output_dir, "test_predictions.csv"), index=False)
    
    def generate_report(self, elapsed_sec: float):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...")
        
        report_content = f"""# Stage 2: 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ (v2)

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å®Ÿè¡Œæ™‚é–“**: {elapsed_sec:.1f}ç§’
**ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢**: Intel Core Ultra 9 285K (n_jobs={N_JOBS})

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆ

- **Stage 1**: Weighted Ensemble (Recall {self.stage1_recall_target:.0%})
- **Stage 2**: LightGBM, CatBoost, TabNet (cat_idxså¯¾å¿œ), MLP (Embeddingå±¤) â†’ Weighted Ensemble

## Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| é–¾å€¤ | {self.stage1_threshold:.4f} |
| Trainé€šéç‡ | {len(self.y_train) / len(self.y_train_full):.1%} |
| Testé€šéç‡ | {len(self.y_test_filtered) / len(self.y_test):.1%} |

## ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

| Model | OOF ROC-AUC | Test ROC-AUC | OOF PR-AUC |
|-------|-------------|--------------|------------|
"""
        for _, row in self.results_df.iterrows():
            report_content += f"| {row['model']} | {row['oof_roc_auc']:.4f} | {row['test_roc_auc']:.4f} | {row['oof_pr_auc']:.4f} |\n"
        
        if hasattr(self, 'ensemble_weights'):
            report_content += "\n## ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿\n\n"
            for model, weight in self.ensemble_weights.items():
                report_content += f"- **{model}**: {weight:.4f}\n"
        
        report_content += f"""
## è€ƒå¯Ÿ

- æœ€é«˜å˜ä½“ãƒ¢ãƒ‡ãƒ« AUC: {self.results_df[self.results_df['model'] != 'ensemble']['test_roc_auc'].max():.4f}
- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« AUC: {self.results_df[self.results_df['model'] == 'ensemble']['test_roc_auc'].values[0]:.4f}

## ä¿®æ­£ç‚¹ (v2)

1. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: Stage 1 OOFäºˆæ¸¬ã¨ train_test_split ã®åŒæœŸã‚’ç¢ºèª
2. MLP: ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã« Embedding å±¤ã‚’ä½¿ç”¨ï¼ˆèª¤ã£ãŸé †åºé–¢ä¿‚ã‚’æ’é™¤ï¼‰
3. TabNet: cat_idxs, cat_dims ã‚’æ­£ã—ãè¨­å®š
4. Intelæœ€é©åŒ–: OMP/MKL ã‚¹ãƒ¬ãƒƒãƒ‰è¨­å®šã‚’æ˜ç¤º
5. PyTorchå†ç¾æ€§: ã‚·ãƒ¼ãƒ‰å›ºå®š
"""
        
        report_path = os.path.join(self.output_dir, "experiment_report.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"   ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    
    def run(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        start = datetime.now()
        
        print("=" * 70)
        print("ğŸš€ Stage 2: 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (v2)")
        print(f"   æœ€é©åŒ–: Intel Core Ultra 9 285K (n_jobs={N_JOBS})")
        print(f"   ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {self.checkpoint_dir}")
        print("=" * 70)
        
        self.load_and_filter_data()
        self.train_lightgbm()
        self.train_catboost()
        self.train_tabnet()
        self.train_mlp()
        self.optimize_ensemble_weights()
        self.evaluate()
        
        elapsed_sec = (datetime.now() - start).total_seconds()
        self.generate_report(elapsed_sec)
        
        print("\n" + "=" * 70)
        print("âœ… å®Œäº†ï¼")
        print(f"   çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {elapsed_sec:.1f}ç§’")
        print("=" * 70)


if __name__ == "__main__":
    pipeline = Stage2EnsemblePipeline()
    pipeline.run()

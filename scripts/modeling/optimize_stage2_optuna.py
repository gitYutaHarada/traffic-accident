"""
Stage 2 Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (Focal Losså¯¾å¿œ)
=========================================================
Implementation Plan v23 - Focal Loss + Recall 99% Precisionæœ€å¤§åŒ–

- Focal Loss: Alpha, Gamma ã‚’æŽ¢ç´¢
- è©•ä¾¡æŒ‡æ¨™: Recall 99%æ™‚ã®Precision
- Pruning: ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ã§æ—©æœŸæ‰“ã¡åˆ‡ã‚Š
- å®‰å®šæ€§: boost_from_average=False, Hessianè¿‘ä¼¼, å‹¾é…ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
"""

import pandas as pd
import numpy as np
import os
import pickle
import gc
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_recall_curve, average_precision_score
import lightgbm as lgb
import optuna
from optuna.integration import LightGBMPruningCallback
import warnings

warnings.filterwarnings('ignore')


# ============================================================
# Focal Loss å®Ÿè£… (ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£)
# ============================================================
def get_focal_loss(alpha, gamma):
    """
    Focal Lossã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£
    
    Args:
        alpha: æ­£ä¾‹ã®é‡ã¿ (0.0~1.0)
        gamma: é›£æ˜“åº¦ã®é‡ã¿ (0.0~5.0)
    
    Returns:
        focal_loss_fixed: lgb.trainã®fobjå¼•æ•°ã«æ¸¡ã™é–¢æ•°
    """
    def focal_loss_fixed(preds, train_data):
        y_true = train_data.get_label()
        
        # Logits -> Probability (æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã‚¯ãƒªãƒƒãƒ—)
        p = 1.0 / (1.0 + np.exp(-preds))
        p = np.clip(p, 1e-15, 1 - 1e-15)
        
        # p_t: æ­£è§£ã‚¯ãƒ©ã‚¹ã®ç¢ºçŽ‡
        p_t = y_true * p + (1 - y_true) * (1 - p)
        
        # alpha_t: ã‚¯ãƒ©ã‚¹ã”ã¨ã®é‡ã¿
        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
        
        # Focal weight: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** gamma
        
        # 1. å‹¾é… (Gradient) - åŽ³å¯†è§£
        grad = alpha_t * focal_weight * (p - y_true)
        
        # 2. ãƒ˜ãƒƒã‚»è¡Œåˆ— (Hessian) - è¿‘ä¼¼è§£ (å®‰å®šæ€§é‡è¦–)
        hess = alpha_t * focal_weight * p * (1 - p)
        hess = np.maximum(hess, 1e-7)  # æ•°å€¤å®‰å®šæ€§
        
        # 3. å‹¾é…ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° (å­¦ç¿’é€²è¡Œä¿ƒé€²)
        factor = 10.0
        return grad * factor, hess * factor
    
    return focal_loss_fixed


# ============================================================
# ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡é–¢æ•° (Recall 99%æ™‚ã®Precision)
# ============================================================
def custom_eval_metric(preds, train_data):
    """
    LightGBMç”¨ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡é–¢æ•°
    Recall >= 98.5% ã‚’æº€ãŸã™æœ€å¤§Precisionã‚’è¿”ã™
    """
    y_true = train_data.get_label()
    
    # Logits -> Probability
    p = 1.0 / (1.0 + np.exp(-preds))
    
    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_true, p)
    
    # Recall >= 0.985 ã®æœ€å¤§Precisionã‚’æŽ¢ã™
    target_recall = 0.985
    valid_indices = recall >= target_recall
    
    if valid_indices.sum() > 0:
        score = precision[valid_indices].max()
    else:
        score = 0.0
    
    return 'prec_at_rec99', score, True  # name, value, higher_is_better


# ============================================================
# Optuna Objectiveé–¢æ•° (Focal Losså¯¾å¿œ)
# ============================================================
class Stage2FocalLossObjective:
    """Stage 2ã®Focal Loss + ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ç”¨Objective"""
    
    def __init__(self, X, y, n_folds=5, random_state=42):
        self.X = X
        self.y = y
        self.n_folds = n_folds
        self.random_state = random_state
        self.trial_count = 0
        self.best_score = 0.0
        self.start_time = datetime.now()
    
    def __call__(self, trial):
        self.trial_count += 1
        
        # Focal Lossãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŽ¢ç´¢
        focal_alpha = trial.suggest_float('focal_alpha', 0.1, 0.9)
        focal_gamma = trial.suggest_float('focal_gamma', 0.0, 5.0)
        
        # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŽ¢ç´¢
        # Focal Lossé–¢æ•°ã‚’ç”Ÿæˆ (paramsã«è¨­å®šã™ã‚‹ãŸã‚å…ˆã«ä½œæˆ)
        fobj = get_focal_loss(focal_alpha, focal_gamma)
        
        params = {
            'objective': fobj,  # LightGBM v4+: paramså†…ã«ã‚«ã‚¹ã‚¿ãƒ ç›®çš„é–¢æ•°ã‚’è¨­å®š
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'n_jobs': -1,
            'random_state': self.random_state,
            
            # ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ä½¿ç”¨æ™‚ã®å¿…é ˆè¨­å®š
            'boost_from_average': False,
            'is_unbalance': False,
            # scale_pos_weightã¯é™¤å¤– (Focal Lossã®Alphaã§åˆ¶å¾¡)
            
            # æŽ¢ç´¢å¯¾è±¡
            'num_leaves': trial.suggest_int('num_leaves', 31, 255),
            'max_depth': trial.suggest_int('max_depth', 8, 15),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),
            'subsample': trial.suggest_float('subsample', 0.5, 0.9),
            'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.2, log=True),
        }
        
        # Cross-Validation
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            X_train = self.X.iloc[train_idx]
            y_train = self.y[train_idx]
            X_val = self.X.iloc[val_idx]
            y_val = self.y[val_idx]
            
            # LightGBM Dataset
            dtrain = lgb.Dataset(X_train, label=y_train)
            dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)
            
            # Pruning Callback (ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ã‚’ç›£è¦–)
            pruning_callback = LightGBMPruningCallback(trial, 'prec_at_rec99')
            
            try:
                model = lgb.train(
                    params,
                    dtrain,
                    num_boost_round=500,
                    valid_sets=[dval],
                    feval=custom_eval_metric,  # ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡
                    callbacks=[
                        lgb.early_stopping(50, verbose=False),
                        pruning_callback
                    ]
                )
                
                # è©•ä¾¡ (Logits -> Probability)
                y_pred_logits = model.predict(X_val)
                y_pred_prob = 1.0 / (1.0 + np.exp(-y_pred_logits))
                
                # Recall 99%æ™‚ã®Precisionã‚’è¨ˆç®—
                precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)
                valid_indices = recall >= 0.985
                if valid_indices.sum() > 0:
                    fold_score = precision[valid_indices].max()
                else:
                    fold_score = 0.0
                
                scores.append(fold_score)
                
            except optuna.TrialPruned:
                raise
            
            del model
            gc.collect()
        
        mean_score = np.mean(scores)
        
        # é€²æ—è¡¨ç¤º
        elapsed = (datetime.now() - self.start_time).total_seconds()
        if mean_score > self.best_score:
            self.best_score = mean_score
            print(f"   ðŸ† Trial {self.trial_count}: Prec@Rec99={mean_score:.4f} "
                  f"(Î±={focal_alpha:.2f}, Î³={focal_gamma:.2f}) [NEW BEST!] [{elapsed/60:.1f}min]")
        else:
            print(f"   Trial {self.trial_count}: Prec@Rec99={mean_score:.4f} "
                  f"(Î±={focal_alpha:.2f}, Î³={focal_gamma:.2f}) [{elapsed/60:.1f}min]")
        
        return mean_score


# ============================================================
# ãƒ¡ã‚¤ãƒ³
# ============================================================
def run_optuna_optimization(
    data_path: str = "results/two_stage_model/optuna_data/stage2_train_data.pkl",
    n_trials: int = 50,
    n_folds: int = 5,
    random_state: int = 42,
    output_dir: str = "results/two_stage_model/optuna_focal_loss_results"
):
    """Optunaæœ€é©åŒ–ã‚’å®Ÿè¡Œ (Focal Losså¯¾å¿œ)"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print("=" * 70)
    print("Stage 2 Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (Focal Loss)")
    print("è©•ä¾¡æŒ‡æ¨™: Precision @ Recall 99%")
    print(f"è©¦è¡Œå›žæ•°: {n_trials}")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nðŸ“‚ Stage 2ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    X_s2 = data['X_s2']
    y_s2 = data['y_s2']
    
    n_pos = y_s2.sum()
    n_neg = len(y_s2) - n_pos
    print(f"   ãƒ‡ãƒ¼ã‚¿æ•°: {len(y_s2):,} (Pos: {n_pos:,}, Neg: {n_neg:,}, æ¯”çŽ‡ 1:{n_neg//n_pos})")
    
    # Optuna Studyä½œæˆ
    print("\nðŸ” æœ€é©åŒ–é–‹å§‹...")
    print("-" * 70)
    
    study = optuna.create_study(
        direction='maximize',
        study_name='stage2_focal_loss_optimization',
        sampler=optuna.samplers.TPESampler(seed=random_state)
    )
    
    objective = Stage2FocalLossObjective(X_s2, y_s2, n_folds=n_folds, random_state=random_state)
    
    study.optimize(
        objective,
        n_trials=n_trials,
        show_progress_bar=True,
        gc_after_trial=True
    )
    
    # çµæžœè¡¨ç¤º
    print("\n" + "=" * 70)
    print("âœ… æœ€é©åŒ–å®Œäº†ï¼")
    print("=" * 70)
    
    best = study.best_trial
    print(f"\nðŸ† ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢: Precision@Recall99% = {best.value:.4f}")
    print(f"\nðŸ“‹ ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    for key, value in best.params.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    # çµæžœä¿å­˜
    results_df = study.trials_dataframe()
    results_df.to_csv(os.path.join(output_dir, "optuna_trials.csv"), index=False)
    
    best_params_df = pd.DataFrame([best.params])
    best_params_df['prec_at_rec99'] = best.value
    best_params_df.to_csv(os.path.join(output_dir, "best_params.csv"), index=False)
    
    print(f"\nðŸ’¾ çµæžœä¿å­˜:")
    print(f"   - {output_dir}/optuna_trials.csv")
    print(f"   - {output_dir}/best_params.csv")
    
    return study


if __name__ == "__main__":
    # å¼•æ•°ã§n_trialsã‚’èª¿æ•´å¯èƒ½
    import sys
    n_trials = int(sys.argv[1]) if len(sys.argv) > 1 else 50
    
    run_optuna_optimization(n_trials=n_trials)

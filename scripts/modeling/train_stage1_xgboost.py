"""
Stage 1 XGBoost å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (æ”¹è‰¯ç‰ˆ)
=======================================
LightGBMã¨ã®æ¯”è¼ƒã®ãŸã‚ã€Stage 1ã®ãƒ¢ãƒ‡ãƒ«ã‚’XGBoostã«å¤‰æ›´ã—ã¦
ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°èƒ½åŠ›ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

æ”¹è‰¯ç‚¹:
- Early Stoppingè¿½åŠ ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
- ä¸å‡è¡¡å¯¾ç­–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆscale_pos_weightï¼‰
- LightGBMã¨ã®ç›¸é–¢åˆ†ææ©Ÿèƒ½ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœè¦‹ç©ã‚‚ã‚Šï¼‰
- OOFçµæœã®IDç´ä»˜ã‘ä¿å­˜

è©•ä¾¡æŒ‡æ¨™:
- OOF AUC
- Recall 99%æ™‚ã®é–¾å€¤
- é™¤å¤–ç‡ (Filter Rate)
"""

import pandas as pd
import numpy as np
import os
import gc
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, roc_auc_score
import xgboost as xgb
import warnings

warnings.filterwarnings('ignore')


class Stage1XGBoostExperiment:
    """Stage 1 XGBoost å®Ÿé¨“ã‚¯ãƒ©ã‚¹ (æ”¹è‰¯ç‰ˆ)"""
    
    def __init__(
        self,
        data_path: str = "data/processed/honhyo_clean_with_features.csv",
        target_col: str = "æ­»è€…æ•°",
        n_folds: int = 5,
        random_state: int = 42,
        stage1_recall_target: float = 0.99,
        undersample_ratio: float = 2.0,
        n_seeds: int = 3,
    ):
        self.data_path = data_path
        self.target_col = target_col
        self.n_folds = n_folds
        self.random_state = random_state
        self.stage1_recall_target = stage1_recall_target
        self.undersample_ratio = undersample_ratio
        self.n_seeds = n_seeds
        
        self.output_dir = "results/two_stage_model/xgboost_experiment"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("=" * 60)
        print("Stage 1 XGBoost å®Ÿé¨“ (æ”¹è‰¯ç‰ˆ)")
        print(f"Under-sampling 1:{int(self.undersample_ratio)}, Recall Target {self.stage1_recall_target:.0%}")
        print("=" * 60)
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        self.df = pd.read_csv(self.data_path)
        self.y = self.df[self.target_col].values
        self.X = self.df.drop(columns=[self.target_col])
        
        if 'ç™ºç”Ÿæ—¥æ™‚' in self.X.columns:
            self.X = self.X.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®å‡¦ç†
        known_categoricals = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
            'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
            'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
            'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
            'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
        ]
        
        self.categorical_cols = []
        for col in self.X.columns:
            if col in known_categoricals or self.X[col].dtype == 'object':
                self.categorical_cols.append(col)
                self.X[col] = self.X[col].astype('category')
            else:
                self.X[col] = self.X[col].astype(np.float32)
        
        self.feature_names = list(self.X.columns)
        print(f"   æ­£ä¾‹: {self.y.sum():,} / {len(self.y):,}")
        print(f"   ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°: {len(self.categorical_cols)}å€‹")
        gc.collect()
    
    def undersample(self, X, y, seed):
        """è² ä¾‹ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        pos_idx = np.where(y == 1)[0]
        neg_idx = np.where(y == 0)[0]
        n_neg_sample = int(len(pos_idx) * self.undersample_ratio)
        np.random.seed(seed)
        sampled_neg_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
        sampled_idx = np.concatenate([pos_idx, sampled_neg_idx])
        np.random.shuffle(sampled_idx)
        return X.iloc[sampled_idx], y[sampled_idx]
    
    def train_stage1_xgboost(self):
        """Stage 1: XGBoost + Under-sampling + Multi-Seed Averaging"""
        print("\nğŸŒ¿ Stage 1: XGBoost + Under-sampling (1:2) + 3-Seed Averaging")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba = np.zeros(len(self.y))
        feature_importances = np.zeros(len(self.feature_names))
        
        # XGBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (æ”¹è‰¯ç‰ˆ)
        xgb_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'tree_method': 'hist',  # é«˜é€ŸåŒ–
            'enable_categorical': True,  # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°å¯¾å¿œ
            'max_depth': 8,
            'learning_rate': 0.05,
            'n_estimators': 1000,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'n_jobs': -1,
            'verbosity': 0,
            # ä¸å‡è¡¡å¯¾ç­–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            'scale_pos_weight': 2.0,  # è² ä¾‹ãŒæ­£ä¾‹ã®2å€ã‚ã‚‹ãŸã‚ã€ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹
            'max_delta_step': 1,      # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ™‚ã®æ›´æ–°ã‚’å®‰å®šã•ã›ã‚‹
        }
        
        self.stage1_models = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_train_full = self.X.iloc[train_idx]
            y_train_full = self.y[train_idx]
            X_val = self.X.iloc[val_idx]
            y_val = self.y[val_idx]
            
            fold_proba = np.zeros(len(val_idx))
            fold_models = []
            
            for seed_offset in range(self.n_seeds):
                seed = self.random_state + fold * 100 + seed_offset
                X_train_under, y_train_under = self.undersample(X_train_full, y_train_full, seed)
                
                model = xgb.XGBClassifier(
                    **xgb_params,
                    random_state=seed,
                    early_stopping_rounds=50  # Early Stoppingï¼ˆã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§æŒ‡å®šï¼‰
                )
                model.fit(
                    X_train_under, y_train_under,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )
                
                fold_proba += model.predict_proba(X_val)[:, 1] / self.n_seeds
                feature_importances += model.feature_importances_ / (self.n_folds * self.n_seeds)
                fold_models.append(model)
                
                del model
                gc.collect()
            
            self.oof_proba[val_idx] = fold_proba
            self.stage1_models.append(fold_models)
        
        # Feature Importance
        self.feature_importance_df = pd.DataFrame({
            'feature': self.feature_names, 'importance': feature_importances
        }).sort_values('importance', ascending=False)
        
        # OOFç²¾åº¦
        oof_pred = (self.oof_proba >= 0.5).astype(int)
        self.oof_auc = roc_auc_score(self.y, self.oof_proba)
        print(f"   OOF (é–¾å€¤0.5): Prec={precision_score(self.y, oof_pred):.4f}, Rec={recall_score(self.y, oof_pred):.4f}, AUC={self.oof_auc:.4f}")
    
    def find_recall_threshold(self):
        """Recallç›®æ¨™é–¾å€¤æ¢ç´¢"""
        for thresh in np.arange(0.50, 0.001, -0.005):
            y_pred = (self.oof_proba >= thresh).astype(int)
            recall = recall_score(self.y, y_pred)
            if recall >= self.stage1_recall_target:
                self.threshold = thresh
                break
        else:
            self.threshold = 0.001
        
        y_pred_final = (self.oof_proba >= self.threshold).astype(int)
        self.recall = recall_score(self.y, y_pred_final)
        self.precision = precision_score(self.y, y_pred_final)
        n_candidates = y_pred_final.sum()
        self.filter_rate = 1 - (n_candidates / len(self.y))
        
        print(f"\nğŸ“Š Recall {self.stage1_recall_target:.0%} è©•ä¾¡:")
        print(f"   é–¾å€¤: {self.threshold:.4f}")
        print(f"   Recall: {self.recall:.4f}")
        print(f"   é™¤å¤–ç‡: {self.filter_rate*100:.2f}% ({len(self.y) - n_candidates:,}ä»¶é™¤å¤–)")
    
    def analyze_correlation(self):
        """LightGBMã¨ã®ç›¸é–¢åˆ†æï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœè¦‹ç©ã‚‚ã‚Šï¼‰"""
        lgbm_path = "results/two_stage_model/lightgbm_stage1_oof.csv"
        
        if os.path.exists(lgbm_path):
            print("\nğŸ”— LightGBMã¨ã®ç›¸é–¢åˆ†æ...")
            lgbm_df = pd.read_csv(lgbm_path)
            lgbm_probs = lgbm_df['oof_proba'].values
            
            # ç›¸é–¢ä¿‚æ•°ã®ç®—å‡º
            corr = np.corrcoef(self.oof_proba, lgbm_probs)[0, 1]
            print(f"   äºˆæ¸¬å€¤ç›¸é–¢: {corr:.4f}")
            
            if corr < 0.95:
                print("   ğŸ‘‰ å¤šæ§˜æ€§ã‚ã‚Šï¼ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆStackingï¼‰ã§ç²¾åº¦å‘ä¸ŠãŒè¦‹è¾¼ã‚ã¾ã™ã€‚")
            else:
                print("   âš ï¸ æŒ™å‹•ãŒé…·ä¼¼ã—ã¦ã„ã¾ã™ã€‚å˜ç‹¬æ€§èƒ½ãŒé«˜ã„æ–¹ã‚’æ¡ç”¨ã—ã¦ãã ã•ã„ã€‚")
            
            self.lgbm_correlation = corr
        else:
            print(f"\nâš ï¸ LightGBMã®OOFçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {lgbm_path}")
            print("   ç›¸é–¢åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚LightGBMã®OOFçµæœã‚’ä¿å­˜ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            self.lgbm_correlation = None
    
    def save_results(self):
        """çµæœä¿å­˜ï¼ˆIDç´ä»˜ã‘å¯¾å¿œï¼‰"""
        # OOFç¢ºç‡ã¨ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜ï¼ˆIDç´ä»˜ã‘ï¼‰
        oof_df = pd.DataFrame({
            'y_true': self.y,
            'oof_proba': self.oof_proba
        })
        oof_df.to_csv(
            os.path.join(self.output_dir, "xgboost_stage1_oof.csv"),
            index=False
        )
        
        # ã‚µãƒãƒªãƒ¼çµæœ
        results = {
            'model': 'XGBoost',
            'oof_auc': self.oof_auc,
            'threshold': self.threshold,
            'recall': self.recall,
            'precision': self.precision,
            'filter_rate': self.filter_rate,
            'lgbm_correlation': self.lgbm_correlation if hasattr(self, 'lgbm_correlation') else None,
        }
        
        pd.DataFrame([results]).to_csv(
            os.path.join(self.output_dir, "xgboost_stage1_results.csv"),
            index=False
        )
        self.feature_importance_df.to_csv(
            os.path.join(self.output_dir, "xgboost_feature_importance.csv"),
            index=False
        )
        
        print(f"\nğŸ’¾ çµæœä¿å­˜: {self.output_dir}/")
        print("   - xgboost_stage1_oof.csv (OOFç¢ºç‡ã€ãƒ©ãƒ™ãƒ«)")
        print("   - xgboost_stage1_results.csv (ã‚µãƒãƒªãƒ¼)")
        print("   - xgboost_feature_importance.csv")
    
    def run(self):
        """å®Ÿé¨“å®Ÿè¡Œ"""
        start = datetime.now()
        
        self.load_data()
        self.train_stage1_xgboost()
        self.find_recall_threshold()
        self.analyze_correlation()
        self.save_results()
        
        elapsed = (datetime.now() - start).total_seconds()
        
        print("\n" + "=" * 60)
        print("âœ… å®Ÿé¨“å®Œäº†!")
        print(f"   å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}ç§’")
        print("=" * 60)
        
        return {
            'oof_auc': self.oof_auc,
            'threshold': self.threshold,
            'recall': self.recall,
            'filter_rate': self.filter_rate,
        }


if __name__ == "__main__":
    experiment = Stage1XGBoostExperiment()
    experiment.run()

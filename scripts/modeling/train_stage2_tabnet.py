"""
Stage 2 TabNet äºŒå€¤åˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
====================================
Stage 1: LightGBM (Binary) + Under-sampling + 3-Seed Averaging (æ—¢å­˜ã¨åŒã˜)
Stage 2: TabNet (Binary: 0=è² å‚·, 1=æ­»äº¡)

TabNetã®åˆ©ç‚¹:
- æ±ºå®šæœ¨ãŒè‹¦æ‰‹ãªã€Œè¤‡é›‘ãªè¡¨ç¾ã€ã®å­¦ç¿’
- Attentionæ©Ÿæ§‹ã«ã‚ˆã‚‹è§£é‡ˆå¯èƒ½æ€§
- è² å‚·äº‹æ•…ï¼ˆHard Negativeï¼‰ã¨æ­»äº¡äº‹æ•…ï¼ˆPositiveï¼‰ã®å¾®ç´°ãªé•ã„ã‚’æ•æ‰

ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† (TabNetç”¨):
- ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: OrdinalEncoder (æ•´æ•°ã¸å¤‰æ›)
- æ•°å€¤å¤‰æ•°: StandardScaler (æ­£è¦åŒ–)
"""

import pandas as pd
import numpy as np
import os
import gc
from datetime import datetime
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import (
    precision_score, recall_score, f1_score, roc_auc_score, 
    precision_recall_curve, accuracy_score, confusion_matrix,
)
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.impute import SimpleImputer
import torch
import lightgbm as lgb
from scipy.special import expit
import warnings

# TabNet
from pytorch_tabnet.tab_model import TabNetClassifier

warnings.filterwarnings('ignore')


# ============================================================================
# ãƒªãƒ¼ã‚¯é˜²æ­¢ãƒã‚§ãƒƒã‚¯é–¢æ•°
# ============================================================================
FORBIDDEN_COLUMNS = [
    'äº‹æ•…å†…å®¹',
    'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',
    'è² å‚·è€…æ•°',
    'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',
    'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Aï¼‰', 'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Bï¼‰',
    'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰', 'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
    'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰', 'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
]

def check_no_leakage(X: pd.DataFrame, context: str = ""):
    """ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒªãƒ¼ã‚¯åˆ—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª"""
    leaked = [col for col in FORBIDDEN_COLUMNS if col in X.columns]
    assert len(leaked) == 0, f"[LEAKAGE ERROR] {context}: ãƒªãƒ¼ã‚¯åˆ—ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {leaked}"
    print(f"   âœ… ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯é€šé ({context}): {len(X.columns)}åˆ—, ãƒªãƒ¼ã‚¯åˆ—ãªã—")


# ============================================================================
# ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ============================================================================
class TwoStageTabNetPipeline:
    """2æ®µéšãƒ¢ãƒ‡ãƒ« + TabNet Stage 2 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (è² å‚· vs æ­»äº¡)"""
    
    def __init__(
        self,
        features_path: str = "data/processed/honhyo_clean_with_features.csv",
        raw_data_path: str = "honhyo_all/csv/honhyo_all_with_datetime.csv",
        target_col: str = "æ­»è€…æ•°",
        n_folds: int = 5,
        random_state: int = 42,
        stage1_recall_target: float = 0.95,
        undersample_ratio: float = 2.0,
        n_seeds: int = 3,
        test_size: float = 0.2,
        # TabNetãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜ã‚¹ãƒšãƒƒã‚¯PCæœ€é©åŒ–ï¼‰
        tabnet_n_d: int = 16,
        tabnet_n_a: int = 16,
        tabnet_n_steps: int = 5,
        tabnet_gamma: float = 1.5,
        tabnet_batch_size: int = 4096,  # 64GB RAMæ´»ç”¨
        tabnet_virtual_batch_size: int = 128,
        tabnet_max_epochs: int = 100,
        tabnet_patience: int = 15,
    ):
        self.features_path = features_path
        self.raw_data_path = raw_data_path
        self.target_col = target_col
        self.n_folds = n_folds
        self.random_state = random_state
        self.stage1_recall_target = stage1_recall_target
        self.undersample_ratio = undersample_ratio
        self.n_seeds = n_seeds
        self.test_size = test_size
        
        # TabNet params
        self.tabnet_n_d = tabnet_n_d
        self.tabnet_n_a = tabnet_n_a
        self.tabnet_n_steps = tabnet_n_steps
        self.tabnet_gamma = tabnet_gamma
        self.tabnet_batch_size = tabnet_batch_size
        self.tabnet_virtual_batch_size = tabnet_virtual_batch_size
        self.tabnet_max_epochs = tabnet_max_epochs
        self.tabnet_patience = tabnet_patience
        
        self.output_dir = "results/two_stage_model/tabnet_pipeline"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨
        self.stage1_models = []
        self.stage2_models = []
        
        print("=" * 60)
        print("2æ®µéšãƒ¢ãƒ‡ãƒ« + TabNet ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (è² å‚· vs æ­»äº¡)")
        print(f"Stage 1: LightGBM 1:{int(self.undersample_ratio)} Under-sampling, Recall {self.stage1_recall_target:.0%}")
        print(f"Stage 2: TabNet (n_d={tabnet_n_d}, n_a={tabnet_n_a}, n_steps={tabnet_n_steps})")
        print(f"         Batch={tabnet_batch_size}, MaxEpochs={tabnet_max_epochs}")
        print(f"Test Set: {self.test_size:.0%}")
        print("=" * 60)
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å¤šã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ç”Ÿæˆ"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        # 1. ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df_features = pd.read_csv(self.features_path)
        print(f"   ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿: {len(df_features):,}ä»¶, {len(df_features.columns)}åˆ—")
        
        # 2. ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è² å‚·è€…æ•°ã®ã¿èª­ã¿è¾¼ã¿
        df_raw = pd.read_csv(self.raw_data_path, usecols=['è² å‚·è€…æ•°'])
        print(f"   ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆè² å‚·è€…æ•°ã®ã¿ï¼‰: {len(df_raw):,}ä»¶")
        
        # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        assert len(df_features) == len(df_raw), \
            f"[ERROR] è¡Œæ•°ä¸ä¸€è‡´: ç‰¹å¾´é‡={len(df_features)}, ç”Ÿãƒ‡ãƒ¼ã‚¿={len(df_raw)}"
        print("   âœ… è¡Œæ•°ä¸€è‡´ç¢ºèªå®Œäº†")
        
        # 4. å¤šã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
        y_fatal = (df_features[self.target_col] > 0).astype(int)
        y_injury = (df_raw['è² å‚·è€…æ•°'] > 0).astype(int)
        
        y_multiclass = np.zeros(len(df_features), dtype=np.int32)
        y_multiclass[y_fatal == 1] = 2  # æ­»äº¡
        y_multiclass[(y_fatal == 0) & (y_injury == 1)] = 1  # è² å‚·
        
        self.y_multiclass = y_multiclass
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒè¡¨ç¤º
        print("\nğŸ“Š å¤šã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for cls in [0, 1, 2]:
            count = (y_multiclass == cls).sum()
            pct = count / len(y_multiclass) * 100
            label = {0: "ç„¡å‚·/è»½å¾®", 1: "è² å‚·", 2: "æ­»äº¡"}[cls]
            print(f"   ã‚¯ãƒ©ã‚¹ {cls} ({label}): {count:,} ({pct:.2f}%)")
        
        # 5. äºŒå€¤ãƒ©ãƒ™ãƒ« (Stage 1ç”¨)
        self.y_binary = (df_features[self.target_col] > 0).astype(int).values
        
        # 6. ç‰¹å¾´é‡æŠ½å‡º
        X_all = df_features.drop(columns=[self.target_col])
        if 'ç™ºç”Ÿæ—¥æ™‚' in X_all.columns:
            X_all = X_all.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        # 7. ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯
        check_no_leakage(X_all, "ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¾Œ")
        
        # Train/Teståˆ†å‰²
        self.X, self.X_test, self.y_mc, self.y_mc_test, \
        self.y_bin, self.y_bin_test = train_test_split(
            X_all, self.y_multiclass, self.y_binary,
            test_size=self.test_size, 
            random_state=self.random_state, 
            stratify=self.y_multiclass
        )
        
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (Train: {1-self.test_size:.0%} / Test: {self.test_size:.0%})")
        print(f"   Train: {len(self.y_mc):,} (æ­»äº¡: {(self.y_mc==2).sum():,})")
        print(f"   Test:  {len(self.y_mc_test):,} (æ­»äº¡: {(self.y_mc_test==2).sum():,})")
        
        # ã‚«ãƒ†ã‚´ãƒª/æ•°å€¤å¤‰æ•°ã®ç‰¹å®š
        known_categoricals = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
            'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
            'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
            'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
            'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
        ]
        
        self.categorical_cols = []
        self.numeric_cols = []
        
        for col in self.X.columns:
            if col in known_categoricals or self.X[col].dtype == 'object':
                self.categorical_cols.append(col)
            else:
                self.numeric_cols.append(col)
        
        self.feature_names = list(self.X.columns)
        
        # LightGBMç”¨: categoryå‹ã«å¤‰æ›
        self.X_lgb = self.X.copy()
        self.X_test_lgb = self.X_test.copy()
        for col in self.categorical_cols:
            self.X_lgb[col] = self.X_lgb[col].astype('category')
            self.X_test_lgb[col] = self.X_test_lgb[col].astype('category')
        for col in self.numeric_cols:
            self.X_lgb[col] = self.X_lgb[col].astype(np.float32)
            self.X_test_lgb[col] = self.X_test_lgb[col].astype(np.float32)
        
        # ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯
        check_no_leakage(self.X, "Train/Teståˆ†å‰²å¾Œ (Train)")
        check_no_leakage(self.X_test, "Train/Teståˆ†å‰²å¾Œ (Test)")
        
        # [TabNetç”¨] å…¨ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãOrdinalEncoderã¨cat_dimsã‚’äº‹å‰è¨ˆç®—
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã€Train+Testã®å…¨ã‚«ãƒ†ã‚´ãƒªã‚’å­¦ç¿’
        print("\nğŸ“¦ TabNetç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’äº‹å‰å­¦ç¿’ä¸­...")
        X_all_for_encoder = pd.concat([self.X, self.X_test], axis=0)
        X_cat_all = X_all_for_encoder[self.categorical_cols].astype(str).fillna('__MISSING__')
        
        self.global_ordinal_encoder = OrdinalEncoder(
            handle_unknown='use_encoded_value', unknown_value=-1
        )
        X_cat_all_encoded = self.global_ordinal_encoder.fit_transform(X_cat_all)
        
        # cat_idxsã¨cat_dimsã‚’äº‹å‰è¨ˆç®— (+1 shiftã‚’è€ƒæ…®)
        self.global_cat_idxs = list(range(X_cat_all_encoded.shape[1]))
        self.global_cat_dims = [
            int(X_cat_all_encoded[:, i].max() + 3)  # +1 shift + unknown + ä½™è£•
            for i in range(X_cat_all_encoded.shape[1])
        ]
        print(f"   Cat Features: {len(self.global_cat_idxs)}")
        print(f"   Cat Dims: {self.global_cat_dims}")
        
        gc.collect()
    
    def train_stage1(self):
        """Stage 1: LightGBM (Binary) + Under-sampling + Multi-Seed"""
        print(f"\nğŸŒ¿ Stage 1: LightGBM (Binary) + Under-sampling (1:{int(self.undersample_ratio)}) + {self.n_seeds}-Seed Averaging")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba_stage1 = np.zeros(len(self.y_bin))
        self.oof_logits_stage1 = np.zeros(len(self.y_bin))
        feature_importances = np.zeros(len(self.feature_names))
        
        lgb_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'num_leaves': 31,
            'max_depth': 8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'is_unbalance': False,
            'n_estimators': 1000,
            'learning_rate': 0.05,
            'n_jobs': -1
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X_lgb, self.y_bin)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_train_full = self.X_lgb.iloc[train_idx]
            X_val = self.X_lgb.iloc[val_idx]
            y_train_full = self.y_bin[train_idx]
            y_val = self.y_bin[val_idx]
            
            fold_models = []
            fold_logits = np.zeros(len(val_idx))
            
            for seed in range(self.n_seeds):
                np.random.seed(self.random_state + seed)
                
                # Under-sampling
                pos_idx = np.where(y_train_full == 1)[0]
                neg_idx = np.where(y_train_full == 0)[0]
                n_pos = len(pos_idx)
                n_neg_sample = int(n_pos * self.undersample_ratio)
                neg_sample_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
                
                train_idx_sampled = np.concatenate([pos_idx, neg_sample_idx])
                X_train = X_train_full.iloc[train_idx_sampled]
                y_train = y_train_full[train_idx_sampled]
                
                model = lgb.LGBMClassifier(**lgb_params, random_state=self.random_state + seed)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(50, verbose=False)]
                )
                
                # Logitså–å¾—
                raw_score = model.predict_proba(X_val)[:, 1]
                raw_score = np.clip(raw_score, 1e-15, 1 - 1e-15)
                logits = np.log(raw_score / (1 - raw_score))
                fold_logits += logits / self.n_seeds
                fold_models.append(model)
                feature_importances += model.feature_importances_ / (self.n_folds * self.n_seeds)
            
            self.oof_logits_stage1[val_idx] = fold_logits
            self.oof_proba_stage1[val_idx] = expit(fold_logits)
            self.stage1_models.append(fold_models)
            
            del X_train, X_val
            gc.collect()
        
        # Feature Importance
        self.feature_importance_df = pd.DataFrame({
            'feature': self.feature_names, 'importance': feature_importances
        }).sort_values('importance', ascending=False)
        self.top_features = self.feature_importance_df.head(10)['feature'].tolist()
        
        # OOFè©•ä¾¡
        oof_pred = (self.oof_proba_stage1 >= 0.5).astype(int)
        oof_auc = roc_auc_score(self.y_bin, self.oof_proba_stage1)
        print(f"   OOF (é–¾å€¤0.5): Prec={precision_score(self.y_bin, oof_pred):.4f}, "
              f"Rec={recall_score(self.y_bin, oof_pred):.4f}, AUC={oof_auc:.4f}")
    
    def find_recall_threshold(self):
        """Recallç›®æ¨™ã‚’é”æˆã™ã‚‹é–¾å€¤ã‚’æ¢ç´¢"""
        for thresh in np.arange(0.5, 0.0, -0.001):
            y_pred = (self.oof_proba_stage1 >= thresh).astype(int)
            recall = recall_score(self.y_bin, y_pred)
            if recall >= self.stage1_recall_target:
                self.threshold_stage1 = thresh
                break
        else:
            self.threshold_stage1 = 0.001
        
        y_pred_final = (self.oof_proba_stage1 >= self.threshold_stage1).astype(int)
        self.stage1_recall = recall_score(self.y_bin, y_pred_final)
        n_candidates = y_pred_final.sum()
        self.filter_rate = 1 - (n_candidates / len(self.y_bin))
        n_filtered = len(self.y_bin) - n_candidates
        
        print(f"   é–¾å€¤: {self.threshold_stage1:.4f}, Recall: {self.stage1_recall:.4f}")
        print(f"   [Result] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {n_filtered:,} ä»¶é™¤å¤– ({self.filter_rate:.2%})")
        print(f"   [Result] æ®‹å­˜ãƒ‡ãƒ¼ã‚¿: {n_candidates:,} ä»¶ (Stage 2 å€™è£œ)")
        print(f"   [Result] æ­»äº¡äº‹ä¾‹æ®‹å­˜: {self.y_bin[self.oof_proba_stage1 >= self.threshold_stage1].sum():,} / {self.y_bin.sum():,}")
        
        self.stage2_mask = self.oof_proba_stage1 >= self.threshold_stage1
        
        # Stage 1 é€šéç‡ (ã‚¯ãƒ©ã‚¹åˆ¥)
        passed_counts = np.bincount(self.y_mc[self.stage2_mask], minlength=3)
        total_counts = np.bincount(self.y_mc, minlength=3)
        
        print("\n   [Check] Stage 1 é€šéç‡ (ã‚¯ãƒ©ã‚¹åˆ¥):")
        class_labels = {0: "ç„¡å‚·/è»½å¾®", 1: "è² å‚·", 2: "æ­»äº¡"}
        self.class_pass_rates = {}
        for cls in [0, 1, 2]:
            ratio = passed_counts[cls] / total_counts[cls] if total_counts[cls] > 0 else 0
            self.class_pass_rates[cls] = ratio
            print(f"     Class {cls} ({class_labels[cls]}): {passed_counts[cls]:,} / {total_counts[cls]:,} ({ratio:.1%})")
    
    def _prepare_tabnet_data(self, X_subset, logits_stage1_subset, fit=True):
        """
        TabNetç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        
        - æ•°å€¤å¤‰æ•°: SimpleImputer(mean) -> StandardScaler
        - ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: äº‹å‰è¨ˆç®—æ¸ˆã¿global_ordinal_encoder -> +1 shift (unknownå¯¾ç­–)
        - logits_stage1ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
        
        æ³¨æ„: OrdinalEncoderã¯å…¨ãƒ‡ãƒ¼ã‚¿(Train+Test)ã§äº‹å‰å­¦ç¿’æ¸ˆã¿
        """
        X_out = X_subset.copy().reset_index(drop=True)
        
        # logits_stage1 è¿½åŠ 
        X_out['logits_stage1'] = logits_stage1_subset
        numeric_cols_extended = self.numeric_cols + ['logits_stage1']
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®æ¬ æã‚’æ–‡å­—åˆ—ã¨ã—ã¦åŸ‹ã‚ã‚‹
        X_cat = X_out[self.categorical_cols].astype(str).fillna('__MISSING__')
        # æ•°å€¤å¤‰æ•°ã‚’æŠ½å‡º
        X_num = X_out[numeric_cols_extended]
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: äº‹å‰å­¦ç¿’æ¸ˆã¿global_ordinal_encoderã‚’ä½¿ç”¨
        X_cat_encoded = self.global_ordinal_encoder.transform(X_cat)
        
        if fit:
            # æ•°å€¤å¤‰æ•°: Mean Imputation (Planæº–æ‹ )
            self.num_imputer = SimpleImputer(strategy='mean')
            X_num_imputed = self.num_imputer.fit_transform(X_num)
            
            # æ•°å€¤å¤‰æ•°: StandardScaler
            self.scaler = StandardScaler()
            X_num_scaled = self.scaler.fit_transform(X_num_imputed)
            
            # cat_idxsã¨cat_dimsã¯äº‹å‰è¨ˆç®—æ¸ˆã¿ã®globalã‚’ä½¿ç”¨
            self.cat_idxs = self.global_cat_idxs
            self.cat_dims = self.global_cat_dims
            
        else:
            # Transform
            X_num_imputed = self.num_imputer.transform(X_num)
            X_num_scaled = self.scaler.transform(X_num_imputed)
        
        # [CRITICAL FIX] æœªçŸ¥ã®ã‚«ãƒ†ã‚´ãƒª (-1) ã‚’ 0 ã«ã€ä»–ã‚’ +1 ã‚·ãƒ•ãƒˆã—ã¦æ­£ã®æ•´æ•°ã«ã™ã‚‹
        X_cat_encoded = X_cat_encoded + 1
        
        # çµåˆ: [cat_encoded, num_scaled]
        X_combined = np.hstack([X_cat_encoded, X_num_scaled]).astype(np.float32)
        
        return X_combined
    
    def train_stage2_tabnet(self):
        """Stage 2: TabNet äºŒå€¤åˆ†é¡"""
        print("\nğŸŒ¿ Stage 2: TabNet Binary Classification (5-Fold CV)")
        print(f"   TabNet: n_d={self.tabnet_n_d}, n_a={self.tabnet_n_a}, n_steps={self.tabnet_n_steps}")
        print(f"   Batch={self.tabnet_batch_size}, MaxEpochs={self.tabnet_max_epochs}")
        
        # ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"   ğŸš€ Using Device: {device}")
        if device == 'cpu':
            print("   âš ï¸ GPU not detected. Training might be slower. Adjust batch_size if RAM fills up.")
        
        # Stage 2ç”¨ãƒ‡ãƒ¼ã‚¿
        X_s2_raw = self.X[self.stage2_mask].copy()
        logits_s2 = self.oof_logits_stage1[self.stage2_mask]
        y_s2_mc = self.y_mc[self.stage2_mask]
        y_s2_binary = (y_s2_mc == 2).astype(int)  # æ­»äº¡=1, è² å‚·=0
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        n_pos = y_s2_binary.sum()
        n_neg = len(y_s2_binary) - n_pos
        print(f"   Stage 2 ãƒ‡ãƒ¼ã‚¿: {len(y_s2_binary):,}")
        print(f"      è² å‚· (Class 0): {n_neg:,}")
        print(f"      æ­»äº¡ (Class 1): {n_pos:,}")
        print(f"      æ­£ä¾‹æ¯”ç‡: {n_pos / len(y_s2_binary) * 100:.2f}%")
        
        # OOFäºˆæ¸¬ä¿å­˜
        self.oof_proba_stage2 = np.zeros(len(y_s2_binary))
        self.stage2_models = []
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_s2_raw, y_s2_binary)):
            print(f"\n   {'='*50}")
            print(f"   ğŸ“‚ Fold {fold+1}/{self.n_folds}")
            print(f"   {'='*50}")
            
            X_train_raw = X_s2_raw.iloc[train_idx]
            X_val_raw = X_s2_raw.iloc[val_idx]
            logits_train = logits_s2[train_idx]
            logits_val = logits_s2[val_idx]
            y_train = y_s2_binary[train_idx]
            y_val = y_s2_binary[val_idx]
            
            print(f"   Train: {len(y_train):,} (Pos: {y_train.sum():,}, Neg: {len(y_train)-y_train.sum():,})")
            print(f"   Val:   {len(y_val):,} (Pos: {y_val.sum():,}, Neg: {len(y_val)-y_val.sum():,})")
            
            # TabNetç”¨å‰å‡¦ç†
            X_train = self._prepare_tabnet_data(X_train_raw, logits_train, fit=True)
            X_val = self._prepare_tabnet_data(X_val_raw, logits_val, fit=False)
            
            print(f"   Features: {X_train.shape[1]} (Cat: {len(self.cat_idxs)}, Num: {X_train.shape[1]-len(self.cat_idxs)})")
            
            # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
            n_pos_fold = y_train.sum()
            n_neg_fold = len(y_train) - n_pos_fold
            weight_for_1 = n_neg_fold / n_pos_fold if n_pos_fold > 0 else 1.0
            
            print(f"\n   ğŸš€ TabNetå­¦ç¿’é–‹å§‹...")
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‘ã‚¹
            model_dir = "results/models/tabnet_stage2"
            os.makedirs(model_dir, exist_ok=True)
            model_path = os.path.join(model_dir, f"tabnet_fold{fold+1}")  # save_modelãŒ.zipã‚’è‡ªå‹•è¿½åŠ 
            
            # TabNetãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            model = TabNetClassifier(
                n_d=self.tabnet_n_d,
                n_a=self.tabnet_n_a,
                n_steps=self.tabnet_n_steps,
                gamma=self.tabnet_gamma,
                cat_idxs=self.cat_idxs,
                cat_dims=self.cat_dims,
                cat_emb_dim=1,  # è»½é‡åŒ–
                optimizer_fn=torch.optim.Adam,
                optimizer_params={'lr': 0.02, 'weight_decay': 1e-5},
                scheduler_fn=torch.optim.lr_scheduler.StepLR,
                scheduler_params={'step_size': 10, 'gamma': 0.9},
                mask_type='sparsemax',
                verbose=2,  # æœ€å¤§è©³ç´°ãƒ¬ãƒ™ãƒ«
                seed=self.random_state,
            )
            
            # é€”ä¸­å†é–‹ãƒ­ã‚¸ãƒƒã‚¯
            model_file = model_path + ".zip"  # save_modelãŒä½œæˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
            if os.path.exists(model_file):
                print(f"   ğŸ“¥ æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™: {model_file}")
                model.load_model(model_file)
            else:
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    eval_metric=['auc'],
                    max_epochs=self.tabnet_max_epochs,
                    patience=self.tabnet_patience,
                    batch_size=self.tabnet_batch_size,
                    virtual_batch_size=self.tabnet_virtual_batch_size,
                    weights=1,
                )
                model.save_model(model_path)
                print(f"   ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")
            
            # ãƒ™ã‚¹ãƒˆã‚¨ãƒãƒƒã‚¯è¡¨ç¤º (ãƒ­ãƒ¼ãƒ‰æ™‚ã¯best_epochãŒå­˜åœ¨ã—ãªã„)
            if hasattr(model, 'best_epoch') and model.best_epoch is not None:
                print(f"\n   âœ… Fold {fold+1} å®Œäº†: Best Epoch = {model.best_epoch}")
            else:
                print(f"\n   âœ… Fold {fold+1} å®Œäº†: (ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«)")
            
            # OOFäºˆæ¸¬
            proba = model.predict_proba(X_val)[:, 1]
            self.oof_proba_stage2[val_idx] = proba
            
            # Foldè©•ä¾¡
            from sklearn.metrics import roc_auc_score as auc_score
            fold_auc = auc_score(y_val, proba)
            print(f"   ğŸ“Š Fold {fold+1} Val AUC: {fold_auc:.4f}")
            
            self.stage2_models.append(model)
            
            del X_train, X_val
            gc.collect()
        
        # Stage 2 OOFè©•ä¾¡
        oof_pred = (self.oof_proba_stage2 >= 0.5).astype(int)
        oof_acc = accuracy_score(y_s2_binary, oof_pred)
        print(f"\n   Stage 2 OOF Accuracy: {oof_acc:.4f}")
        print(f"   Confusion Matrix:\n{confusion_matrix(y_s2_binary, oof_pred)}")
        
        # AUCè©•ä¾¡
        auc_fatal = roc_auc_score(y_s2_binary, self.oof_proba_stage2)
        print(f"   Fatal AUC: {auc_fatal:.4f}")
        print(f"   Precision(0.5): {precision_score(y_s2_binary, oof_pred):.4f}")
        print(f"   Recall(0.5): {recall_score(y_s2_binary, oof_pred):.4f}")
        
        self.y_s2_binary = y_s2_binary
    
    def evaluate(self):
        """æœ€çµ‚è©•ä¾¡ (CV OOF) - Binary Classification with Dynamic Threshold"""
        print("\nğŸ“ˆ æœ€çµ‚è©•ä¾¡ (Cross Validation OOF)")
        
        y_s2_bin = self.y_s2_binary
        prob_fatal = self.oof_proba_stage2
        
        # Precision-Recall Curve
        precisions, recalls, thresholds = precision_recall_curve(y_s2_bin, prob_fatal)
        
        # 1. Best F1 Score æ¢ç´¢
        numerator = 2 * precisions * recalls
        denominator = precisions + recalls
        f1_scores = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)
        
        best_f1_idx = np.argmax(f1_scores)
        self.best_f1_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5
        best_f1 = f1_scores[best_f1_idx]
        best_prec = precisions[best_f1_idx]
        best_rec = recalls[best_f1_idx]
        
        print(f"\n   ğŸ† Best F1 Score: {best_f1:.4f} (é–¾å€¤: {self.best_f1_threshold:.4f})")
        print(f"      Precision: {best_prec:.4f}, Recall: {best_rec:.4f}")
        
        # Confusion Matrix at Best Threshold
        y_pred_best = (prob_fatal >= self.best_f1_threshold).astype(int)
        conf_mat = confusion_matrix(y_s2_bin, y_pred_best)
        print(f"      Confusion Matrix:\n{conf_mat}")
        
        # 2. Recall Oriented Thresholds
        self.dynamic_results = {}
        target_recalls = [0.99, 0.98, 0.95]
        
        print("\n   ğŸ“Š Recallé‡è¦–ã®è©•ä¾¡:")
        for target_recall in target_recalls:
            idx = np.where(recalls >= target_recall)[0]
            if len(idx) > 0:
                idx = idx[-1]
                thresh = thresholds[idx] if idx < len(thresholds) else 0.0
                prec = precisions[idx]
            else:
                thresh = 0.0
                prec = 0.0
            
            self.dynamic_results[target_recall] = {'threshold': thresh, 'precision': prec}
            print(f"      Recall ~{target_recall:.0%}: é–¾å€¤={thresh:.4f}, Precision={prec:.4f}")
        
        # Global Metrics (Best F1 Threshold)
        y_bin_all = (self.y_mc == 2).astype(int)
        final_proba = np.zeros(len(self.y_mc))
        final_proba[self.stage2_mask] = prob_fatal
        y_pred_global = (final_proba >= self.best_f1_threshold).astype(int)
        
        self.final_precision = precision_score(y_bin_all, y_pred_global)
        self.final_recall = recall_score(y_bin_all, y_pred_global)
        self.final_f1 = f1_score(y_bin_all, y_pred_global)
        self.final_auc = roc_auc_score(y_bin_all, final_proba)
        
        print(f"\n   [å…¨ä½“è©•ä¾¡ @ Best Thresh] Precision: {self.final_precision:.4f}, Recall: {self.final_recall:.4f}, F1: {self.final_f1:.4f}")
        print(f"   [å…¨ä½“AUC]: {self.final_auc:.4f}")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨OOFä¿å­˜
        oof_df = pd.DataFrame({
            'index': self.X[self.stage2_mask].index,
            'true_label': y_s2_bin,
            'prob': prob_fatal
        })
        os.makedirs('results/oof', exist_ok=True)
        oof_df.to_csv('results/oof/oof_stage2_tabnet.csv', index=False)
        print("\n   ğŸ’¾ OOFäºˆæ¸¬ã‚’ä¿å­˜ã—ã¾ã—ãŸ: results/oof/oof_stage2_tabnet.csv")
        
        return {
            'stage1_threshold': self.threshold_stage1,
            'stage1_recall': self.stage1_recall,
            'filter_rate': self.filter_rate,
            'best_f1_threshold': self.best_f1_threshold,
            'best_f1': best_f1,
            'best_f1_precision': best_prec,
            'best_f1_recall': best_rec,
            'final_precision': self.final_precision,
            'final_recall': self.final_recall,
            'final_f1': self.final_f1,
            'final_auc': self.final_auc,
            'recall_99_precision': self.dynamic_results[0.99]['precision'],
            'recall_95_precision': self.dynamic_results[0.95]['precision'],
        }
    
    def evaluate_test_set(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®æœ€çµ‚è©•ä¾¡"""
        print("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ (Hold-Out)")
        
        # Stage 1: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        test_logits_stage1 = np.zeros(len(self.y_mc_test))
        for fold_models in self.stage1_models:
            for model in fold_models:
                proba = model.predict_proba(self.X_test_lgb)[:, 1]
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
                logits = np.log(proba / (1 - proba))
                test_logits_stage1 += logits
        test_logits_stage1 /= (self.n_folds * self.n_seeds)
        test_proba_stage1 = expit(test_logits_stage1)
        
        # Stage 1é–¾å€¤é©ç”¨
        test_stage2_mask = test_proba_stage1 >= self.threshold_stage1
        n_candidates = test_stage2_mask.sum()
        n_fatal_in_candidates = (self.y_mc_test[test_stage2_mask] == 2).sum()
        
        print(f"   Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {n_candidates:,} / {len(self.y_mc_test):,}")
        print(f"   æ­»äº¡äº‹ä¾‹æ®‹å­˜: {n_fatal_in_candidates:,} / {(self.y_mc_test==2).sum():,}")
        
        if n_candidates == 0:
            print("   âš ï¸ Stage 2ã«é€²ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return {'error': 'No candidates after Stage 1'}
        
        # Stage 2ç”¨ç‰¹å¾´é‡
        X_test_s2_raw = self.X_test[test_stage2_mask].copy()
        logits_test_s2 = test_logits_stage1[test_stage2_mask]
        y_test_s2_mc = self.y_mc_test[test_stage2_mask]
        y_test_bin = (y_test_s2_mc == 2).astype(int)
        
        # TabNetç”¨å‰å‡¦ç† (fit=False: å­¦ç¿’æ™‚ã®encoderã‚’ä½¿ç”¨)
        X_test_s2 = self._prepare_tabnet_data(X_test_s2_raw, logits_test_s2, fit=False)
        
        # Stage 2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        test_proba_stage2 = np.zeros(len(y_test_bin))
        for model in self.stage2_models:
            proba = model.predict_proba(X_test_s2)[:, 1]
            test_proba_stage2 += proba / self.n_folds
        
        # ãƒ†ã‚¹ãƒˆè©•ä¾¡
        prob_fatal = test_proba_stage2
        
        # å‹•çš„é–¾å€¤è©•ä¾¡
        precisions, recalls, thresholds = precision_recall_curve(y_test_bin, prob_fatal)
        
        self.test_dynamic_results = {}
        target_recalls = [0.99, 0.98, 0.95]
        
        print("\n   ğŸ“Š ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå‹•çš„é–¾å€¤è©•ä¾¡:")
        for target_recall in target_recalls:
            idx = np.where(recalls >= target_recall)[0]
            if len(idx) > 0:
                idx = idx[-1]
                if idx < len(thresholds):
                    best_thresh = thresholds[idx]
                    best_prec = precisions[idx]
                else:
                    best_thresh = 0.0
                    best_prec = precisions[-1]
            else:
                best_thresh = 0.0
                best_prec = 0.0
            
            self.test_dynamic_results[target_recall] = {
                'threshold': best_thresh,
                'precision': best_prec
            }
            print(f"      Recall ~{target_recall:.0%}: é–¾å€¤={best_thresh:.4f}, Precision={best_prec:.4f}")
        
        # CVã§ã®æœ€é©é–¾å€¤ã‚’é©ç”¨
        cv_best_thresh = self.best_f1_threshold
        
        # Testã‚»ãƒƒãƒˆã§ã® Best F1 ã‚‚æ¢ç´¢ï¼ˆæ¯”è¼ƒç”¨ï¼‰
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-15)
        test_best_idx = np.argmax(f1_scores)
        test_best_f1 = f1_scores[test_best_idx]
        test_best_thresh = thresholds[test_best_idx] if test_best_idx < len(thresholds) else 0.5
        
        print(f"   ğŸ† Test Best F1: {test_best_f1:.4f} (Ideal Threshold: {test_best_thresh:.4f})")
        
        # CVé–¾å€¤ã§ã®è©•ä¾¡
        y_test_pred_cv = (prob_fatal >= cv_best_thresh).astype(int)
        conf_mat = confusion_matrix(y_test_bin, y_test_pred_cv)
        print(f"\n   [CVé–¾å€¤é©ç”¨ ({cv_best_thresh:.4f})] Confusion Matrix:\n{conf_mat}")
        
        # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (CVé–¾å€¤)
        final_test_proba = np.zeros(len(self.y_mc_test))
        final_test_proba[test_stage2_mask] = prob_fatal
        y_test_pred_global = (final_test_proba >= cv_best_thresh).astype(int)
        y_test_all_bin = (self.y_mc_test == 2).astype(int)
        
        test_precision = precision_score(y_test_all_bin, y_test_pred_global)
        test_recall = recall_score(y_test_all_bin, y_test_pred_global)
        test_f1 = f1_score(y_test_all_bin, y_test_pred_global)
        test_auc = roc_auc_score(y_test_all_bin, final_test_proba)
        
        print(f"   [å…¨ä½“è©•ä¾¡] Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")
        print(f"   [å…¨ä½“AUC]: {test_auc:.4f}")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨Testäºˆæ¸¬ä¿å­˜
        test_df = pd.DataFrame({
            'index': self.X_test[test_stage2_mask].index,
            'true_label': y_test_bin,
            'prob': prob_fatal
        })
        os.makedirs('results/test_preds', exist_ok=True)
        test_df.to_csv('results/test_preds/test_stage2_tabnet.csv', index=False)
        print("\n   ğŸ’¾ Testäºˆæ¸¬ã‚’ä¿å­˜ã—ã¾ã—ãŸ: results/test_preds/test_stage2_tabnet.csv")
        
        return {
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'test_auc': test_auc,
            'cv_threshold_used': cv_best_thresh,
            'ideal_test_best_f1': test_best_f1
        }
    
    def generate_report(self, results: dict, elapsed_sec: float):
        """å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownã§å‡ºåŠ›"""
        report_path = os.path.join(self.output_dir, "experiment_report.md")
        
        report_content = f"""# TabNet Stage 2 å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å®Ÿè¡Œæ™‚é–“**: {elapsed_sec:.1f}ç§’

## ãƒ¢ãƒ‡ãƒ«æ§‹æˆ
- **Stage 1**: LightGBM Binary Classification (æ­»äº¡ vs ãã®ä»–)
- **Stage 2**: TabNet Binary Classification (è² å‚· vs æ­»äº¡)
- **TabNetè¨­å®š**: n_d={self.tabnet_n_d}, n_a={self.tabnet_n_a}, n_steps={self.tabnet_n_steps}
- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: {self.tabnet_batch_size}

## çµæœã‚µãƒãƒª

### Stage 1 (Recall {self.stage1_recall_target:.0%})
- **é–¾å€¤**: {results['stage1_threshold']:.4f}
- **Recall**: {results['stage1_recall']:.4f}
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‡**: {results['filter_rate']*100:.2f}%
- **è² å‚·äº‹æ•…(Class 1) é€šéç‡**: {self.class_pass_rates.get(1, 0)*100:.1f}%

### Stage 2 TabNet (CV OOF)

**Best F1 é–¾å€¤ ({results['best_f1_threshold']:.4f}) ã§ã®è©•ä¾¡**:
| æŒ‡æ¨™ | å€¤ |
|------|----| 
| F1 Score | {results['best_f1']:.4f} |
| Precision | {results['best_f1_precision']:.4f} |
| Recall | {results['best_f1_recall']:.4f} |

**Overall Metrics (å…¨ä½“ã«å¯¾ã™ã‚‹è©•ä¾¡)**:
| æŒ‡æ¨™ | å€¤ |
|------|----| 
| Final Precision | {results['final_precision']:.4f} |
| Final Recall | {results['final_recall']:.4f} |
| Final F1 | {results['final_f1']:.4f} |
| AUC | {results['final_auc']:.4f} |

### ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ (Hold-Out {self.test_size:.0%})

**CVæœ€é©é–¾å€¤ ({results.get('cv_threshold_used', 0):.4f}) ã‚’é©ç”¨**:
| æŒ‡æ¨™ | å€¤ |
|------|----| 
| Precision | {results.get('test_precision', 0):.4f} |
| Recall | {results.get('test_recall', 0):.4f} |
| F1 | {results.get('test_f1', 0):.4f} |
| AUC | {results.get('test_auc', 0):.4f} |

**å‚è€ƒ: Test Ideal F1**: {results.get('ideal_test_best_f1', 0):.4f}

## è€ƒå¯Ÿ

- TabNetã¯Attentionæ©Ÿæ§‹ã«ã‚ˆã‚Šã€æ±ºå®šæœ¨ã§ã¯æ‰ãˆã«ãã„è¤‡é›‘ãªç‰¹å¾´è¡¨ç¾ã‚’å­¦ç¿’
- è² å‚·äº‹æ•…ï¼ˆHard Negativesï¼‰ã¨æ­»äº¡äº‹æ•…ã®å¾®ç´°ãªé•ã„ã‚’Deep Learningã®è¡¨ç¾åŠ›ã§è­˜åˆ¥
- LightGBMã¨ã®æ¯”è¼ƒã§ã€Precision/Recallã®æ”¹å–„ã‚’ç¢ºèªã™ã‚‹å¿…è¦ã‚ã‚Š
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\n   ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {report_path}")
        return report_path
    
    def run(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        start = datetime.now()
        self.load_data()
        self.train_stage1()
        self.find_recall_threshold()
        self.train_stage2_tabnet()
        results = self.evaluate()
        
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡
        test_results = self.evaluate_test_set()
        results.update(test_results)
        
        elapsed_sec = (datetime.now() - start).total_seconds()
        results['elapsed_sec'] = elapsed_sec
        
        # çµæœä¿å­˜
        pd.DataFrame([results]).to_csv(os.path.join(self.output_dir, "final_results.csv"), index=False)
        self.feature_importance_df.to_csv(os.path.join(self.output_dir, "stage1_feature_importance.csv"), index=False)
        
        # Markdown ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_report(results, elapsed_sec)
        
        print("\n" + "=" * 60)
        print("âœ… å®Œäº†ï¼")
        print(f"   çµæœCSV: {self.output_dir}/final_results.csv")
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆMD: {self.output_dir}/experiment_report.md")
        print("=" * 60)
        
        return results


if __name__ == "__main__":
    pipeline = TwoStageTabNetPipeline()
    pipeline.run()

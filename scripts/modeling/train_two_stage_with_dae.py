"""
2æ®µéšãƒ¢ãƒ‡ãƒ« + DAEç‰¹å¾´é‡çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
========================================
Stage 1: LightGBM + 1:2 Under-sampling + 3-Seed Averaging
Stage 2: LightGBM + Focal Loss + DAEç‰¹å¾´é‡

DAE (Denoising Autoencoder) ã«ã‚ˆã‚‹ç‰¹å¾´é‡æŠ½å‡º:
- CVã®å„Foldå†…ã§DAEã‚’å­¦ç¿’ã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡ (128æ¬¡å…ƒ) ã‚’æŠ½å‡º
- ãƒªãƒ¼ã‚¯é˜²æ­¢: DAEã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã—ã€æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯å¤‰æ›ã®ã¿é©ç”¨

æ³¨æ„:
- Focal Lossä½¿ç”¨æ™‚ã®äºˆæ¸¬å€¤ã¯ã€å®Ÿéš›ã®ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿç¢ºç‡ã¨ã¯ä¹–é›¢ã—ã¾ã™ã€‚
  ãƒ“ã‚¸ãƒã‚¹ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€Œã‚¹ã‚³ã‚¢ã€ã¨ã—ã¦æ‰±ã†ã‹ã€Isotonic Regressionç­‰ã§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
"""

import pandas as pd
import numpy as np
import os
import gc
from datetime import datetime
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve
import lightgbm as lgb
from scipy.special import expit
import warnings

# DAE Feature Extractor (local import)
from dae_feature_extractor import DAEFeatureExtractor

warnings.filterwarnings('ignore')


def get_focal_loss_lgb(alpha: float = 0.75, gamma: float = 1.0):
    """
    LightGBMç”¨ Focal Loss ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°
    
    Args:
        alpha: æ­£ä¾‹(æ­»äº¡äº‹æ•…)ã®é‡ã¿ (0.5ã‚ˆã‚Šå¤§ãã„ã¨æ­£ä¾‹ã‚’é‡è¦–)
        gamma: é›£æ˜“åº¦ã«å¿œã˜ãŸé‡ã¿ä»˜ã‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0ã§é€šå¸¸ã®CE, å¤§ãã„ã»ã©é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’é‡è¦–)
    
    Returns:
        focal_loss_lgb: LightGBMç”¨ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°
    """
    def focal_loss_lgb(y_true, preds):
        """
        LightGBMç”¨ Focal Loss
        
        æ³¨æ„: LGBMClassifier (sklearn API) ã§ã¯å¼•æ•°ã®é †åºãŒ (y_true, preds) ã¨ãªã‚‹
        preds: ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿå‡ºåŠ› (Logits)
        y_true: æ­£è§£ãƒ©ãƒ™ãƒ« (numpy array)
        """
        # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›
        p = expit(preds)
        p = np.clip(p, 1e-15, 1 - 1e-15)
        
        # p_t: æ­£è§£ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡
        p_t = y_true * p + (1 - y_true) * (1 - p)
        
        # alpha_t: ã‚¯ãƒ©ã‚¹ã”ã¨ã®é‡ã¿
        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
        
        # Focal weight: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** gamma
        
        # å‹¾é…
        grad = alpha_t * focal_weight * (p - y_true)
        
        # ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆè¿‘ä¼¼ï¼‰
        # æ³¨æ„: å³å¯†ãªFocal Lossã®2éšå¾®åˆ†ã¯ã‚ˆã‚Šè¤‡é›‘ãªé …ã‚’å«ã¿ã¾ã™ãŒã€
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€grad * (1 - 2*p) ã®é …ã‚’ç„¡è¦–ã—ãŸè¿‘ä¼¼ã‚’ä½¿ç”¨ã€‚
        # focal_weightã¯å®šæ•°ã¨ã—ã¦æ‰±ã£ã¦ã„ã¾ã™ï¼ˆå¾®åˆ†ã®é€£é–å¾‹ã«å«ã¾ã‚Œã¦ã„ãªã„ï¼‰ã€‚
        # ã“ã®è¿‘ä¼¼ã¯å®Ÿç”¨ä¸Šå¤šãã®ã‚±ãƒ¼ã‚¹ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚
        # å­¦ç¿’ãŒä¸å®‰å®šãªå ´åˆã¯ scale_pos_weight ã‚’ä½¿ç”¨ã—ãŸé‡ã¿ä»˜ã‘LogLossã¨æ¯”è¼ƒæ¤œè¨ã—ã¦ãã ã•ã„ã€‚
        hess = alpha_t * focal_weight * p * (1 - p)
        hess = np.maximum(hess, 1e-7)
        
        return grad, hess
    
    return focal_loss_lgb


class TwoStageDAEPipeline:
    """2æ®µéšãƒ¢ãƒ‡ãƒ« + DAEç‰¹å¾´é‡çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        data_path: str = "data/processed/honhyo_clean_with_features.csv",
        target_col: str = "æ­»è€…æ•°",
        n_folds: int = 5,
        random_state: int = 42,
        stage1_recall_target: float = 0.95,
        undersample_ratio: float = 2.0,
        n_seeds: int = 3,
        top_k_interactions: int = 5,
        test_size: float = 0.2,
        # Optunaæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        focal_alpha: float = 0.6321,
        focal_gamma: float = 1.1495,
        # DAEãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        dae_bottleneck_dim: int = 128,
        dae_hidden_dim: int = 768,    # é«˜é€ŸåŒ–: 1500->768
        dae_epochs: int = 15,         # é«˜é€ŸåŒ–: 50->15
        dae_swap_noise: float = 0.15,
        dae_batch_size: int = 32768,  # GPUæœ€é©åŒ– (RTX 5080ç”¨)
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        use_prob_stage1: bool = True,
    ):
        self.data_path = data_path
        self.target_col = target_col
        self.n_folds = n_folds
        self.random_state = random_state
        self.stage1_recall_target = stage1_recall_target
        self.undersample_ratio = undersample_ratio
        self.n_seeds = n_seeds
        self.top_k_interactions = top_k_interactions
        self.test_size = test_size
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        
        # DAE parameters
        self.dae_bottleneck_dim = dae_bottleneck_dim
        self.dae_hidden_dim = dae_hidden_dim
        self.dae_epochs = dae_epochs
        self.dae_swap_noise = dae_swap_noise
        self.dae_batch_size = dae_batch_size
        self.use_prob_stage1 = use_prob_stage1
        
        self.output_dir = "results/two_stage_model/dae_pipeline"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Storage for models
        self.stage1_models = []
        self.stage2_models = []
        self.dae_models = []
        
        print("=" * 60)
        print("2æ®µéšãƒ¢ãƒ‡ãƒ« + DAEç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
        print(f"Stage 1: 1:{int(self.undersample_ratio)} Under-sampling, Recall {self.stage1_recall_target:.0%}")
        print(f"ğŸ“ Stage 1äºˆæ¸¬ã¯ Logits ã§ä¿æŒã—ã€Seedé–“ã§å¹³å‡åŒ–ã—ã¦ã‹ã‚‰ Sigmoid é©ç”¨")
        print(f"Focal Loss: Alpha={self.focal_alpha:.4f}, Gamma={self.focal_gamma:.4f}")
        print(f"DAE: Bottleneck={self.dae_bottleneck_dim}, Epochs={self.dae_epochs}, Batch={self.dae_batch_size}")
        print(f"use_prob_stage1: {self.use_prob_stage1}")
        print(f"Test Set: {self.test_size:.0%}")
        print("=" * 60)
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨Train/Teståˆ†å‰²"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        self.df = pd.read_csv(self.data_path)
        
        y_all = self.df[self.target_col].values
        X_all = self.df.drop(columns=[self.target_col])
        
        if 'ç™ºç”Ÿæ—¥æ™‚' in X_all.columns:
            X_all = X_all.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        # Train/Teståˆ†å‰² (å±¤åŒ–æŠ½å‡º)
        self.X, self.X_test, self.y, self.y_test = train_test_split(
            X_all, y_all, test_size=self.test_size, 
            random_state=self.random_state, stratify=y_all
        )
        
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (Train: {1-self.test_size:.0%} / Test: {self.test_size:.0%})")
        print(f"   Train: æ­£ä¾‹ {self.y.sum():,} / {len(self.y):,}")
        print(f"   Test:  æ­£ä¾‹ {self.y_test.sum():,} / {len(self.y_test):,}")
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç‰¹å®š
        known_categoricals = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
            'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
            'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
            'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
            'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
        ]
        
        self.categorical_cols = []
        self.numeric_cols = []
        
        for col in self.X.columns:
            if col in known_categoricals or self.X[col].dtype == 'object':
                self.categorical_cols.append(col)
                self.X[col] = self.X[col].astype('category')
                self.X_test[col] = self.X_test[col].astype('category')
            else:
                self.numeric_cols.append(col)
                self.X[col] = self.X[col].astype(np.float32)
                self.X_test[col] = self.X_test[col].astype(np.float32)
        
        self.feature_names = list(self.X.columns)
        gc.collect()
    
    def train_stage1(self):
        """Stage 1: LightGBM + Under-sampling + Multi-Seed"""
        print(f"\nğŸŒ¿ Stage 1: LightGBM + Under-sampling (1:{int(self.undersample_ratio)}) + {self.n_seeds}-Seed Averaging")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba_stage1 = np.zeros(len(self.y))
        feature_importances = np.zeros(len(self.feature_names))
        
        lgb_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'num_leaves': 31,
            'max_depth': 8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'is_unbalance': False,
            'n_estimators': 1000,
            'learning_rate': 0.05,
            'n_jobs': -1
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_train_full = self.X.iloc[train_idx]
            X_val = self.X.iloc[val_idx]
            y_train_full = self.y[train_idx]
            y_val = self.y[val_idx]
            
            fold_models = []
            fold_logits = np.zeros(len(val_idx))  # ç¢ºç‡ã§ã¯ãªãLogitsã§å¹³å‡åŒ–
            
            for seed in range(self.n_seeds):
                np.random.seed(self.random_state + seed)
                
                # Under-sampling
                pos_idx = np.where(y_train_full == 1)[0]
                neg_idx = np.where(y_train_full == 0)[0]
                n_pos = len(pos_idx)
                n_neg_sample = int(n_pos * self.undersample_ratio)
                neg_sample_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
                
                train_idx_sampled = np.concatenate([pos_idx, neg_sample_idx])
                X_train = X_train_full.iloc[train_idx_sampled]
                y_train = y_train_full[train_idx_sampled]
                
                model = lgb.LGBMClassifier(**lgb_params, random_state=self.random_state + seed)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(50, verbose=False)]
                )
                
                # Logitsã§å–å¾—ã—ã¦å¹³å‡åŒ–ï¼ˆæ¥µç«¯ãªäºˆæ¸¬ã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆï¼‰
                raw_score = model.predict_proba(X_val)[:, 1]
                # predict_probaã¯ç¢ºç‡ã‚’è¿”ã™ã®ã§ã€logitå¤‰æ›ã—ã¦ã‹ã‚‰å¹³å‡
                raw_score = np.clip(raw_score, 1e-15, 1 - 1e-15)
                logits = np.log(raw_score / (1 - raw_score))  # logitå¤‰æ›
                fold_logits += logits / self.n_seeds
                fold_models.append(model)
                feature_importances += model.feature_importances_ / (self.n_folds * self.n_seeds)
            
            # Logitså¹³å‡ã‹ã‚‰Sigmoidã§ç¢ºç‡ã«å¤‰æ›
            self.oof_proba_stage1[val_idx] = expit(fold_logits)
            # Logitsã‚‚ä¿å­˜ï¼ˆStage 2ã®ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ï¼‰
            if not hasattr(self, 'oof_logits_stage1'):
                self.oof_logits_stage1 = np.zeros(len(self.y))
            self.oof_logits_stage1[val_idx] = fold_logits
            self.stage1_models.append(fold_models)
            
            del X_train, X_val
            gc.collect()
        
        # Feature Importance
        self.feature_importance_df = pd.DataFrame({
            'feature': self.feature_names, 'importance': feature_importances
        }).sort_values('importance', ascending=False)
        self.top_features = self.feature_importance_df.head(10)['feature'].tolist()
        
        # OOFè©•ä¾¡
        oof_pred = (self.oof_proba_stage1 >= 0.5).astype(int)
        oof_auc = roc_auc_score(self.y, self.oof_proba_stage1)
        print(f"   OOF (é–¾å€¤0.5): Prec={precision_score(self.y, oof_pred):.4f}, "
              f"Rec={recall_score(self.y, oof_pred):.4f}, AUC={oof_auc:.4f}")
    
    def find_recall_threshold(self):
        """Recallç›®æ¨™ã‚’é”æˆã™ã‚‹é–¾å€¤ã‚’æ¢ç´¢"""
        # 0.5ã‹ã‚‰ä¸‹ã’ã¦ã„ãã€Recallç›®æ¨™ã‚’æº€ãŸã™æœ€å¤§ã®é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹
        # (ä»¥å‰ã®å®Ÿè£…ã¯0.001ã‹ã‚‰ä¸Šã’ã¦ã„ãŸãŸã‚ã€æœ€å°ã®é–¾å€¤ã§æ­¢ã¾ã£ã¦ã—ã¾ã£ã¦ã„ãŸ)
        for thresh in np.arange(0.5, 0.0, -0.001):
            y_pred = (self.oof_proba_stage1 >= thresh).astype(int)
            recall = recall_score(self.y, y_pred)
            if recall >= self.stage1_recall_target:
                self.threshold_stage1 = thresh
                break
        else:
            self.threshold_stage1 = 0.001
        
        y_pred_final = (self.oof_proba_stage1 >= self.threshold_stage1).astype(int)
        self.stage1_recall = recall_score(self.y, y_pred_final)
        n_candidates = y_pred_final.sum()
        self.filter_rate = 1 - (n_candidates / len(self.y))
        n_filtered = len(self.y) - n_candidates
        
        print(f"   é–¾å€¤: {self.threshold_stage1:.4f}, Recall: {self.stage1_recall:.4f}")
        print(f"   [Result] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {n_filtered:,} ä»¶é™¤å¤– ({self.filter_rate:.2%})")
        print(f"   [Result] æ®‹å­˜ãƒ‡ãƒ¼ã‚¿: {n_candidates:,} ä»¶ (Stage 2 å€™è£œ)")
        print(f"   [Result] æ­£ä¾‹æ®‹å­˜: {self.y[self.oof_proba_stage1 >= self.threshold_stage1].sum():,} / {self.y.sum():,}")
        
        self.stage2_mask = self.oof_proba_stage1 >= self.threshold_stage1
    
    def generate_stage2_features(self, X_subset, logits_stage1_subset, fit_categories=True):
        """
        Stage 2ç”¨ç‰¹å¾´é‡ç”Ÿæˆ (DAEç‰¹å¾´é‡ãªã—ã€åŸºæœ¬ç‰¹å¾´é‡ã®ã¿)
        
        Args:
            logits_stage1_subset: Stage 1ã®Logitså€¤ï¼ˆç¢ºç‡ã§ã¯ãªãç”Ÿã‚¹ã‚³ã‚¢ï¼‰
                                   Logitsã¯æƒ…å ±ã®è§£åƒåº¦ãŒé«˜ãã€å­¦ç¿’ã—ã‚„ã™ã„
        """
        X_out = X_subset.copy()
        
        # (a) logits_stage1 è¿½åŠ  (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        # ç¢ºç‡(0-1)ã§ã¯ãªãLogitsã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ç«¯ã®æƒ…å ±ã‚’ä¿æŒ
        if self.use_prob_stage1:
            X_out['logits_stage1'] = logits_stage1_subset
        
        # (b) Categorical Interaction Features
        top_cat_features = [f for f in self.top_features if f in self.categorical_cols]
        
        if fit_categories:
            self.interaction_categories = {}
        
        for i, f1 in enumerate(top_cat_features[:self.top_k_interactions]):
            for f2 in top_cat_features[i+1:self.top_k_interactions]:
                name = f"{f1}_{f2}"
                interaction_values = X_subset[f1].astype(str) + "_" + X_subset[f2].astype(str)
                
                if fit_categories:
                    # å­¦ç¿’æ™‚: ã‚«ãƒ†ã‚´ãƒªã‚’ä½œæˆã—ã¦ä¿å­˜
                    cat_type = pd.CategoricalDtype(categories=list(interaction_values.unique()) + ['__UNKNOWN__'])
                    self.interaction_categories[name] = cat_type
                    X_out[name] = pd.Categorical(interaction_values, dtype=cat_type)
                else:
                    # ãƒ†ã‚¹ãƒˆæ™‚: ä¿å­˜æ¸ˆã¿ã‚«ãƒ†ã‚´ãƒªã‚’ä½¿ç”¨ã€æœªçŸ¥ã®çµ„ã¿åˆã‚ã›ã¯ __UNKNOWN__ ã«ãƒãƒƒãƒ—
                    if hasattr(self, 'interaction_categories') and name in self.interaction_categories:
                        known_cats = set(self.interaction_categories[name].categories)
                        # æœªçŸ¥ã®çµ„ã¿åˆã‚ã›ã‚’ __UNKNOWN__ ã«ç½®æ›
                        interaction_values = interaction_values.apply(
                            lambda x: x if x in known_cats else '__UNKNOWN__'
                        )
                        X_out[name] = pd.Categorical(interaction_values, dtype=self.interaction_categories[name])
                    else:
                        X_out[name] = interaction_values.astype('category')
        
        return X_out
    
    def train_stage2_with_dae(self):
        """
        Stage 2: DAEç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸLightGBMå­¦ç¿’
        CVã®å„Foldå†…ã§DAEã‚’å­¦ç¿’ã—ã€ç‰¹å¾´é‡ã‚’è¿½åŠ 
        """
        print("\nğŸŒ¿ Stage 2: LightGBM + DAEç‰¹å¾´é‡ (5-Fold CV)")
        print(f"   Focal Loss: Alpha={self.focal_alpha:.4f}, Gamma={self.focal_gamma:.4f}")
        print(f"   DAE: Bottleneck={self.dae_bottleneck_dim}, Epochs={self.dae_epochs}")
        
        # Stage 2ç”¨ã®å…¨ãƒ‡ãƒ¼ã‚¿ (åŸºæœ¬ç‰¹å¾´é‡ã®ã¿) - Logitsã‚’ä½¿ç”¨
        X_s2_base = self.generate_stage2_features(
            self.X[self.stage2_mask].copy(),
            self.oof_logits_stage1[self.stage2_mask],  # ç¢ºç‡ã§ã¯ãªãLogitsã‚’ä½¿ç”¨
            fit_categories=True
        ).reset_index(drop=True)
        
        y_s2_full = self.y[self.stage2_mask]
        
        n_pos, n_neg = y_s2_full.sum(), len(y_s2_full) - y_s2_full.sum()
        print(f"   Stage 2 ãƒ‡ãƒ¼ã‚¿: {len(y_s2_full):,} (Pos: {n_pos:,}, Neg: {n_neg:,})")
        print(f"   Top Features for Interaction: {self.top_features[:5]}")
        
        # OOFäºˆæ¸¬å€¤ã‚’æ ¼ç´
        self.oof_proba_stage2 = np.zeros(len(y_s2_full))
        self.stage2_models = []
        self.dae_models = []
        
        # CVè¨­å®š
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Optunaæœ€é©åŒ–æ¸ˆã¿)
        focal_loss_fn = get_focal_loss_lgb(alpha=self.focal_alpha, gamma=self.focal_gamma)
        lgb_params = {
            'objective': focal_loss_fn,
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'num_leaves': 127,
            'max_depth': -1,
            'min_child_samples': 44,
            'reg_alpha': 2.3897,
            'reg_lambda': 2.2842,
            'colsample_bytree': 0.8646,
            'subsample': 0.6328,
            'learning_rate': 0.0477,
            'is_unbalance': False,
            'n_estimators': 1000,
            'n_jobs': -1
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_s2_base, y_s2_full)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            X_train_base = X_s2_base.iloc[train_idx].reset_index(drop=True)
            X_val_base = X_s2_base.iloc[val_idx].reset_index(drop=True)
            y_train = y_s2_full[train_idx]
            y_val = y_s2_full[val_idx]
            
            # === DAEå­¦ç¿’ & ç‰¹å¾´é‡æŠ½å‡º ===
            print(f"      ğŸ“¦ DAEå­¦ç¿’ä¸­...")
            dae = DAEFeatureExtractor(
                numeric_cols=self.numeric_cols + (['logits_stage1'] if self.use_prob_stage1 else []),
                cat_cols=self.categorical_cols,
                bottleneck_dim=self.dae_bottleneck_dim,
                hidden_dim=self.dae_hidden_dim,
                epochs=self.dae_epochs,
                swap_noise_rate=self.dae_swap_noise,
                batch_size=self.dae_batch_size,
                verbose=True,  # ãƒ­ã‚°ã‚’è¡¨ç¤ºã—ã¦GPUç¢ºèª
                n_workers=4    # é«˜é€ŸåŒ–ã®ãŸã‚ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ç¢ºèªç”¨ãƒ­ã‚°
            print(f"      ğŸ–¥ï¸  Device being used: {dae.device}")
            
            # DAEã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’
            dae.fit(X_train_base)
            
            # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º
            dae_train_features = dae.transform(X_train_base)
            dae_val_features = dae.transform(X_val_base)
            
            # DAEç‰¹å¾´é‡ã‚’DataFrameã«å¤‰æ›
            dae_cols = [f'dae_{i}' for i in range(self.dae_bottleneck_dim)]
            dae_train_df = pd.DataFrame(dae_train_features, columns=dae_cols)
            dae_val_df = pd.DataFrame(dae_val_features, columns=dae_cols)
            
            # åŸºæœ¬ç‰¹å¾´é‡ã¨DAEç‰¹å¾´é‡ã‚’çµåˆ
            X_train_full = pd.concat([X_train_base.reset_index(drop=True), dae_train_df], axis=1)
            X_val_full = pd.concat([X_val_base.reset_index(drop=True), dae_val_df], axis=1)
            
            # === LightGBMå­¦ç¿’ ===
            model = lgb.LGBMClassifier(**lgb_params, random_state=self.random_state)
            model.fit(
                X_train_full, y_train,
                eval_set=[(X_val_full, y_val)],
                callbacks=[lgb.early_stopping(50, verbose=False)]
            )
            
            # OOFäºˆæ¸¬ (raw_scoreã‹ã‚‰ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›)
            raw_score = model.predict(X_val_full, raw_score=True)
            proba = 1.0 / (1.0 + np.exp(-raw_score))
            self.oof_proba_stage2[val_idx] = proba
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            self.stage2_models.append(model)
            self.dae_models.append(dae)
            
            del X_train_full, X_val_full, dae_train_features, dae_val_features
            gc.collect()
        
        # Stage 2 OOFè©•ä¾¡
        oof_auc = roc_auc_score(y_s2_full, self.oof_proba_stage2)
        print(f"   Stage 2 OOF AUC: {oof_auc:.4f}")
    
    def evaluate(self):
        """æœ€çµ‚è©•ä¾¡ (CV OOF)"""
        print("\nğŸ“ˆ æœ€çµ‚è©•ä¾¡ (Cross Validation OOF)")
        
        y_s2 = self.y[self.stage2_mask]
        
        # æœ€çµ‚äºˆæ¸¬ç¢ºç‡
        self.final_proba = np.zeros(len(self.y))
        self.final_proba[self.stage2_mask] = self.oof_proba_stage2
        
        # å‹•çš„é–¾å€¤è©•ä¾¡
        precisions, recalls, thresholds = precision_recall_curve(y_s2, self.oof_proba_stage2)
        
        self.dynamic_results = {}
        target_recalls = [0.99, 0.98, 0.95]
        
        print("\n   ğŸ“Š å‹•çš„é–¾å€¤è©•ä¾¡:")
        for target_recall in target_recalls:
            idx = np.where(recalls >= target_recall)[0]
            if len(idx) > 0:
                idx = idx[-1]
                if idx < len(thresholds):
                    best_thresh = thresholds[idx]
                    best_prec = precisions[idx]
                else:
                    best_thresh = 0.0
                    best_prec = precisions[-1]
            else:
                best_thresh = 0.0
                best_prec = 0.0
            
            self.dynamic_results[target_recall] = {
                'threshold': best_thresh,
                'precision': best_prec
            }
            print(f"      Recall ~{target_recall:.0%}: é–¾å€¤={best_thresh:.4f}, Precision={best_prec:.4f}")
        
        # å›ºå®šé–¾å€¤è©•ä¾¡
        y_pred = (self.final_proba >= 0.5).astype(int)
        
        self.final_precision = precision_score(self.y, y_pred) if y_pred.sum() > 0 else 0
        self.final_recall = recall_score(self.y, y_pred)
        self.final_f1 = f1_score(self.y, y_pred)
        self.final_auc = roc_auc_score(self.y, self.final_proba)
        
        print(f"\n   [é–¾å€¤0.5] Precision: {self.final_precision:.4f}, Recall: {self.final_recall:.4f}, F1: {self.final_f1:.4f}")
        
        # Baseline (Stage 1)
        y_pred_bl = (self.oof_proba_stage1 >= 0.5).astype(int)
        self.baseline_precision = precision_score(self.y, y_pred_bl)
        self.baseline_recall = recall_score(self.y, y_pred_bl)
        print(f"   [ãƒ™ãƒ¼ã‚¹(Stage1)] Precision: {self.baseline_precision:.4f}, Recall: {self.baseline_recall:.4f}")
        
        improvement = (self.final_precision - self.baseline_precision) / self.baseline_precision * 100 if self.baseline_precision > 0 else 0
        print(f"   Precisionæ”¹å–„ç‡ (é–¾å€¤0.5): {improvement:+.2f}%")
        
        return {
            'stage1_threshold': self.threshold_stage1,
            'stage1_recall': self.stage1_recall,
            'filter_rate': self.filter_rate,
            'final_precision': self.final_precision,
            'final_recall': self.final_recall,
            'final_f1': self.final_f1,
            'final_auc': self.final_auc,
            'baseline_precision': self.baseline_precision,
            'baseline_recall': self.baseline_recall,
            'precision_improvement_pct': improvement,
            'dynamic_recall_99_precision': self.dynamic_results.get(0.99, {}).get('precision', 0),
            'dynamic_recall_98_precision': self.dynamic_results.get(0.98, {}).get('precision', 0),
        }
    
    def evaluate_test_set(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®æœ€çµ‚è©•ä¾¡"""
        print("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ (Hold-Out)")
        
        # Stage 1: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ (Logitså¹³å‡)
        test_logits_stage1 = np.zeros(len(self.y_test))
        for fold_models in self.stage1_models:
            for model in fold_models:
                proba = model.predict_proba(self.X_test)[:, 1]
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
                logits = np.log(proba / (1 - proba))
                test_logits_stage1 += logits
        test_logits_stage1 /= (self.n_folds * self.n_seeds)
        test_proba_stage1 = expit(test_logits_stage1)
        
        # Stage 1é–¾å€¤é©ç”¨
        test_stage2_mask = test_proba_stage1 >= self.threshold_stage1
        n_candidates = test_stage2_mask.sum()
        n_pos_in_candidates = self.y_test[test_stage2_mask].sum()
        
        print(f"   Stage 1 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {n_candidates:,} / {len(self.y_test):,}")
        print(f"   æ­£ä¾‹æ®‹å­˜: {n_pos_in_candidates:,} / {self.y_test.sum():,}")
        
        if n_candidates == 0:
            print("   âš ï¸ Stage 2ã«é€²ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            self.test_results = {'error': 'No candidates after Stage 1'}
            return self.test_results
        
        # Stage 2ç”¨åŸºæœ¬ç‰¹å¾´é‡ (Logitsã‚’ä½¿ç”¨)
        X_test_s2_base = self.generate_stage2_features(
            self.X_test[test_stage2_mask].copy(),
            test_logits_stage1[test_stage2_mask],  # ç¢ºç‡ã§ã¯ãªãLogitsã‚’ä½¿ç”¨
            fit_categories=False
        )
        y_test_s2 = self.y_test[test_stage2_mask]
        
        # Stage 2: å„Foldã®DAE+LightGBMã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        test_proba_stage2 = np.zeros(len(y_test_s2))
        
        for fold, (dae, model) in enumerate(zip(self.dae_models, self.stage2_models)):
            # DAEç‰¹å¾´é‡æŠ½å‡º
            dae_features = dae.transform(X_test_s2_base)
            dae_cols = [f'dae_{i}' for i in range(self.dae_bottleneck_dim)]
            dae_df = pd.DataFrame(dae_features, columns=dae_cols)
            
            # çµåˆ
            X_test_full = pd.concat([X_test_s2_base.reset_index(drop=True), dae_df], axis=1)
            
            # äºˆæ¸¬
            raw_score = model.predict(X_test_full, raw_score=True)
            proba = 1.0 / (1.0 + np.exp(-raw_score))
            test_proba_stage2 += proba / self.n_folds
        
        # å‹•çš„é–¾å€¤è©•ä¾¡
        precisions, recalls, thresholds = precision_recall_curve(y_test_s2, test_proba_stage2)
        
        self.test_dynamic_results = {}
        target_recalls = [0.99, 0.98, 0.95]
        
        print("\n   ğŸ“Š ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå‹•çš„é–¾å€¤è©•ä¾¡:")
        for target_recall in target_recalls:
            idx = np.where(recalls >= target_recall)[0]
            if len(idx) > 0:
                idx = idx[-1]
                if idx < len(thresholds):
                    best_thresh = thresholds[idx]
                    best_prec = precisions[idx]
                else:
                    best_thresh = 0.0
                    best_prec = precisions[-1]
            else:
                best_thresh = 0.0
                best_prec = 0.0
            
            self.test_dynamic_results[target_recall] = {
                'threshold': best_thresh,
                'precision': best_prec
            }
            print(f"      Recall ~{target_recall:.0%}: é–¾å€¤={best_thresh:.4f}, Precision={best_prec:.4f}")
        
        # å›ºå®šé–¾å€¤è©•ä¾¡
        final_test_proba = np.zeros(len(self.y_test))
        final_test_proba[test_stage2_mask] = test_proba_stage2
        y_test_pred = (final_test_proba >= 0.5).astype(int)
        
        test_precision = precision_score(self.y_test, y_test_pred) if y_test_pred.sum() > 0 else 0
        test_recall = recall_score(self.y_test, y_test_pred)
        test_f1 = f1_score(self.y_test, y_test_pred)
        test_auc = roc_auc_score(self.y_test, final_test_proba)
        
        print(f"\n   [ãƒ†ã‚¹ãƒˆé–¾å€¤0.5] Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")
        print(f"   [ãƒ†ã‚¹ãƒˆAUC]: {test_auc:.4f}")
        
        self.test_results = {
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'test_auc': test_auc,
            'test_precision_at_recall99': self.test_dynamic_results.get(0.99, {}).get('precision', 0),
            'test_precision_at_recall98': self.test_dynamic_results.get(0.98, {}).get('precision', 0),
            'test_precision_at_recall95': self.test_dynamic_results.get(0.95, {}).get('precision', 0),
        }
        
        return self.test_results
    
    def generate_report(self, results: dict, elapsed_sec: float):
        """å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownã§å‡ºåŠ›"""
        report_path = os.path.join(self.output_dir, "experiment_report.md")
        
        report_content = f"""# DAEç‰¹å¾´é‡çµ±åˆå®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å®Ÿè¡Œæ™‚é–“**: {elapsed_sec:.1f}ç§’

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ |
|-----------|----| 
| Focal Alpha | {self.focal_alpha:.4f} |
| Focal Gamma | {self.focal_gamma:.4f} |
| DAE Bottleneck | {self.dae_bottleneck_dim} |
| DAE Epochs | {self.dae_epochs} |
| DAE Swap Noise | {self.dae_swap_noise:.2f} |
| Stage 1 Recall Target | {self.stage1_recall_target:.0%} |
| Test Set Ratio | {self.test_size:.0%} |

## çµæœã‚µãƒãƒª

### Stage 1
- **é–¾å€¤**: {results['stage1_threshold']:.4f}
- **Recall**: {results['stage1_recall']:.4f}
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‡**: {results['filter_rate']*100:.2f}%

### Stage 2 (Focal Loss + DAE) - CV OOFè©•ä¾¡

#### å›ºå®šé–¾å€¤ (0.5) ã§ã®è©•ä¾¡
| æŒ‡æ¨™ | å€¤ |
|------|----| 
| Precision | {results['final_precision']:.4f} |
| Recall | {results['final_recall']:.4f} |
| F1 | {results['final_f1']:.4f} |
| AUC | {results['final_auc']:.4f} |

#### å‹•çš„é–¾å€¤ã§ã®è©•ä¾¡ (CV OOF)
| Target Recall | Precision |
|---------------|----------|
| 99% | {results.get('dynamic_recall_99_precision', 0):.4f} |
| 98% | {results.get('dynamic_recall_98_precision', 0):.4f} |

### ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ (Hold-Out {self.test_size:.0%})

| æŒ‡æ¨™ | å€¤ |
|------|----| 
| Precision | {results.get('test_precision', 0):.4f} |
| Recall | {results.get('test_recall', 0):.4f} |
| F1 | {results.get('test_f1', 0):.4f} |
| AUC | {results.get('test_auc', 0):.4f} |

#### å‹•çš„é–¾å€¤ã§ã®è©•ä¾¡ (Test Set)
| Target Recall | Precision |
|---------------|----------|
| 99% | {results.get('test_precision_at_recall99', 0):.4f} |
| 98% | {results.get('test_precision_at_recall98', 0):.4f} |
| 95% | {results.get('test_precision_at_recall95', 0):.4f} |

## è€ƒå¯Ÿ

- DAEç‰¹å¾´é‡ ({self.dae_bottleneck_dim}æ¬¡å…ƒ) ã«ã‚ˆã‚Šã€LightGBMãŒè‹¦æ‰‹ãªéç·šå½¢é–¢ä¿‚ã‚’æ•æ‰
- Swap Noise ({self.dae_swap_noise:.0%}) ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ
- CV OOF ã¨ Test Set ã®çµæœãŒè¿‘ã„ã»ã©ã€æ±åŒ–æ€§èƒ½ãŒé«˜ã„
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\n   ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {report_path}")
        return report_path
    
    def run(self):
        start = datetime.now()
        self.load_data()
        self.train_stage1()
        self.find_recall_threshold()
        self.train_stage2_with_dae()
        results = self.evaluate()
        
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡
        test_results = self.evaluate_test_set()
        results.update(test_results)
        
        elapsed_sec = (datetime.now() - start).total_seconds()
        results['elapsed_sec'] = elapsed_sec
        
        # çµæœä¿å­˜
        pd.DataFrame([results]).to_csv(os.path.join(self.output_dir, "final_results.csv"), index=False)
        self.feature_importance_df.to_csv(os.path.join(self.output_dir, "stage1_feature_importance.csv"), index=False)
        
        # Markdown ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_report(results, elapsed_sec)
        
        print("\n" + "=" * 60)
        print("âœ… å®Œäº†ï¼")
        print(f"   çµæœCSV: {self.output_dir}/final_results.csv")
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆMD: {self.output_dir}/experiment_report.md")
        print("=" * 60)
        
        return results


if __name__ == "__main__":
    pipeline = TwoStageDAEPipeline()
    pipeline.run()

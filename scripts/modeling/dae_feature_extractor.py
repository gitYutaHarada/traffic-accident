"""
Denoising Autoencoder (DAE) ç‰¹å¾´é‡æŠ½å‡ºå™¨
==========================================
Porto Seguroã‚³ãƒ³ãƒšå„ªå‹æ‰‹æ³•ã‚’å‚è€ƒã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿å‘ã‘ã®DAEã‚’å®Ÿè£…ã€‚
å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«Swap Noiseã‚’åŠ ãˆã€ãã‚Œã‚’å¾©å…ƒã™ã‚‹å­¦ç¿’ã‚’é€šã˜ã¦ã€
ã€Œéš ã‚ŒãŸé–¢ä¿‚æ€§ã€ã‚’æ‰ãˆãŸç‰¹å¾´é‡ï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã®å‡ºåŠ›ï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ç‰¹å¾´:
- RankGauss: æ•°å€¤å¤‰æ•°ã‚’æ­£è¦åˆ†å¸ƒã«å¤‰æ›
- Swap Noise: å…¥åŠ›ç›´å¾Œã«ãƒ©ãƒ³ãƒ€ãƒ ã«å€¤ã‚’å…¥ã‚Œæ›¿ãˆã‚‹
- Embedding: ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ä½æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«åŸ‹ã‚è¾¼ã‚€
- Loss: MSE(æ•°å€¤) + CrossEntropy(ã‚«ãƒ†ã‚´ãƒª)
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, QuantileTransformer
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from typing import List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')


class RankGaussTransformer:
    """æ•°å€¤å¤‰æ•°ã‚’RankGaussï¼ˆæ­£è¦åˆ†å¸ƒï¼‰ã«å¤‰æ›"""
    
    def __init__(self):
        self.transformers = {}
    
    def fit(self, X: pd.DataFrame, numeric_cols: List[str]):
        for col in numeric_cols:
            qt = QuantileTransformer(output_distribution='normal', random_state=42)
            qt.fit(X[[col]].values)
            self.transformers[col] = qt
        return self
    
    def transform(self, X: pd.DataFrame, numeric_cols: List[str]) -> np.ndarray:
        result = []
        for col in numeric_cols:
            if col in self.transformers:
                result.append(self.transformers[col].transform(X[[col]].values))
            else:
                result.append(X[[col]].values)
        return np.hstack(result).astype(np.float32)
    
    def fit_transform(self, X: pd.DataFrame, numeric_cols: List[str]) -> np.ndarray:
        self.fit(X, numeric_cols)
        return self.transform(X, numeric_cols)


class CategoryEncoder:
    """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’Label Encodingã—ã€Embeddingç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ"""
    
    def __init__(self):
        self.encoders = {}
        self.n_classes = {}
    
    def fit(self, X: pd.DataFrame, cat_cols: List[str]):
        for col in cat_cols:
            le = LabelEncoder()
            # ã‚«ãƒ†ã‚´ãƒªå‹ã®å ´åˆã¯å…ˆã«strå¤‰æ›ã—ã¦ã‹ã‚‰fillna
            col_values = X[col].astype(str).fillna('__missing__').tolist()
            # æœªçŸ¥ã®ã‚«ãƒ†ã‚´ãƒªã«å¯¾å¿œã™ã‚‹ãŸã‚ã€fitæ™‚ã«'__unknown__'ã‚’è¿½åŠ 
            le.fit(col_values + ['__unknown__', '__missing__'])
            self.encoders[col] = le
            self.n_classes[col] = len(le.classes_)
        return self
    
    def transform(self, X: pd.DataFrame, cat_cols: List[str]) -> np.ndarray:
        result = []
        for col in cat_cols:
            le = self.encoders[col]
            # ã‚«ãƒ†ã‚´ãƒªå‹ã®å ´åˆã¯å…ˆã«strå¤‰æ›ã—ã¦ã‹ã‚‰fillna
            values = X[col].astype(str).fillna('__missing__')
            # æœªçŸ¥ã®ã‚«ãƒ†ã‚´ãƒªã‚’'__unknown__'ã«å¤‰æ›
            encoded = []
            for v in values:
                if v in le.classes_:
                    encoded.append(le.transform([v])[0])
                else:
                    encoded.append(le.transform(['__unknown__'])[0])
            result.append(np.array(encoded).reshape(-1, 1))
        return np.hstack(result).astype(np.int64)
    
    def fit_transform(self, X: pd.DataFrame, cat_cols: List[str]) -> np.ndarray:
        self.fit(X, cat_cols)
        return self.transform(X, cat_cols)


class DenoisingAutoencoder(nn.Module):
    """
    Denoising Autoencoder ãƒ¢ãƒ‡ãƒ«
    
    æ§‹é€ :
    Input -> [Swap Noise] -> [Embeddings + Numeric] -> Dense(1500) -> Dense(128) [Bottleneck] -> Dense(1500) -> Output
    """
    
    def __init__(
        self,
        n_numeric: int,
        cat_cardinalities: List[int],
        embedding_dim: int = 8,
        hidden_dim: int = 1500,
        bottleneck_dim: int = 128,
        swap_noise_rate: float = 0.15,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        self.n_numeric = n_numeric
        self.cat_cardinalities = cat_cardinalities
        self.swap_noise_rate = swap_noise_rate
        
        # Embeddingå±¤ï¼ˆå„ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ï¼‰
        self.embeddings = nn.ModuleList([
            nn.Embedding(n_classes, embedding_dim) for n_classes in cat_cardinalities
        ])
        
        total_cat_dim = len(cat_cardinalities) * embedding_dim
        input_dim = n_numeric + total_cat_dim
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, bottleneck_dim),  # Bottleneck (Linear activation for features)
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(bottleneck_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        
        # å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ï¼ˆæ•°å€¤: å›å¸°, ã‚«ãƒ†ã‚´ãƒª: åˆ†é¡ï¼‰
        self.numeric_head = nn.Linear(hidden_dim, n_numeric)
        self.cat_heads = nn.ModuleList([
            nn.Linear(hidden_dim, n_classes) for n_classes in cat_cardinalities
        ])
    
    def swap_noise(self, x: torch.Tensor) -> torch.Tensor:
        """Swap Noise: ãƒ©ãƒ³ãƒ€ãƒ ã«ä»–ã®è¡Œã®å€¤ã¨å…¥ã‚Œæ›¿ãˆã‚‹"""
        if not self.training or self.swap_noise_rate == 0:
            return x
        
        noise_mask = torch.rand_like(x) < self.swap_noise_rate
        shuffle_idx = torch.randperm(x.size(0))
        noisy_x = torch.where(noise_mask, x[shuffle_idx], x)
        return noisy_x
    
    def forward(self, numeric: torch.Tensor, categories: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        # Swap Noiseé©ç”¨ï¼ˆå…¥åŠ›ç›´å¾Œï¼‰
        numeric_noisy = self.swap_noise(numeric)
        categories_noisy = self.swap_noise(categories.float()).long()
        
        # Embedding
        cat_embedded = [emb(categories_noisy[:, i]) for i, emb in enumerate(self.embeddings)]
        cat_embedded = torch.cat(cat_embedded, dim=1) if cat_embedded else torch.zeros(numeric.size(0), 0)
        
        # çµåˆ
        x = torch.cat([numeric_noisy, cat_embedded], dim=1)
        
        # Encode -> Bottleneck
        bottleneck = self.encoder(x)
        
        # Decode
        decoded = self.decoder(bottleneck)
        
        # å‡ºåŠ›
        numeric_out = self.numeric_head(decoded)
        cat_outs = [head(decoded) for head in self.cat_heads]
        
        return bottleneck, numeric_out, cat_outs
    
    def get_features(self, numeric: torch.Tensor, categories: torch.Tensor) -> torch.Tensor:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆæ¨è«–ç”¨ï¼‰"""
        self.eval()
        with torch.no_grad():
            cat_embedded = [emb(categories[:, i]) for i, emb in enumerate(self.embeddings)]
            cat_embedded = torch.cat(cat_embedded, dim=1) if cat_embedded else torch.zeros(numeric.size(0), 0)
            x = torch.cat([numeric, cat_embedded], dim=1)
            bottleneck = self.encoder(x)
        return bottleneck


class DAEFeatureExtractor:
    """
    DAEç‰¹å¾´é‡æŠ½å‡ºå™¨ (é«˜ãƒ¬ãƒ™ãƒ«API)
    
    ä½¿ã„æ–¹:
        extractor = DAEFeatureExtractor(numeric_cols, cat_cols)
        extractor.fit(X_train)
        features = extractor.transform(X_test)
    """
    
    def __init__(
        self,
        numeric_cols: List[str],
        cat_cols: List[str],
        bottleneck_dim: int = 128,
        hidden_dim: int = 1500,
        embedding_dim: int = 8,
        swap_noise_rate: float = 0.15,
        batch_size: int = 512,
        epochs: int = 50,
        lr: float = 1e-3,
        patience: int = 5,
        verbose: bool = True,
        n_workers: int = 0,  # DataLoaderç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (Windowsã¯0æ¨å¥¨)
    ):
        self.numeric_cols = numeric_cols
        self.cat_cols = cat_cols
        self.bottleneck_dim = bottleneck_dim
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.swap_noise_rate = swap_noise_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.lr = lr
        self.patience = patience
        self.verbose = verbose
        self.n_workers = n_workers
        
        self.rank_gauss = RankGaussTransformer()
        self.cat_encoder = CategoryEncoder()
        self.model = None
        # GPUè‡ªå‹•æ¤œå‡º: CUDAãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã‚’ä½¿ç”¨
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if self.verbose:
            print(f"ğŸ–¥ï¸ DAE Device: {self.device}")
    
    def fit(self, X: pd.DataFrame):
        """DAEã‚’å­¦ç¿’"""
        import time
        start_time = time.time()
        
        if self.verbose:
            print(f"ğŸ“¦ DAEå­¦ç¿’é–‹å§‹ (Bottleneck={self.bottleneck_dim}, epochs={self.epochs}, device={self.device})")
        
        # å‰å‡¦ç†
        print(f"   â³ [Preproc] RankGauss & LabelEncoding starting...")
        X_num = self.rank_gauss.fit_transform(X, self.numeric_cols)
        X_cat = self.cat_encoder.fit_transform(X, self.cat_cols)
        print(f"   âœ… [Preproc] Done in {time.time() - start_time:.1f}s")
        
        # Train/Valåˆ†å‰²
        X_num_train, X_num_val, X_cat_train, X_cat_val = train_test_split(
            X_num, X_cat, test_size=0.1, random_state=42
        )
        
        train_dataset = TensorDataset(
            torch.tensor(X_num_train, dtype=torch.float32),
            torch.tensor(X_cat_train, dtype=torch.long)
        )
        val_dataset = TensorDataset(
            torch.tensor(X_num_val, dtype=torch.float32),
            torch.tensor(X_cat_val, dtype=torch.long)
        )
        
        use_cuda = self.device.type == 'cuda'
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, shuffle=True,
            num_workers=self.n_workers, pin_memory=use_cuda
        )
        val_loader = DataLoader(
            val_dataset, batch_size=self.batch_size, shuffle=False,
            num_workers=self.n_workers, pin_memory=use_cuda
        )
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        cat_cardinalities = [self.cat_encoder.n_classes[col] for col in self.cat_cols]
        self.model = DenoisingAutoencoder(
            n_numeric=len(self.numeric_cols),
            cat_cardinalities=cat_cardinalities,
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim,
            bottleneck_dim=self.bottleneck_dim,
            swap_noise_rate=self.swap_noise_rate,
        ).to(self.device)
        
        # æå¤±é–¢æ•°
        mse_loss = nn.MSELoss()
        ce_loss = nn.CrossEntropyLoss()
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer, max_lr=self.lr, epochs=self.epochs, steps_per_epoch=len(train_loader)
        )
        scaler = torch.amp.GradScaler('cuda') if use_cuda else None
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        best_val_loss = float('inf')
        patience_counter = 0
        best_state = None
        
        train_start_time = time.time()
        print(f"   ğŸš€ [Train] Start training loop...")
        
        for epoch in range(self.epochs):
            epoch_start = time.time()
            
            # Train
            self.model.train()
            train_loss = 0.0
            
            for numeric, categories in train_loader:
                numeric = numeric.to(self.device)
                categories = categories.to(self.device)
                
                optimizer.zero_grad()
                
                if use_cuda:
                    with torch.amp.autocast('cuda'):
                        _, numeric_out, cat_outs = self.model(numeric, categories)
                        loss = mse_loss(numeric_out, numeric)
                        for i, cat_out in enumerate(cat_outs):
                            loss += ce_loss(cat_out, categories[:, i])
                    
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    _, numeric_out, cat_outs = self.model(numeric, categories)
                    loss = mse_loss(numeric_out, numeric)
                    for i, cat_out in enumerate(cat_outs):
                        loss += ce_loss(cat_out, categories[:, i])
                    loss.backward()
                    optimizer.step()
                
                scheduler.step()
                train_loss += loss.item()
            
            # Validation
            self.model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for numeric, categories in val_loader:
                    numeric = numeric.to(self.device)
                    categories = categories.to(self.device)
                    
                    if use_cuda:
                        with torch.amp.autocast('cuda'):
                            _, numeric_out, cat_outs = self.model(numeric, categories)
                            loss = mse_loss(numeric_out, numeric)
                            for i, cat_out in enumerate(cat_outs):
                                loss += ce_loss(cat_out, categories[:, i])
                    else:
                        _, numeric_out, cat_outs = self.model(numeric, categories)
                        loss = mse_loss(numeric_out, numeric)
                        for i, cat_out in enumerate(cat_outs):
                            loss += ce_loss(cat_out, categories[:, i])
                    
                    val_loss += loss.item()
            
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            
            # ãƒ­ã‚°è¡¨ç¤º (æ¯å›è¡¨ç¤ºã—ã¦é€Ÿåº¦æ„Ÿã‚’ç¢ºèª)
            elapsed = time.time() - epoch_start
            print(f"      Epoch {epoch+1}/{self.epochs}: T-Loss={train_loss:.4f}, V-Loss={val_loss:.4f} ({elapsed:.2f}s/ep)")
            
            # Early Stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= self.patience:
                    if self.verbose:
                        print(f"   â¹ï¸ Early Stopping at epoch {epoch+1}")
                    break
        
        if best_state is not None:
            self.model.load_state_dict(best_state)
        
        total_time = time.time() - train_start_time
        if self.verbose:
            print(f"âœ… DAEå­¦ç¿’å®Œäº† (Best Val Loss={best_val_loss:.4f}, Total Train Time={total_time:.1f}s)")
        
        return self
    
    def transform(self, X: pd.DataFrame) -> np.ndarray:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        if self.model is None:
            raise ValueError("DAEãŒã¾ã å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚fit()ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")
        
        X_num = self.rank_gauss.transform(X, self.numeric_cols)
        X_cat = self.cat_encoder.transform(X, self.cat_cols)
        
        dataset = TensorDataset(
            torch.tensor(X_num, dtype=torch.float32),
            torch.tensor(X_cat, dtype=torch.long)
        )
        use_cuda = self.device.type == 'cuda'
        loader = DataLoader(
            dataset, batch_size=self.batch_size, shuffle=False,
            num_workers=self.n_workers, pin_memory=use_cuda
        )
        
        features = []
        self.model.eval()
        with torch.no_grad():
            for numeric, categories in loader:
                numeric = numeric.to(self.device)
                categories = categories.to(self.device)
                bottleneck = self.model.get_features(numeric, categories)
                features.append(bottleneck.cpu().numpy())
        
        return np.vstack(features)
    
    def fit_transform(self, X: pd.DataFrame) -> np.ndarray:
        """å­¦ç¿’ã—ã¦ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º"""
        self.fit(X)
        return self.transform(X)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    print("DAE Feature Extractor - Test Run")
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 1000
    df = pd.DataFrame({
        'num1': np.random.randn(n_samples),
        'num2': np.random.randn(n_samples) * 10,
        'cat1': np.random.choice(['A', 'B', 'C'], n_samples),
        'cat2': np.random.choice(['X', 'Y'], n_samples),
    })
    
    extractor = DAEFeatureExtractor(
        numeric_cols=['num1', 'num2'],
        cat_cols=['cat1', 'cat2'],
        bottleneck_dim=16,
        epochs=20,
        verbose=True
    )
    
    features = extractor.fit_transform(df)
    print(f"Generated features shape: {features.shape}")  # (1000, 16)

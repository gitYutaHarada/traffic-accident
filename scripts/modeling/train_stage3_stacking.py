"""
Stage 3: Stacking Meta-Model (Enhanced Robust Version)
=======================================================
Two-Stage CatBoostã¨Single-Stage TabNetã®äºˆæ¸¬å€¤ã‚’ã€
ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§çµ±åˆã™ã‚‹ã€‚

ã€ç‰¹å¾´ã€‘
- ID-Based Alignment: å…¨ã¦ã®ãƒãƒ¼ã‚¸ã¯ `original_index` ã‚«ãƒ©ãƒ ã‚’ã‚­ãƒ¼ã¨ã—ã¦å®Ÿè¡Œ
- Dynamic Feature Selection: å…¨ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’å€™è£œã¨ã—ã¦æ¯”è¼ƒã—ã€æœ€é©ãªçµ„ã¿åˆã‚ã›ã‚’é¸æŠ
- Multicollinearityå¯¾ç­–: å¤šé‡å…±ç·šæ€§ã®æ¤œå‡ºã¨å¼·åŒ–ã•ã‚ŒãŸæ­£å‰‡åŒ–
- Robust Missing Value Imputation: Easy Sampleã¯Single-Stageã®äºˆæ¸¬å€¤ã§è£œå®Œ
- Intel Extension for Scikit-learn (sklearnex) ã‚µãƒãƒ¼ãƒˆ
- Intel Core Ultra 9 285K æœ€é©åŒ–

å®Ÿè¡Œæ–¹æ³•:
    python scripts/modeling/train_stage3_stacking.py

å‰ææ¡ä»¶:
    - Single-Stage OOF (`spatio_temporal_ensemble/oof_predictions.csv`) ã« `original_index` ãŒå«ã¾ã‚Œã‚‹ã“ã¨
    - Two-Stage OOF (`twostage_spatiotemporal_ensemble/oof_predictions.csv`) ã« `original_index` ãŒå«ã¾ã‚Œã‚‹ã“ã¨
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, Optional, List
import json
import argparse
import warnings
warnings.filterwarnings('ignore')

# Intel Extension for Scikit-learn (ã‚ªãƒ—ã‚·ãƒ§ãƒ³é«˜é€ŸåŒ–)
try:
    from sklearnex import patch_sklearn
    patch_sklearn()
    SKLEARNEX_AVAILABLE = True
    print("âœ… Intel Extension for Scikit-learn ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¾ã—ãŸ")
except ImportError:
    SKLEARNEX_AVAILABLE = False

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.preprocessing import StandardScaler

# ========================================
# å®šæ•°
# ========================================
RANDOM_SEED = 42
N_FOLDS = 5

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = Path("data")
SPATIO_TEMPORAL_DIR = DATA_DIR / "spatio_temporal"
PROCESSED_DIR = DATA_DIR / "processed"
RESULTS_DIR = Path("results")

# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«
STAGE1_OOF_PATH = PROCESSED_DIR / "stage1_oof_predictions.csv"
STAGE1_TEST_PATH = PROCESSED_DIR / "stage1_test_predictions.csv"
SINGLE_STAGE_OOF_PATH = RESULTS_DIR / "spatio_temporal_ensemble" / "oof_predictions.csv"
SINGLE_STAGE_TEST_PATH = RESULTS_DIR / "spatio_temporal_ensemble" / "test_predictions.csv"
TWO_STAGE_OOF_PATH = RESULTS_DIR / "twostage_spatiotemporal_ensemble" / "oof_predictions.csv"
TWO_STAGE_TEST_PATH = RESULTS_DIR / "twostage_spatiotemporal_ensemble" / "test_predictions.csv"

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = RESULTS_DIR / "stage3_stacking"


class StackingMetaModel:
    """Stage 3 Stacking ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆID-Based Alignment + Dynamic Feature Selection + Robustï¼‰"""
    
    def __init__(
        self,
        output_dir: Path = OUTPUT_DIR,
        n_folds: int = N_FOLDS,
        random_state: int = RANDOM_SEED,
        use_all_models: bool = True,
        regularization_c: float = 0.1,  # å¤šé‡å…±ç·šæ€§å¯¾ç­–ã®ãŸã‚å¼·åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0â†’0.1ï¼‰
        use_single_stage_imputation: bool = True,  # Easy Sampleã‚’Single-Stageã§è£œå®Œ
    ):
        self.output_dir = output_dir
        self.n_folds = n_folds
        self.random_state = random_state
        self.use_all_models = use_all_models
        self.regularization_c = regularization_c
        self.use_single_stage_imputation = use_single_stage_imputation
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´
        self.df_train = None
        self.df_test = None
        self.feature_names = None
        self.all_feature_names = None
        
        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒç”¨
        self.model_aucs = {}
        
        # äºˆæ¸¬æ ¼ç´
        self.oof_predictions = None
        self.test_predictions = None
        
        print("=" * 70)
        print("ğŸš€ Stage 3: Stacking Meta-Model (Enhanced Robust Version)")
        print(f"   Output: {self.output_dir}")
        print(f"   Folds: {n_folds}, Seed: {random_state}")
        print(f"   Use All Models: {use_all_models}")
        print(f"   Regularization C: {regularization_c} (ä½ã„ã»ã©æ­£å‰‡åŒ–ãŒå¼·ã„)")
        print(f"   Use Single-Stage Imputation: {use_single_stage_imputation}")
        print(f"   Intel sklearnex: {'æœ‰åŠ¹' if SKLEARNEX_AVAILABLE else 'ç„¡åŠ¹'}")
        print("=" * 70)
    
    def _check_unique(self, df: pd.DataFrame, name: str):
        """original_index ã®ä¸€æ„æ€§ã‚’ç¢ºèª"""
        if df['original_index'].duplicated().any():
            dup_count = df['original_index'].duplicated().sum()
            raise ValueError(
                f"âŒ {name} ã« original_index ã®é‡è¤‡ãŒ {dup_count} ä»¶ã‚ã‚Šã¾ã™ã€‚\n"
                "ãƒãƒ¼ã‚¸å‰ã«è§£æ¶ˆã—ã¦ãã ã•ã„ã€‚"
            )
    
    def _ensure_index_type(self, df: pd.DataFrame) -> pd.DataFrame:
        """original_index ã®å‹ã‚’çµ±ä¸€ (int)"""
        if 'original_index' in df.columns:
            df['original_index'] = df['original_index'].astype(int)
        return df
    
    def load_data(self):
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ID-Basedãƒãƒ¼ã‚¸ï¼ˆæ”¹å–„ç‰ˆï¼‰
        - é‡è¤‡ãƒã‚§ãƒƒã‚¯ã®è¿½åŠ 
        - Two-Stageã®æ¬ æè£œå®Œæˆ¦ç•¥ã®æ”¹å–„
        - original_index ã®å‹çµ±ä¸€
        """
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ï¼ˆID-Basedãƒãƒ¼ã‚¸ + æ”¹å–„ç‰ˆï¼‰...")
        
        # ========================================
        # 1. Single-Stage OOF ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å–å¾—ï¼‰
        # ========================================
        print("\n   ğŸ“Š Single-Stage OOFã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ™ãƒ¼ã‚¹ï¼‰...")
        single_oof = pd.read_csv(SINGLE_STAGE_OOF_PATH)
        single_oof = self._ensure_index_type(single_oof)
        # ã€Fixã€‘Train+ValãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€é‡è¤‡ã‚’å‰Šé™¤ï¼ˆæœ€åˆã®è¡Œã‚’ä¿æŒï¼‰
        if single_oof['original_index'].duplicated().any():
            dup_count = single_oof['original_index'].duplicated().sum()
            print(f"      âš ï¸ é‡è¤‡ {dup_count:,} ä»¶ã‚’æ¤œå‡ºï¼ˆTrain+Valå½¢å¼ï¼‰ã€‚é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™...")
            single_oof = single_oof.drop_duplicates(subset='original_index', keep='first')
        self._check_unique(single_oof, "Single-Stage OOF")
        
        print(f"      Single-Stage OOF: {len(single_oof):,} è¡Œ")
        print(f"      ã‚«ãƒ©ãƒ : {list(single_oof.columns)}")
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
        single_model_cols = [c for c in single_oof.columns 
                            if c not in ['original_index', 'target', 'ensemble']]
        print(f"      åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {single_model_cols}")
        
        # ãƒ™ãƒ¼ã‚¹DataFrameä½œæˆ
        base_cols = ['original_index', 'target'] + single_model_cols
        self.df_train = single_oof[[c for c in base_cols if c in single_oof.columns]].copy()
        
        # ã‚«ãƒ©ãƒ åã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        rename_dict = {col: f'single_{col}' for col in single_model_cols}
        self.df_train = self.df_train.rename(columns=rename_dict)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ©ãƒ ã®å‡¦ç†
        if 'ensemble' in single_oof.columns:
            self.df_train['single_ensemble'] = single_oof['ensemble'].values
        else:
            model_preds = single_oof[single_model_cols].values
            self.df_train['single_ensemble'] = model_preds.mean(axis=1)
        
        # ========================================
        # 2. Stage 1 OOF ã‚’ãƒãƒ¼ã‚¸
        # ========================================
        print("\n   ğŸ“Š Stage 1 OOFã‚’ãƒãƒ¼ã‚¸...")
        stage1_oof = pd.read_csv(STAGE1_OOF_PATH)
        stage1_oof = self._ensure_index_type(stage1_oof)
        self._check_unique(stage1_oof, "Stage 1 OOF")
        
        if 'ensemble_prob' in stage1_oof.columns:
            stage1_prob_col = 'ensemble_prob'
        elif 'prob_catboost' in stage1_oof.columns:
            stage1_oof['ensemble_prob'] = 0.85 * stage1_oof['prob_catboost'] + 0.15 * stage1_oof['prob_lgbm']
            stage1_prob_col = 'ensemble_prob'
        else:
            raise ValueError("Stage 1 OOFã«ç¢ºç‡ã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
        
        stage1_for_merge = stage1_oof[['original_index', stage1_prob_col]].copy()
        stage1_for_merge = stage1_for_merge.rename(columns={stage1_prob_col: 'stage1_prob'})
        
        self.df_train = self.df_train.merge(stage1_for_merge, on='original_index', how='left')
        
        n_merged_s1 = self.df_train['stage1_prob'].notna().sum()
        print(f"      ãƒãƒ¼ã‚¸æˆåŠŸ: {n_merged_s1:,} / {len(self.df_train):,}")
        self.df_train['stage1_prob'] = self.df_train['stage1_prob'].fillna(0)
        
        # ========================================
        # 3. Two-Stage OOF ã‚’ãƒãƒ¼ã‚¸ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å–å¾—ï¼‰
        # ========================================
        print("\n   ğŸ“Š Two-Stage OOFã‚’ãƒãƒ¼ã‚¸...")
        two_stage_oof = pd.read_csv(TWO_STAGE_OOF_PATH)
        two_stage_oof = self._ensure_index_type(two_stage_oof)
        # ã€Fixã€‘Train+ValãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€é‡è¤‡ã‚’å‰Šé™¤ï¼ˆæœ€åˆã®è¡Œã‚’ä¿æŒï¼‰
        if two_stage_oof['original_index'].duplicated().any():
            dup_count = two_stage_oof['original_index'].duplicated().sum()
            print(f"      âš ï¸ é‡è¤‡ {dup_count:,} ä»¶ã‚’æ¤œå‡ºã€‚é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™...")
            two_stage_oof = two_stage_oof.drop_duplicates(subset='original_index', keep='first')
        self._check_unique(two_stage_oof, "Two-Stage OOF")
        
        print(f"      Two-Stage OOF: {len(two_stage_oof):,} è¡Œ (Hard Samples)")
        
        two_stage_model_cols = [c for c in two_stage_oof.columns 
                               if c not in ['original_index', 'target', 'ensemble']]
        print(f"      åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {two_stage_model_cols}")
        
        two_stage_for_merge = two_stage_oof[['original_index'] + two_stage_model_cols].copy()
        rename_dict_ts = {col: f'twostage_{col}' for col in two_stage_model_cols}
        two_stage_for_merge = two_stage_for_merge.rename(columns=rename_dict_ts)
        
        self.df_train = self.df_train.merge(two_stage_for_merge, on='original_index', how='left')
        
        n_merged_ts = self.df_train['twostage_catboost'].notna().sum() if 'twostage_catboost' in self.df_train.columns else 0
        print(f"      ãƒãƒ¼ã‚¸æˆåŠŸ: {n_merged_ts:,} / {len(self.df_train):,}")
        
        # ========================================
        # ã€æ”¹å–„ã€‘Easy Sampleã®æ¬ æè£œå®Œæˆ¦ç•¥
        # ========================================
        ts_first_col = f'twostage_{two_stage_model_cols[0]}' if two_stage_model_cols else None
        if ts_first_col:
            self.df_train['is_easy_sample'] = self.df_train[ts_first_col].isna().astype(int)
        else:
            self.df_train['is_easy_sample'] = 0
        
        n_easy = self.df_train['is_easy_sample'].sum()
        print(f"\n   ğŸ“Š Easy Sample: {n_easy:,} / {len(self.df_train):,} ({n_easy/len(self.df_train)*100:.1f}%)")
        
        if self.use_single_stage_imputation:
            print("   ğŸ”§ Easy Sampleã‚’Single-Stageäºˆæ¸¬å€¤ã§è£œå®Œ...")
            for ts_col in [c for c in self.df_train.columns if c.startswith('twostage_')]:
                # å¯¾å¿œã™ã‚‹Single-Stageã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
                single_counterpart = ts_col.replace('twostage_', 'single_')
                if single_counterpart in self.df_train.columns:
                    # NaNéƒ¨åˆ†ã‚’Single-Stageã®å€¤ã§è£œå®Œ
                    mask = self.df_train[ts_col].isna()
                    self.df_train.loc[mask, ts_col] = self.df_train.loc[mask, single_counterpart]
                    filled_count = mask.sum()
                    if filled_count > 0:
                        print(f"      {ts_col}: {filled_count:,} ä»¶ã‚’ {single_counterpart} ã§è£œå®Œ")
                else:
                    # å¯¾å¿œãŒãªã„å ´åˆã¯0åŸ‹ã‚
                    self.df_train[ts_col] = self.df_train[ts_col].fillna(0)
        else:
            # å¾“æ¥ã®0åŸ‹ã‚
            for col in [c for c in self.df_train.columns if c.startswith('twostage_')]:
                self.df_train[col] = self.df_train[col].fillna(0)
        
        # ========================================
        # 4. äº¤äº’ä½œç”¨é …ã‚’è¿½åŠ 
        # ========================================
        if 'single_tabnet' in self.df_train.columns and 'twostage_catboost' in self.df_train.columns:
            self.df_train['tabnet_x_catboost'] = self.df_train['single_tabnet'] * self.df_train['twostage_catboost']
        
        # ========================================
        # 5. ç‰¹å¾´é‡å€™è£œã‚’æ•´ç†
        # ========================================
        self.all_feature_names = [c for c in self.df_train.columns 
                                 if c not in ['original_index', 'target'] 
                                 and self.df_train[c].dtype in ['float64', 'int64', 'float32', 'int32']]
        
        print(f"\n      å…¨ç‰¹å¾´é‡å€™è£œ: {len(self.all_feature_names)} å€‹")
        print(f"      å­¦ç¿’ç”¨DataFrame: {len(self.df_train):,} è¡Œ")
        
        # ========================================
        # 6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚åŒæ§˜ã«ãƒãƒ¼ã‚¸
        # ========================================
        self._load_test_data(two_stage_model_cols)
        
        # ========================================
        # 7. æ•´åˆæ€§æ¤œè¨¼
        # ========================================
        self._validate_data()
        
        print("\n   âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    
    def _load_test_data(self, two_stage_model_cols: List[str]):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        print("\n   ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸...")
        
        single_test = pd.read_csv(SINGLE_STAGE_TEST_PATH)
        single_test = self._ensure_index_type(single_test)
        self._check_unique(single_test, "Single-Stage Test")
        
        single_test_model_cols = [c for c in single_test.columns 
                                 if c not in ['original_index', 'target', 'ensemble']]
        
        base_cols_test = ['original_index'] + single_test_model_cols
        self.df_test = single_test[[c for c in base_cols_test if c in single_test.columns]].copy()
        rename_dict_test = {col: f'single_{col}' for col in single_test_model_cols}
        self.df_test = self.df_test.rename(columns=rename_dict_test)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        if 'ensemble' in single_test.columns:
            self.df_test['single_ensemble'] = single_test['ensemble'].values
        else:
            model_preds_test = single_test[single_test_model_cols].values
            self.df_test['single_ensemble'] = model_preds_test.mean(axis=1)
        
        # Stage 1 Test
        stage1_test = pd.read_csv(STAGE1_TEST_PATH)
        stage1_test = self._ensure_index_type(stage1_test)
        self._check_unique(stage1_test, "Stage 1 Test")
        
        if 'ensemble_prob' in stage1_test.columns:
            s1_test_prob_col = 'ensemble_prob'
        elif 'prob_catboost' in stage1_test.columns:
            stage1_test['ensemble_prob'] = 0.85 * stage1_test['prob_catboost'] + 0.15 * stage1_test['prob_lgbm']
            s1_test_prob_col = 'ensemble_prob'
        else:
            s1_test_prob_col = None
        
        if s1_test_prob_col:
            s1_test_merge = stage1_test[['original_index', s1_test_prob_col]].copy()
            s1_test_merge = s1_test_merge.rename(columns={s1_test_prob_col: 'stage1_prob'})
            self.df_test = self.df_test.merge(s1_test_merge, on='original_index', how='left')
            self.df_test['stage1_prob'] = self.df_test['stage1_prob'].fillna(0)
        else:
            self.df_test['stage1_prob'] = 0
        
        # Two-Stage Test
        two_stage_test = pd.read_csv(TWO_STAGE_TEST_PATH)
        two_stage_test = self._ensure_index_type(two_stage_test)
        self._check_unique(two_stage_test, "Two-Stage Test")
        
        ts_test_model_cols = [c for c in two_stage_test.columns 
                             if c not in ['original_index', 'target', 'ensemble']]
        ts_test_merge = two_stage_test[['original_index'] + ts_test_model_cols].copy()
        rename_dict_ts_test = {col: f'twostage_{col}' for col in ts_test_model_cols}
        ts_test_merge = ts_test_merge.rename(columns=rename_dict_ts_test)
        self.df_test = self.df_test.merge(ts_test_merge, on='original_index', how='left')
        
        # Easy Sample ãƒ•ãƒ©ã‚°
        ts_first_col = f'twostage_{ts_test_model_cols[0]}' if ts_test_model_cols else None
        if ts_first_col and ts_first_col in self.df_test.columns:
            self.df_test['is_easy_sample'] = self.df_test[ts_first_col].isna().astype(int)
        else:
            self.df_test['is_easy_sample'] = 0
        
        # Two-Stageã®æ¬ æè£œå®Œ
        if self.use_single_stage_imputation:
            for ts_col in [c for c in self.df_test.columns if c.startswith('twostage_')]:
                single_counterpart = ts_col.replace('twostage_', 'single_')
                if single_counterpart in self.df_test.columns:
                    mask = self.df_test[ts_col].isna()
                    self.df_test.loc[mask, ts_col] = self.df_test.loc[mask, single_counterpart]
                else:
                    self.df_test[ts_col] = self.df_test[ts_col].fillna(0)
        else:
            for col in [c for c in self.df_test.columns if c.startswith('twostage_')]:
                self.df_test[col] = self.df_test[col].fillna(0)
        
        # äº¤äº’ä½œç”¨é …
        if 'single_tabnet' in self.df_test.columns and 'twostage_catboost' in self.df_test.columns:
            self.df_test['tabnet_x_catboost'] = self.df_test['single_tabnet'] * self.df_test['twostage_catboost']
        
        # ãƒ†ã‚¹ãƒˆã®target
        raw_test = pd.read_parquet(SPATIO_TEMPORAL_DIR / "raw_test.parquet")
        if 'fatal' in raw_test.columns:
            raw_test['original_index'] = raw_test.index
            target_merge = raw_test[['original_index', 'fatal']].copy()
            target_merge = self._ensure_index_type(target_merge)
            self.df_test = self.df_test.merge(target_merge, on='original_index', how='left')
            self.df_test = self.df_test.rename(columns={'fatal': 'target'})
        
        print(f"      ãƒ†ã‚¹ãƒˆç”¨DataFrame: {len(self.df_test):,} è¡Œ")
    
    def _validate_data(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼"""
        print("\n   ğŸ” æ•´åˆæ€§æ¤œè¨¼...")
        
        # NaNãƒã‚§ãƒƒã‚¯
        for col in self.all_feature_names:
            if col in self.df_train.columns:
                train_nan = self.df_train[col].isna().sum()
                test_nan = self.df_test[col].isna().sum() if col in self.df_test.columns else 0
                if train_nan > 0 or test_nan > 0:
                    print(f"      âš ï¸ {col}: Train NaN={train_nan}, Test NaN={test_nan}")
        
        # ãƒãƒ¼ã‚¸å¾Œã®è¡Œæ•°æ¤œè¨¼
        print(f"      Trainè¡Œæ•°: {len(self.df_train):,}")
        print(f"      Testè¡Œæ•°: {len(self.df_test):,}")
    
    def evaluate_feature_sets(self):
        """
        è¤‡æ•°ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’è©•ä¾¡ã—ã€æœ€é©ãªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹
        å¤šé‡å…±ç·šæ€§ãŒé«˜ã„ã‚»ãƒƒãƒˆã«ã¯è­¦å‘Šã‚’å‡ºã™
        """
        print("\nğŸ”¬ ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®è©•ä¾¡ä¸­...")
        
        y = self.df_train['target'].values
        results = []
        
        # è©•ä¾¡ã™ã‚‹ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®å®šç¾©ï¼ˆå¤šé‡å…±ç·šæ€§ã‚’è€ƒæ…®ï¼‰
        feature_sets = {
            # åŸºæœ¬ã‚»ãƒƒãƒˆï¼ˆå¤šé‡å…±ç·šæ€§ãŒä½ã„çµ„ã¿åˆã‚ã›ï¼‰
            "baseline": ['stage1_prob', 'single_tabnet', 'twostage_catboost', 'tabnet_x_catboost', 'is_easy_sample'],
            
            # TabNeté‡è¦–ã‚»ãƒƒãƒˆï¼ˆæœ€å°é™ã®ç‰¹å¾´é‡ï¼‰
            "tabnet_focus": ['single_tabnet', 'twostage_catboost', 'is_easy_sample'],
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½¿ç”¨ã‚»ãƒƒãƒˆï¼ˆå¤šé‡å…±ç·šæ€§ãƒªã‚¹ã‚¯ä½ï¼‰
            "ensemble_based": ['stage1_prob', 'single_ensemble', 'twostage_catboost', 'is_easy_sample'],
            
            # å¤šæ§˜æ€§é‡è¦–ã‚»ãƒƒãƒˆï¼ˆç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
            "diversity": ['single_tabnet', 'single_lgbm', 'twostage_catboost', 'is_easy_sample'],
        }
        
        # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã«çµã‚‹
        valid_feature_sets = {}
        for name, cols in feature_sets.items():
            valid_cols = [c for c in cols if c in self.df_train.columns]
            if len(valid_cols) >= 2:
                valid_feature_sets[name] = valid_cols
        
        print(f"   è©•ä¾¡å¯èƒ½ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆ: {list(valid_feature_sets.keys())}")
        
        for set_name, feature_cols in valid_feature_sets.items():
            X = self.df_train[feature_cols].values
            
            # ç›¸é–¢è¡Œåˆ—ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¤šé‡å…±ç·šæ€§ã®è­¦å‘Šï¼‰
            corr_matrix = self.df_train[feature_cols].corr().abs()
            high_corr_pairs = []
            for i in range(len(feature_cols)):
                for j in range(i+1, len(feature_cols)):
                    if corr_matrix.iloc[i, j] > 0.8:
                        high_corr_pairs.append((feature_cols[i], feature_cols[j], corr_matrix.iloc[i, j]))
            
            if high_corr_pairs:
                print(f"   âš ï¸ {set_name}: é«˜ç›¸é–¢ãƒšã‚¢æ¤œå‡º")
                for pair in high_corr_pairs:
                    print(f"      - {pair[0]} â†” {pair[1]}: {pair[2]:.3f}")
            
            oof_preds = np.zeros(len(X))
            scaler = StandardScaler()
            skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
            
            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train = y[train_idx]
                
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
                
                model = LogisticRegression(
                    C=self.regularization_c,  # å¼·åŒ–ã•ã‚ŒãŸæ­£å‰‡åŒ–
                    penalty='l2', 
                    solver='lbfgs', 
                    max_iter=1000, 
                    random_state=self.random_state
                )
                model.fit(X_train_scaled, y_train)
                
                oof_preds[val_idx] = model.predict_proba(X_val_scaled)[:, 1]
            
            oof_auc = roc_auc_score(y, oof_preds)
            oof_prauc = average_precision_score(y, oof_preds)
            
            results.append({
                'set_name': set_name,
                'features': feature_cols,
                'oof_auc': oof_auc,
                'oof_prauc': oof_prauc,
                'high_corr_pairs': len(high_corr_pairs),
            })
            
            print(f"   {set_name}: AUC={oof_auc:.4f}, PR-AUC={oof_prauc:.4f}, é«˜ç›¸é–¢ãƒšã‚¢={len(high_corr_pairs)}")
        
        # æœ€è‰¯ã®ã‚»ãƒƒãƒˆã‚’é¸æŠï¼ˆPR-AUCå„ªå…ˆã€é«˜ç›¸é–¢ãƒšã‚¢ãŒå°‘ãªã„ã‚‚ã®ã‚’å¥½ã‚€ï¼‰
        # PR-AUCãŒ0.01ä»¥ä¸Šå·®ãŒãªã‘ã‚Œã°ã€é«˜ç›¸é–¢ãƒšã‚¢ãŒå°‘ãªã„æ–¹ã‚’é¸ã¶
        sorted_results = sorted(results, key=lambda x: (-x['oof_prauc'], x['high_corr_pairs']))
        best_result = sorted_results[0]
        
        self.feature_names = best_result['features']
        
        print(f"\n   ğŸ“Œ é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚»ãƒƒãƒˆ: {best_result['set_name']}")
        print(f"      ç‰¹å¾´é‡: {self.feature_names}")
        print(f"      OOF AUC: {best_result['oof_auc']:.4f}")
        print(f"      OOF PR-AUC: {best_result['oof_prauc']:.4f}")
        
        # çµæœã‚’Jsonã§ä¿å­˜
        with open(self.output_dir / "feature_selection_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return best_result
    
    def train(self):
        """ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆä¿‚æ•°ç›£è¦–ä»˜ãï¼‰"""
        print("\nğŸ§  Stacking ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
        print(f"   æ­£å‰‡åŒ–å¼·åº¦ C={self.regularization_c} (ä½ã„ã»ã©æ­£å‰‡åŒ–ãŒå¼·ã„)")
        
        X = self.df_train[self.feature_names].values
        y = self.df_train['target'].values
        X_test = self.df_test[self.feature_names].values
        
        self.oof_predictions = np.zeros(len(X))
        self.test_predictions = np.zeros(len(X_test))
        
        scaler = StandardScaler()
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        
        fold_aucs = []
        all_coefs = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test)
            
            model = LogisticRegression(
                C=self.regularization_c,
                penalty='l2',
                solver='lbfgs',
                max_iter=1000,
                random_state=self.random_state,
            )
            model.fit(X_train_scaled, y_train)
            
            val_pred = model.predict_proba(X_val_scaled)[:, 1]
            test_pred = model.predict_proba(X_test_scaled)[:, 1]
            
            self.oof_predictions[val_idx] = val_pred
            self.test_predictions += test_pred / self.n_folds
            
            fold_auc = roc_auc_score(y_val, val_pred)
            fold_aucs.append(fold_auc)
            all_coefs.append(model.coef_[0])
            print(f"      Fold {fold+1} AUC: {fold_auc:.4f}")
        
        oof_auc = roc_auc_score(y, self.oof_predictions)
        oof_prauc = average_precision_score(y, self.oof_predictions)
        
        print(f"\n   ğŸ“Š Stacking OOF AUC:    {oof_auc:.4f}")
        print(f"   ğŸ“Š Stacking OOF PR-AUC: {oof_prauc:.4f}")
        
        # ä¿‚æ•°ã®åˆ†æï¼ˆå¤šé‡å…±ç·šæ€§ã®è­¦å‘Šä»˜ãï¼‰
        print("\n   ğŸ“ˆ ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ä¿‚æ•°ç¢ºèª (è² ã®ä¿‚æ•°ã«æ³¨æ„):")
        mean_coefs = np.mean(all_coefs, axis=0)
        std_coefs = np.std(all_coefs, axis=0)
        
        coef_df = pd.DataFrame({
            'Feature': self.feature_names,
            'Coeff_Mean': mean_coefs,
            'Coeff_Std': std_coefs,
        }).sort_values(by='Coeff_Mean', ascending=False)
        
        print(coef_df.to_string(index=False))
        print(f"      intercept: {model.intercept_[0]:.4f}")
        
        # è­¦å‘Š: è² ã®ä¿‚æ•°ã¾ãŸã¯ä¸å®‰å®šãªä¿‚æ•°
        neg_coefs = coef_df[coef_df['Coeff_Mean'] < 0]
        if not neg_coefs.empty:
            print(f"\n   âš ï¸ è­¦å‘Š: ä»¥ä¸‹ã®ç‰¹å¾´é‡ã®ä¿‚æ•°ãŒè² ã«ãªã£ã¦ã„ã¾ã™ã€‚å¤šé‡å…±ç·šæ€§ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:")
            print(neg_coefs.to_string(index=False))
        
        unstable_coefs = coef_df[coef_df['Coeff_Std'] > abs(coef_df['Coeff_Mean']) * 0.5]
        if not unstable_coefs.empty:
            print(f"\n   âš ï¸ è­¦å‘Š: ä»¥ä¸‹ã®ç‰¹å¾´é‡ã®ä¿‚æ•°ãŒFoldé–“ã§ä¸å®‰å®šã§ã™:")
            print(unstable_coefs.to_string(index=False))
        
        # ä¿‚æ•°æƒ…å ±ã‚’ä¿å­˜
        coef_df.to_csv(self.output_dir / "model_coefficients.csv", index=False)
        
        return oof_auc, oof_prauc
    
    def save_results(self):
        """çµæœã®ä¿å­˜"""
        print("\nğŸ“ˆ çµæœä¿å­˜ä¸­...")
        
        y_train = self.df_train['target'].values
        y_test = self.df_test['target'].values if 'target' in self.df_test.columns and self.df_test['target'].notna().all() else None
        
        # OOFäºˆæ¸¬
        oof_df = pd.DataFrame({
            'original_index': self.df_train['original_index'].values,
            'stacking_prob': self.oof_predictions,
            'target': y_train,
        })
        oof_df.to_csv(self.output_dir / "oof_predictions.csv", index=False)
        
        # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
        test_df = pd.DataFrame({
            'original_index': self.df_test['original_index'].values,
            'stacking_prob': self.test_predictions,
        })
        if y_test is not None:
            test_df['target'] = y_test
        test_df.to_csv(self.output_dir / "test_predictions.csv", index=False)
        
        # æœ€çµ‚æå‡ºç”¨ãƒ•ã‚¡ã‚¤ãƒ«
        submission_df = pd.DataFrame({
            'original_index': self.df_test['original_index'].values,
            'prob': self.test_predictions,
        })
        submission_df.to_csv(self.output_dir / "final_submission_stacking.csv", index=False)
        
        # ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼
        oof_auc = roc_auc_score(y_train, self.oof_predictions)
        oof_prauc = average_precision_score(y_train, self.oof_predictions)
        
        scores = {
            'oof_auc': float(oof_auc),
            'oof_prauc': float(oof_prauc),
            'selected_features': self.feature_names,
            'regularization_c': self.regularization_c,
            'use_single_stage_imputation': self.use_single_stage_imputation,
        }
        if y_test is not None:
            test_auc = roc_auc_score(y_test, self.test_predictions)
            test_prauc = average_precision_score(y_test, self.test_predictions)
            scores['test_auc'] = float(test_auc)
            scores['test_prauc'] = float(test_prauc)
            print(f"   Test AUC:    {test_auc:.4f}")
            print(f"   Test PR-AUC: {test_prauc:.4f}")
        
        with open(self.output_dir / "scores.json", 'w') as f:
            json.dump(scores, f, indent=2)
        
        print(f"   âœ… å®Œäº†: {self.output_dir}")
    
    def run(self):
        """å…¨å·¥ç¨‹å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        self.load_data()
        
        if self.use_all_models:
            self.evaluate_feature_sets()
        else:
            # å¾“æ¥ã®å›ºå®šç‰¹å¾´é‡
            self.feature_names = ['stage1_prob', 'single_tabnet', 'twostage_catboost', 'tabnet_x_catboost', 'is_easy_sample']
            self.feature_names = [c for c in self.feature_names if c in self.df_train.columns]
        
        self.train()
        self.save_results()
        
        elapsed = (datetime.now() - start_time).total_seconds() / 60
        
        print("\n" + "=" * 70)
        print(f"âœ… å…¨å·¥ç¨‹å®Œäº†ï¼ å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}åˆ†")
        print("=" * 70)


def main():
    parser = argparse.ArgumentParser(description="Stage 3 Stacking Meta-Model (Enhanced Robust Version)")
    parser.add_argument(
        '--use-all-models',
        action='store_true',
        default=True,
        help='å…¨ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’å€™è£œã¨ã—ã¦å‹•çš„ã«é¸æŠï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰'
    )
    parser.add_argument(
        '--regularization-c',
        type=float,
        default=0.1,
        help='ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®æ­£å‰‡åŒ–å¼·åº¦ Cï¼ˆä½ã„ã»ã©å¼·ã„æ­£å‰‡åŒ–ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ï¼‰'
    )
    parser.add_argument(
        '--use-single-stage-imputation',
        action='store_true',
        default=True,
        help='Easy Sampleã‚’Single-Stageäºˆæ¸¬å€¤ã§è£œå®Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=str(OUTPUT_DIR),
        help='çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    args = parser.parse_args()
    
    stacking = StackingMetaModel(
        output_dir=Path(args.output_dir),
        n_folds=N_FOLDS,
        random_state=RANDOM_SEED,
        use_all_models=args.use_all_models,
        regularization_c=args.regularization_c,
        use_single_stage_imputation=args.use_single_stage_imputation,
    )
    stacking.run()


if __name__ == "__main__":
    main()

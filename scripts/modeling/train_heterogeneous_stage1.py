"""
ç•°ç¨®æ··åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« Stage 1 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
=========================================
3ç¨®é¡ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã§Stage 1ã‚’æ§‹æˆã—ã€ORæ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

ãƒ¢ãƒ‡ãƒ«æ§‹æˆ:
- Model A: LightGBM (æ±ºå®šæœ¨) - ç›¸äº’ä½œç”¨ã¨éç·šå½¢ãŒå¾—æ„
- Model B: MLP (Neural Network) - æ»‘ã‚‰ã‹ãªæ±ºå®šå¢ƒç•Œ
- Model C: Logistic Regression (ç·šå½¢) - å¤§å±€çš„ãªå‚¾å‘åˆ¤å®š

ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æˆ¦ç•¥:
- ã€Œèª°ã‹1äººã§ã‚‚å±é™ºã¨è¨€ã£ãŸã‚‰æ®‹ã™ï¼ˆORæ¡ä»¶ï¼‰ã€
- Keep if (Prob_LGBM > Th_LGBM) OR (Prob_MLP > Th_MLP) OR (Prob_LR > Th_LR)
"""

import pandas as pd
import numpy as np
import os
import gc
from datetime import datetime
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, QuantileTransformer, OrdinalEncoder
import joblib
import lightgbm as lgb
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import warnings

warnings.filterwarnings('ignore')


class SimpleMLP(nn.Module):
    """ã‚·ãƒ³ãƒ—ãƒ«ãª3å±¤MLP"""
    
    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            # æ³¨æ„: nn.Sigmoid()ã¯å‰Šé™¤ï¼BCEWithLogitsLossã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚
            # æ¨è«–æ™‚ã«torch.sigmoid()ã‚’é©ç”¨ã™ã‚‹
        )
    
    def forward(self, x):
        return self.net(x).squeeze(-1)


class HeterogeneousStage1Pipeline:
    """ç•°ç¨®æ··åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« Stage 1 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self,
        data_path: str = "data/processed/honhyo_clean_with_features.csv",
        target_col: str = "æ­»è€…æ•°",
        n_folds: int = 5,
        random_state: int = 42,
        target_recall: float = 0.99,
        undersample_ratio: float = 2.0,
        n_seeds: int = 3,
        test_size: float = 0.2,
        # MLP parameters
        mlp_hidden_dim: int = 256,
        mlp_epochs: int = 30,
        mlp_batch_size: int = 1024,  # æ±åŒ–æ€§èƒ½ã®ãŸã‚å°ã•ã‚ã«è¨­å®š
        mlp_lr: float = 0.001,
    ):
        self.data_path = data_path
        self.target_col = target_col
        self.n_folds = n_folds
        self.random_state = random_state
        self.target_recall = target_recall
        self.undersample_ratio = undersample_ratio
        self.n_seeds = n_seeds
        self.test_size = test_size
        
        # MLP parameters
        self.mlp_hidden_dim = mlp_hidden_dim
        self.mlp_epochs = mlp_epochs
        self.mlp_batch_size = mlp_batch_size
        self.mlp_lr = mlp_lr
        
        self.output_dir = "results/two_stage_model/heterogeneous_stage1"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Storage for models and predictions
        self.lgbm_models = []
        self.mlp_models = []
        self.lr_models = []
        self.scalers = []  # For MLP preprocessing
        self.ordinal_encoder = None  # For categorical encoding
        
        print("=" * 60)
        print("ç•°ç¨®æ··åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« Stage 1 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
        print(f"Target Recall: {self.target_recall:.0%}")
        print(f"Models: LightGBM + MLP + Logistic Regression")
        print(f"Strategy: OR-gate filtering")
        print(f"Test Set: {self.test_size:.0%}")
        print("=" * 60)
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨Train/Teståˆ†å‰²"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        self.df = pd.read_csv(self.data_path)
        
        y_all = self.df[self.target_col].values
        X_all = self.df.drop(columns=[self.target_col])
        
        if 'ç™ºç”Ÿæ—¥æ™‚' in X_all.columns:
            X_all = X_all.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
        
        # Train/Teståˆ†å‰² (å±¤åŒ–æŠ½å‡º)
        self.X, self.X_test, self.y, self.y_test = train_test_split(
            X_all, y_all, test_size=self.test_size,
            random_state=self.random_state, stratify=y_all
        )
        
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (Train: {1-self.test_size:.0%} / Test: {self.test_size:.0%})")
        print(f"   Train: æ­£ä¾‹ {self.y.sum():,} / {len(self.y):,}")
        print(f"   Test:  æ­£ä¾‹ {self.y_test.sum():,} / {len(self.y_test):,}")
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¨æ•°å€¤å¤‰æ•°ã®ç‰¹å®š
        known_categoricals = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
            'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
            'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
            'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
            'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰'
        ]
        
        self.categorical_cols = []
        self.numeric_cols = []
        
        for col in self.X.columns:
            if col in known_categoricals or self.X[col].dtype == 'object':
                self.categorical_cols.append(col)
                self.X[col] = self.X[col].astype('category')
                self.X_test[col] = self.X_test[col].astype('category')
            else:
                self.numeric_cols.append(col)
                self.X[col] = self.X[col].astype(np.float32)
                self.X_test[col] = self.X_test[col].astype(np.float32)
        
        self.feature_names = list(self.X.columns)
        gc.collect()
    
    def prepare_numeric_features(self, X, fit=True):
        """MLP/Logisticç”¨ã«æ•°å€¤ç‰¹å¾´é‡ã®ã¿ã‚’æŠ½å‡ºãƒ»æ­£è¦åŒ–"""
        X_numeric = X[self.numeric_cols].copy()
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’OrdinalEncoderã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ (æœªçŸ¥ã®ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œ)
        cat_data = X[self.categorical_cols].astype(str).values
        
        if fit:
            self.ordinal_encoder = OrdinalEncoder(
                handle_unknown='use_encoded_value',
                unknown_value=-1
            )
            encoded = self.ordinal_encoder.fit_transform(cat_data)
        else:
            encoded = self.ordinal_encoder.transform(cat_data)
        
        for i, col in enumerate(self.categorical_cols):
            X_numeric[col] = encoded[:, i].astype(np.float32)
        
        # æ¬ æå€¤å‡¦ç†
        X_numeric = X_numeric.fillna(0)
        
        return X_numeric
    
    def train_lgbm(self):
        """Model A: LightGBM (æ±ºå®šæœ¨)"""
        print("\nğŸŒ² Model A: LightGBM å­¦ç¿’ä¸­...")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba_lgbm = np.zeros(len(self.y))
        
        lgb_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'num_leaves': 31,
            'max_depth': 8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'is_unbalance': False,
            'n_estimators': 1000,
            'learning_rate': 0.05,
            'n_jobs': -1
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            X_train_full = self.X.iloc[train_idx]
            X_val = self.X.iloc[val_idx]
            y_train_full = self.y[train_idx]
            y_val = self.y[val_idx]
            
            fold_models = []
            fold_proba = np.zeros(len(val_idx))
            
            for seed in range(self.n_seeds):
                np.random.seed(self.random_state + seed)
                
                # Under-sampling
                pos_idx = np.where(y_train_full == 1)[0]
                neg_idx = np.where(y_train_full == 0)[0]
                n_pos = len(pos_idx)
                n_neg_sample = int(n_pos * self.undersample_ratio)
                neg_sample_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
                
                train_idx_sampled = np.concatenate([pos_idx, neg_sample_idx])
                X_train = X_train_full.iloc[train_idx_sampled]
                y_train = y_train_full[train_idx_sampled]
                
                model = lgb.LGBMClassifier(**lgb_params, random_state=self.random_state + seed)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(50, verbose=False)]
                )
                
                fold_proba += model.predict_proba(X_val)[:, 1] / self.n_seeds
                fold_models.append(model)
            
            self.oof_proba_lgbm[val_idx] = fold_proba
            self.lgbm_models.append(fold_models)
            
            del X_train, X_val
            gc.collect()
        
        oof_auc = roc_auc_score(self.y, self.oof_proba_lgbm)
        print(f"   LightGBM OOF AUC: {oof_auc:.4f}")
    
    def train_mlp(self):
        """Model B: MLP (Neural Network)"""
        print("\nğŸ§  Model B: MLP å­¦ç¿’ä¸­...")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"   Device: {device}")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba_mlp = np.zeros(len(self.y))
        
        # æ•°å€¤ç‰¹å¾´é‡ã‚’æº–å‚™
        X_numeric_all = self.prepare_numeric_features(self.X, fit=True)
        input_dim = X_numeric_all.shape[1]
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            X_train = X_numeric_all.iloc[train_idx].values
            X_val = X_numeric_all.iloc[val_idx].values
            y_train = self.y[train_idx]
            y_val = self.y[val_idx]
            
            # Under-sampling
            pos_idx = np.where(y_train == 1)[0]
            neg_idx = np.where(y_train == 0)[0]
            n_pos = len(pos_idx)
            n_neg_sample = int(n_pos * self.undersample_ratio)
            np.random.seed(self.random_state + fold)
            neg_sample_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
            train_idx_sampled = np.concatenate([pos_idx, neg_sample_idx])
            
            X_train_sampled = X_train[train_idx_sampled]
            y_train_sampled = y_train[train_idx_sampled]
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = QuantileTransformer(output_distribution='normal', random_state=self.random_state)
            X_train_scaled = scaler.fit_transform(X_train_sampled)
            X_val_scaled = scaler.transform(X_val)
            self.scalers.append(scaler)
            
            # PyTorchãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train_scaled),
                torch.FloatTensor(y_train_sampled)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val_scaled),
                torch.FloatTensor(y_val)
            )
            
            train_loader = DataLoader(train_dataset, batch_size=self.mlp_batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=self.mlp_batch_size, shuffle=False)
            
            # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
            model = SimpleMLP(input_dim, self.mlp_hidden_dim).to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=self.mlp_lr)
            
            # æ­£ä¾‹ã®é‡ã¿ã‚’è¨ˆç®—
            pos_weight = torch.tensor([len(y_train_sampled) / (2 * y_train_sampled.sum())]).to(device)
            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            
            # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
            best_auc = 0
            patience_counter = 0
            best_model_state = None
            
            for epoch in range(self.mlp_epochs):
                model.train()
                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    optimizer.zero_grad()
                    output = model(batch_X)
                    loss = criterion(output, batch_y)
                    loss.backward()
                    optimizer.step()
                
                # æ¤œè¨¼
                model.eval()
                val_preds = []
                with torch.no_grad():
                    for batch_X, _ in val_loader:
                        batch_X = batch_X.to(device)
                        val_preds.append(model(batch_X).cpu().numpy())
                
                val_proba = np.concatenate(val_preds)
                # Logitsãªã®ã§sigmoidã‚’é©ç”¨
                val_proba = 1.0 / (1.0 + np.exp(-val_proba))
                val_auc = roc_auc_score(y_val, val_proba)
                
                if val_auc > best_auc:
                    best_auc = val_auc
                    best_model_state = model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= 5:
                        break
            
            # Best model ã§äºˆæ¸¬
            model.load_state_dict(best_model_state)
            model.eval()
            with torch.no_grad():
                val_X_tensor = torch.FloatTensor(X_val_scaled).to(device)
                logits = model(val_X_tensor).cpu().numpy()
                # Logitsãªã®ã§sigmoidã‚’é©ç”¨
                fold_proba = 1.0 / (1.0 + np.exp(-logits))
            
            self.oof_proba_mlp[val_idx] = fold_proba
            self.mlp_models.append(model.cpu())
            
            del train_loader, val_loader
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        oof_auc = roc_auc_score(self.y, self.oof_proba_mlp)
        print(f"   MLP OOF AUC: {oof_auc:.4f}")
    
    def train_logistic_regression(self):
        """Model C: Logistic Regression (ç·šå½¢)"""
        print("\nğŸ“ˆ Model C: Logistic Regression å­¦ç¿’ä¸­...")
        
        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
        self.oof_proba_lr = np.zeros(len(self.y))
        
        # æ•°å€¤ç‰¹å¾´é‡ã‚’æº–å‚™
        X_numeric_all = self.prepare_numeric_features(self.X, fit=True)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.X, self.y)):
            print(f"   Fold {fold+1}/{self.n_folds}...")
            
            X_train = X_numeric_all.iloc[train_idx].values
            X_val = X_numeric_all.iloc[val_idx].values
            y_train = self.y[train_idx]
            y_val = self.y[val_idx]
            
            # Under-sampling
            pos_idx = np.where(y_train == 1)[0]
            neg_idx = np.where(y_train == 0)[0]
            n_pos = len(pos_idx)
            n_neg_sample = int(n_pos * self.undersample_ratio)
            np.random.seed(self.random_state + fold)
            neg_sample_idx = np.random.choice(neg_idx, size=min(n_neg_sample, len(neg_idx)), replace=False)
            train_idx_sampled = np.concatenate([pos_idx, neg_sample_idx])
            
            X_train_sampled = X_train[train_idx_sampled]
            y_train_sampled = y_train[train_idx_sampled]
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_sampled)
            X_val_scaled = scaler.transform(X_val)
            
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model = LogisticRegression(
                max_iter=1000,
                solver='lbfgs',
                class_weight='balanced',
                random_state=self.random_state
            )
            model.fit(X_train_scaled, y_train_sampled)
            
            # äºˆæ¸¬
            fold_proba = model.predict_proba(X_val_scaled)[:, 1]
            self.oof_proba_lr[val_idx] = fold_proba
            self.lr_models.append((model, scaler))
            
            gc.collect()
        
        oof_auc = roc_auc_score(self.y, self.oof_proba_lr)
        print(f"   Logistic Regression OOF AUC: {oof_auc:.4f}")
    
    def find_individual_thresholds(self, target_recall=0.995):
        """å„ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥é–¾å€¤ã‚’æ±ºå®šï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¾¼ã¿ï¼‰"""
        print(f"\nğŸ¯ å€‹åˆ¥é–¾å€¤æ±ºå®š (Target Recall: {target_recall:.1%})...")
        
        self.thresholds = {}
        
        for name, proba in [
            ('lgbm', self.oof_proba_lgbm),
            ('mlp', self.oof_proba_mlp),
            ('lr', self.oof_proba_lr)
        ]:
            # Recall >= target_recallã‚’é”æˆã™ã‚‹æœ€å¤§ã®é–¾å€¤ã‚’æ¢ç´¢
            for thresh in np.arange(0.001, 0.5, 0.001):
                pred = (proba >= thresh).astype(int)
                rec = recall_score(self.y, pred)
                if rec < target_recall:
                    self.thresholds[name] = thresh - 0.001
                    break
            else:
                self.thresholds[name] = 0.001
            
            pred = (proba >= self.thresholds[name]).astype(int)
            rec = recall_score(self.y, pred)
            filter_rate = 1 - pred.mean()
            print(f"   {name.upper()}: é–¾å€¤={self.thresholds[name]:.4f}, Recall={rec:.4f}, å‰Šæ¸›ç‡={filter_rate:.2%}")
    
    def evaluate_or_gate(self):
        """ORæ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®è©•ä¾¡"""
        print("\nğŸ”— ORæ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡...")
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®å€‹åˆ¥åˆ¤å®š
        pred_lgbm = (self.oof_proba_lgbm >= self.thresholds['lgbm']).astype(int)
        pred_mlp = (self.oof_proba_mlp >= self.thresholds['mlp']).astype(int)
        pred_lr = (self.oof_proba_lr >= self.thresholds['lr']).astype(int)
        
        # ORæ¡ä»¶: ã„ãšã‚Œã‹ãŒ1ãªã‚‰1
        pred_or = np.maximum.reduce([pred_lgbm, pred_mlp, pred_lr])
        
        # è©•ä¾¡
        or_recall = recall_score(self.y, pred_or)
        or_precision = precision_score(self.y, pred_or) if pred_or.sum() > 0 else 0
        or_filter_rate = 1 - pred_or.mean()
        
        print(f"\n   ğŸ“Š ORæ¡ä»¶çµæœ:")
        print(f"      Recall: {or_recall:.4f}")
        print(f"      Precision: {or_precision:.4f}")
        print(f"      å‰Šæ¸›ç‡ (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‡): {or_filter_rate:.2%}")
        print(f"      æ®‹å­˜ãƒ‡ãƒ¼ã‚¿: {pred_or.sum():,} / {len(pred_or):,}")
        
        # æ¯”è¼ƒ: å˜ç‹¬ãƒ¢ãƒ‡ãƒ« vs ORæ¡ä»¶
        print(f"\n   ğŸ“ˆ æ¯”è¼ƒ:")
        print(f"      LGBMå˜ç‹¬: å‰Šæ¸›ç‡={1-pred_lgbm.mean():.2%}, Recall={recall_score(self.y, pred_lgbm):.4f}")
        print(f"      MLPå˜ç‹¬:  å‰Šæ¸›ç‡={1-pred_mlp.mean():.2%}, Recall={recall_score(self.y, pred_mlp):.4f}")
        print(f"      LRå˜ç‹¬:   å‰Šæ¸›ç‡={1-pred_lr.mean():.2%}, Recall={recall_score(self.y, pred_lr):.4f}")
        print(f"      ORæ¡ä»¶:   å‰Šæ¸›ç‡={or_filter_rate:.2%}, Recall={or_recall:.4f}")
        
        self.or_results = {
            'or_recall': or_recall,
            'or_precision': or_precision,
            'or_filter_rate': or_filter_rate,
            'lgbm_filter_rate': 1 - pred_lgbm.mean(),
            'mlp_filter_rate': 1 - pred_mlp.mean(),
            'lr_filter_rate': 1 - pred_lr.mean(),
            'lgbm_recall': recall_score(self.y, pred_lgbm),
            'mlp_recall': recall_score(self.y, pred_mlp),
            'lr_recall': recall_score(self.y, pred_lr),
        }
        
        return self.or_results
    
    def evaluate_test_set(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡"""
        print("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡...")
        
        # LightGBM predictions on test
        test_proba_lgbm = np.zeros(len(self.y_test))
        for fold_models in self.lgbm_models:
            for model in fold_models:
                test_proba_lgbm += model.predict_proba(self.X_test)[:, 1]
        test_proba_lgbm /= (self.n_folds * self.n_seeds)
        
        # MLP predictions on test
        X_test_numeric = self.prepare_numeric_features(self.X_test, fit=False)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        test_proba_mlp = np.zeros(len(self.y_test))
        
        for fold, (model, scaler) in enumerate(zip(self.mlp_models, self.scalers)):
            X_test_scaled = scaler.transform(X_test_numeric.values)
            model = model.to(device)
            model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X_test_scaled).to(device)
                logits = model(X_tensor).cpu().numpy()
                # Logitsãªã®ã§sigmoidã‚’é©ç”¨
                fold_proba = 1.0 / (1.0 + np.exp(-logits))
            test_proba_mlp += fold_proba / self.n_folds
            model.cpu()
        
        # Logistic Regression predictions on test
        test_proba_lr = np.zeros(len(self.y_test))
        for model, scaler in self.lr_models:
            X_test_scaled = scaler.transform(X_test_numeric.values)
            test_proba_lr += model.predict_proba(X_test_scaled)[:, 1] / self.n_folds
        
        # å€‹åˆ¥åˆ¤å®š
        pred_lgbm = (test_proba_lgbm >= self.thresholds['lgbm']).astype(int)
        pred_mlp = (test_proba_mlp >= self.thresholds['mlp']).astype(int)
        pred_lr = (test_proba_lr >= self.thresholds['lr']).astype(int)
        
        # ORæ¡ä»¶
        pred_or = np.maximum.reduce([pred_lgbm, pred_mlp, pred_lr])
        
        test_recall = recall_score(self.y_test, pred_or)
        test_precision = precision_score(self.y_test, pred_or) if pred_or.sum() > 0 else 0
        test_filter_rate = 1 - pred_or.mean()
        
        print(f"\n   ğŸ“Š ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ ORæ¡ä»¶çµæœ:")
        print(f"      Recall: {test_recall:.4f}")
        print(f"      Precision: {test_precision:.4f}")
        print(f"      å‰Šæ¸›ç‡ (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‡): {test_filter_rate:.2%}")
        
        self.test_results = {
            'test_or_recall': test_recall,
            'test_or_precision': test_precision,
            'test_or_filter_rate': test_filter_rate,
        }
        
        return self.test_results
    
    def generate_report(self, elapsed_sec: float):
        """å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownã§å‡ºåŠ›"""
        report_path = os.path.join(self.output_dir, "experiment_report.md")
        
        report_content = f"""# ç•°ç¨®æ··åˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« Stage 1 å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å®Ÿè¡Œæ™‚é–“**: {elapsed_sec:.1f}ç§’

## ãƒ¢ãƒ‡ãƒ«æ§‹æˆ

| ãƒ¢ãƒ‡ãƒ« | èª¬æ˜ |
|--------|------|
| LightGBM | æ±ºå®šæœ¨ãƒ™ãƒ¼ã‚¹ã€ç›¸äº’ä½œç”¨ã¨éç·šå½¢ãŒå¾—æ„ |
| MLP | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã€æ»‘ã‚‰ã‹ãªæ±ºå®šå¢ƒç•Œ |
| Logistic Regression | ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã€å¤§å±€çš„ãªå‚¾å‘åˆ¤å®š |

## æˆ¦ç•¥: ORæ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

ã€Œèª°ã‹1äººã§ã‚‚å±é™ºã¨è¨€ã£ãŸã‚‰æ®‹ã™ï¼ˆORæ¡ä»¶ï¼‰ã€

```
Keep if (Prob_LGBM > Th_LGBM) OR (Prob_MLP > Th_MLP) OR (Prob_LR > Th_LR)
```

## å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«çµæœ (CV OOF)

| ãƒ¢ãƒ‡ãƒ« | é–¾å€¤ | Recall | å‰Šæ¸›ç‡ |
|--------|------|--------|--------|
| LightGBM | {self.thresholds['lgbm']:.4f} | {self.or_results['lgbm_recall']:.4f} | {self.or_results['lgbm_filter_rate']:.2%} |
| MLP | {self.thresholds['mlp']:.4f} | {self.or_results['mlp_recall']:.4f} | {self.or_results['mlp_filter_rate']:.2%} |
| Logistic Regression | {self.thresholds['lr']:.4f} | {self.or_results['lr_recall']:.4f} | {self.or_results['lr_filter_rate']:.2%} |

## ORæ¡ä»¶çµæœ (CV OOF)

| æŒ‡æ¨™ | å€¤ |
|------|----| 
| Recall | {self.or_results['or_recall']:.4f} |
| Precision | {self.or_results['or_precision']:.4f} |
| å‰Šæ¸›ç‡ | {self.or_results['or_filter_rate']:.2%} |

## ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆçµæœ

| æŒ‡æ¨™ | å€¤ |
|------|----| 
| Recall | {self.test_results['test_or_recall']:.4f} |
| Precision | {self.test_results['test_or_precision']:.4f} |
| å‰Šæ¸›ç‡ | {self.test_results['test_or_filter_rate']:.2%} |

## è€ƒå¯Ÿ

- ORæ¡ä»¶ã«ã‚ˆã‚Šã€å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ­»è§’ã‚’è£œå®Œã—åˆã„ã€Recall {self.or_results['or_recall']:.2%} ã‚’é”æˆã€‚
- å‰Šæ¸›ç‡ {self.or_results['or_filter_rate']:.2%} ã§Stage 2ã«æ¸¡ã™ãƒ‡ãƒ¼ã‚¿é‡ã‚’å‰Šæ¸›ã€‚
- ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã‚‚é¡ä¼¼ã®çµæœãŒå¾—ã‚‰ã‚Œã€æ±åŒ–æ€§èƒ½ã‚’ç¢ºèªã€‚
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\n   ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {report_path}")
        return report_path
    
    def save_models(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        save_dir = os.path.join(self.output_dir, "models")
        os.makedirs(save_dir, exist_ok=True)
        
        # LightGBM
        joblib.dump(self.lgbm_models, os.path.join(save_dir, "lgbm_models.pkl"))
        
        # Logistic Regression (ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼)
        joblib.dump(self.lr_models, os.path.join(save_dir, "lr_models.pkl"))
        
        # MLP (PyTorch models - state_dict)
        mlp_state_dicts = [model.state_dict() for model in self.mlp_models]
        torch.save(mlp_state_dicts, os.path.join(save_dir, "mlp_models.pt"))
        
        # Scalers (MLPç”¨)
        joblib.dump(self.scalers, os.path.join(save_dir, "mlp_scalers.pkl"))
        
        # OrdinalEncoder
        joblib.dump(self.ordinal_encoder, os.path.join(save_dir, "ordinal_encoder.pkl"))
        
        # Thresholds
        joblib.dump(self.thresholds, os.path.join(save_dir, "thresholds.pkl"))
        
        print(f"\n   ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {save_dir}/")
    
    def run(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        start = datetime.now()
        
        self.load_data()
        self.train_lgbm()
        self.train_mlp()
        self.train_logistic_regression()
        self.find_individual_thresholds(target_recall=0.995)  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¾¼ã¿
        self.evaluate_or_gate()
        self.evaluate_test_set()
        
        elapsed_sec = (datetime.now() - start).total_seconds()
        
        # çµæœä¿å­˜
        results = {**self.or_results, **self.test_results, 'elapsed_sec': elapsed_sec}
        pd.DataFrame([results]).to_csv(os.path.join(self.output_dir, "results.csv"), index=False)
        
        self.generate_report(elapsed_sec)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_models()
        
        print("\n" + "=" * 60)
        print("âœ… å®Œäº†ï¼")
        print(f"   çµæœCSV: {self.output_dir}/results.csv")
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆMD: {self.output_dir}/experiment_report.md")
        print("=" * 60)
        
        return results


if __name__ == "__main__":
    pipeline = HeterogeneousStage1Pipeline()
    pipeline.run()

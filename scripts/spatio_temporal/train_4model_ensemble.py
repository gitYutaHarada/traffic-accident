"""
4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ + ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« (é«˜æ€§èƒ½PCæœ€é©åŒ–ç‰ˆ v2)
==============================================
Intel Core Ultra 9 285K (24ã‚³ã‚¢) + 64GB RAM å‘ã‘ã«æœ€é©åŒ–

ä¿®æ­£ç‚¹ (v2):
1. MLP: Target Encoding ã‚’ä½¿ç”¨ï¼ˆLabel Encoding + Scaler ã®ç†è«–çš„æ¬ é™¥ã‚’ä¿®æ­£ï¼‰
2. MLP: Train/Valåˆ†å‰²å¾Œã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: LogLoss ã«ã‚ˆã‚‹é‡ã¿æœ€é©åŒ–ï¼ˆF1+å›ºå®šé–¾å€¤ã®å•é¡Œã‚’ä¿®æ­£ï¼‰

ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: honhyo_for_analysis_with_traffic_hospital_no_leakage.csv
æ¯”è¼ƒãƒ¢ãƒ‡ãƒ«: LightGBM, CatBoost, TabNet, MLP

å®Ÿè¡Œæ–¹æ³•:
    python scripts/spatio_temporal/train_4model_ensemble.py
"""

import pandas as pd
import numpy as np
import json
import os
import gc
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_recall_curve, 
    precision_score, recall_score, f1_score, log_loss
)
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from scipy.optimize import minimize
import warnings

warnings.filterwarnings('ignore')

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# LightGBM & CatBoost
import lightgbm as lgb
from catboost import CatBoostClassifier

# TabNet
from pytorch_tabnet.tab_model import TabNetClassifier

# ============================================================================
# è¨­å®š (Intel Core Ultra 9 285K + 64GB RAM æœ€é©åŒ–)
# ============================================================================
DATA_PATH = Path("data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv")
OUTPUT_DIR = Path("results/spatio_temporal/4model_ensemble")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
N_JOBS = 20
N_FOLDS = 5
RANDOM_SEED = 42
TEST_SIZE = 0.2

LGB_N_JOBS = 10
CAT_THREADS = 10
TABNET_BATCH = 8192
MLP_BATCH = 4096

torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ Device: {DEVICE}")
print(f"ğŸ”§ Parallel Jobs: {N_JOBS}")


# ============================================================================
# Target Encoding ã‚¯ãƒ©ã‚¹ï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ç‰ˆï¼‰
# ============================================================================
class TargetEncoder:
    """
    Target Encoding: ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã§ç½®æ›
    - Trainæ™‚ã®ã¿fitã—ã€Valã«ã¯transformã®ã¿é©ç”¨ï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
    - æœªçŸ¥ã‚«ãƒ†ã‚´ãƒªã«ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹³å‡ã‚’ä½¿ç”¨
    """
    def __init__(self, smoothing=10.0):
        self.smoothing = smoothing
        self.global_mean = None
        self.encodings = {}
    
    def fit(self, X: pd.DataFrame, y: np.ndarray, columns: list):
        self.global_mean = y.mean()
        self.columns = columns
        
        for col in columns:
            stats = X.groupby(col)[y.name if hasattr(y, 'name') else 'target'].agg(['mean', 'count'])
            # Smoothing: (n * mean + m * global_mean) / (n + m)
            smoothed = (stats['count'] * stats['mean'] + self.smoothing * self.global_mean) / (stats['count'] + self.smoothing)
            self.encodings[col] = smoothed.to_dict()
        
        return self
    
    def fit_transform(self, X: pd.DataFrame, y: np.ndarray, columns: list):
        # yã‚’Seriesã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
        y_series = pd.Series(y, name='target', index=X.index)
        X_temp = X.copy()
        X_temp['target'] = y_series
        
        self.global_mean = y.mean()
        self.columns = columns
        
        X_encoded = X.copy()
        for col in columns:
            stats = X_temp.groupby(col)['target'].agg(['mean', 'count'])
            smoothed = (stats['count'] * stats['mean'] + self.smoothing * self.global_mean) / (stats['count'] + self.smoothing)
            self.encodings[col] = smoothed.to_dict()
            X_encoded[col] = X[col].map(self.encodings[col]).fillna(self.global_mean)
        
        return X_encoded
    
    def transform(self, X: pd.DataFrame):
        X_encoded = X.copy()
        for col in self.columns:
            X_encoded[col] = X[col].map(self.encodings[col]).fillna(self.global_mean)
        return X_encoded


# ============================================================================
# MLP ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ============================================================================
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.75, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        
    def forward(self, inputs, targets):
        bce = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)
        p = torch.sigmoid(inputs)
        pt = targets * p + (1 - targets) * (1 - p)
        alpha_w = targets * self.alpha + (1 - targets) * (1 - self.alpha)
        focal_w = alpha_w * (1 - pt) ** self.gamma
        return (focal_w * bce).mean()


class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
        )
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        return self.net(x)


# ============================================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ============================================================================
def load_data():
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    df = pd.read_csv(DATA_PATH)
    print(f"   ãƒ‡ãƒ¼ã‚¿: {len(df):,} ä»¶, {len(df.columns)} åˆ—")
    
    target_col = 'fatal'
    y = df[target_col].astype(int).values
    X = df.drop(columns=[target_col])
    
    if 'ç™ºç”Ÿæ—¥æ™‚' in X.columns:
        X = X.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
    
    known_cats = ['éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰', 'æ˜¼å¤œ', 'å¤©å€™', 
                  'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ', 'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 
                  'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†', 'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 
                  'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰']
    
    cat_cols = []
    num_cols = []
    for col in X.columns:
        if col in known_cats or X[col].dtype == 'object':
            cat_cols.append(col)
            X[col] = X[col].astype(str)
        else:
            num_cols.append(col)
            X[col] = X[col].astype(np.float32)
    
    print(f"   ã‚«ãƒ†ã‚´ãƒª: {len(cat_cols)}, æ•°å€¤: {len(num_cols)}")
    print(f"   ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: Neg={sum(y==0):,}, Pos={sum(y==1):,} ({sum(y==1)/len(y)*100:.2f}%)")
    
    return X, y, cat_cols, num_cols


# ============================================================================
# å„ãƒ¢ãƒ‡ãƒ«ã®Foldå­¦ç¿’é–¢æ•°
# ============================================================================
def train_lgb_fold(fold, X_tr, y_tr, X_val, y_val, cat_cols):
    """LightGBM å˜ä¸€Foldå­¦ç¿’"""
    X_tr_lgb = X_tr.copy()
    X_val_lgb = X_val.copy()
    for col in cat_cols:
        if col in X_tr_lgb.columns:
            X_tr_lgb[col] = X_tr_lgb[col].astype('category')
            X_val_lgb[col] = X_val_lgb[col].astype('category')
    
    n_pos = y_tr.sum()
    n_neg = len(y_tr) - n_pos
    scale_pos = n_neg / n_pos if n_pos > 0 else 1.0
    
    model = lgb.LGBMClassifier(
        objective='binary', metric='auc', boosting_type='gbdt',
        num_leaves=127, max_depth=-1, min_child_samples=44,
        reg_alpha=2.4, reg_lambda=2.3, colsample_bytree=0.87,
        subsample=0.63, learning_rate=0.05, n_estimators=500,
        scale_pos_weight=scale_pos, n_jobs=LGB_N_JOBS, verbosity=-1,
        random_state=RANDOM_SEED + fold
    )
    model.fit(X_tr_lgb, y_tr, eval_set=[(X_val_lgb, y_val)],
              callbacks=[lgb.early_stopping(30, verbose=False)])
    
    pred = model.predict_proba(X_val_lgb)[:, 1]
    return fold, pred, model


def train_cat_fold(fold, X_tr, y_tr, X_val, y_val, cat_cols):
    """CatBoost å˜ä¸€Foldå­¦ç¿’"""
    cat_features = [c for c in cat_cols if c in X_tr.columns]
    
    model = CatBoostClassifier(
        iterations=500, learning_rate=0.05, depth=8, l2_leaf_reg=3,
        loss_function='Logloss', eval_metric='AUC', random_seed=RANDOM_SEED + fold,
        verbose=False, early_stopping_rounds=30, task_type='CPU',
        thread_count=CAT_THREADS, cat_features=cat_features
    )
    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=False)
    pred = model.predict_proba(X_val)[:, 1]
    return fold, pred, model


def train_tabnet_fold(fold, X_tr, y_tr, X_val, y_val, cat_cols, num_cols):
    """TabNet å˜ä¸€Foldå­¦ç¿’"""
    imputer = SimpleImputer(strategy='mean')
    scaler = StandardScaler()
    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
    
    X_num_tr = scaler.fit_transform(imputer.fit_transform(X_tr[num_cols].values))
    X_cat_tr = encoder.fit_transform(X_tr[cat_cols].values) + 1
    X_tr_tab = np.hstack([X_num_tr, X_cat_tr]).astype(np.float32)
    
    X_num_val = scaler.transform(imputer.transform(X_val[num_cols].values))
    X_cat_val = encoder.transform(X_val[cat_cols].values) + 1
    X_val_tab = np.hstack([X_num_val, X_cat_val]).astype(np.float32)
    
    cat_idxs = list(range(len(num_cols), len(num_cols) + len(cat_cols)))
    cat_dims = [int(X_tr_tab[:, i].max() + 2) for i in cat_idxs]
    
    model = TabNetClassifier(
        n_d=32, n_a=32, n_steps=5, gamma=1.5,
        cat_idxs=cat_idxs, cat_dims=cat_dims, cat_emb_dim=1,
        optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=0.02),
        scheduler_fn=torch.optim.lr_scheduler.StepLR,
        scheduler_params=dict(step_size=10, gamma=0.9),
        seed=RANDOM_SEED + fold, verbose=0
    )
    model.fit(
        X_tr_tab, y_tr.astype(int),
        eval_set=[(X_val_tab, y_val.astype(int))],
        eval_metric=['auc'],
        max_epochs=50, patience=10, batch_size=TABNET_BATCH, virtual_batch_size=256
    )
    pred = model.predict_proba(X_val_tab)[:, 1]
    return fold, pred, model, (imputer, scaler, encoder)


def train_mlp_fold(fold, X_tr, y_tr, X_val, y_val, num_cols, cat_cols):
    """
    MLP å˜ä¸€Foldå­¦ç¿’ (v2: Target Encodingä½¿ç”¨, ãƒªãƒ¼ã‚¯é˜²æ­¢)
    
    ä¿®æ­£ç‚¹:
    - ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«Target Encodingã‚’ä½¿ç”¨ï¼ˆé †åºæ€§ã®ä»®å®šã‚’å›é¿ï¼‰
    - Train dataã®ã¿ã§Encoder/Scalerã‚’fitï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
    """
    # Target Encoding: Trainã®ã¿ã§fitã€Valã«ã¯transformã®ã¿
    target_encoder = TargetEncoder(smoothing=10.0)
    X_tr_encoded = target_encoder.fit_transform(X_tr.copy(), y_tr, cat_cols)
    X_val_encoded = target_encoder.transform(X_val.copy())
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã®æ¬ æè£œå®Œ
    imputer = SimpleImputer(strategy='mean')
    X_tr_encoded[num_cols] = imputer.fit_transform(X_tr_encoded[num_cols])
    X_val_encoded[num_cols] = imputer.transform(X_val_encoded[num_cols])
    
    # StandardScaler: Trainã®ã¿ã§fit
    scaler = StandardScaler()
    X_tr_scaled = scaler.fit_transform(X_tr_encoded.values.astype(np.float32))
    X_val_scaled = scaler.transform(X_val_encoded.values.astype(np.float32))
    
    train_ds = TensorDataset(
        torch.tensor(X_tr_scaled, dtype=torch.float32),
        torch.tensor(y_tr, dtype=torch.float32).unsqueeze(1)
    )
    val_ds = TensorDataset(
        torch.tensor(X_val_scaled, dtype=torch.float32),
        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)
    )
    
    train_loader = DataLoader(train_ds, batch_size=MLP_BATCH, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_ds, batch_size=MLP_BATCH, shuffle=False, num_workers=0)
    
    model = MLPClassifier(input_dim=X_tr_scaled.shape[1], hidden_dim=256).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = FocalLoss()
    
    best_auc = 0.0
    patience_cnt = 0
    best_state = None
    
    for epoch in range(50):
        model.train()
        for bx, by in train_loader:
            bx, by = bx.to(DEVICE), by.to(DEVICE)
            optimizer.zero_grad()
            loss = criterion(model(bx), by)
            loss.backward()
            optimizer.step()
        
        model.eval()
        preds, targets = [], []
        with torch.no_grad():
            for bx, by in val_loader:
                preds.extend(torch.sigmoid(model(bx.to(DEVICE))).cpu().numpy().flatten())
                targets.extend(by.numpy().flatten())
        
        val_auc = roc_auc_score(targets, preds)
        if val_auc > best_auc:
            best_auc = val_auc
            best_state = model.state_dict().copy()
            patience_cnt = 0
        else:
            patience_cnt += 1
            if patience_cnt >= 10:
                break
    
    model.load_state_dict(best_state)
    model.eval()
    with torch.no_grad():
        X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32).to(DEVICE)
        pred = torch.sigmoid(model(X_val_t)).cpu().numpy().flatten()
    
    # preprocessorsã‚’è¿”ã™ï¼ˆTestæ™‚ã«å†åˆ©ç”¨ï¼‰
    return fold, pred, model, (target_encoder, imputer, scaler)


# ============================================================================
# ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ============================================================================
def main():
    start = datetime.now()
    print("=" * 70)
    print(" ğŸš€ 4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ + ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« v2 (ä¿®æ­£ç‰ˆ)")
    print("   - MLP: Target Encoding (ãƒªãƒ¼ã‚¯é˜²æ­¢)")
    print("   - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: LogLossæœ€é©åŒ–")
    print("=" * 70)
    
    X, y, cat_cols, num_cols = load_data()
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y
    )
    X_train = X_train.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
    
    print(f"\nğŸ“Š Train: {len(y_train):,} (Pos: {y_train.sum():,})")
    print(f"ğŸ“Š Test:  {len(y_test):,} (Pos: {y_test.sum():,})")
    
    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)
    folds = list(skf.split(X_train, y_train))
    
    oof_lgb = np.zeros(len(y_train))
    oof_cat = np.zeros(len(y_train))
    oof_tab = np.zeros(len(y_train))
    oof_mlp = np.zeros(len(y_train))
    
    models_lgb = []
    models_cat = []
    models_tab = []
    models_mlp = []
    tab_preprocessors = []
    mlp_preprocessors = []
    
    print("\n" + "=" * 70)
    print(" ğŸŒ¿ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ (5-Fold CV)")
    print("=" * 70)
    
    for fold, (train_idx, val_idx) in enumerate(folds):
        if fold < 4:  # Fold 1, 2, 3, 4 ã¯å®Œäº†æ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ— (Fold 5ã‹ã‚‰å†é–‹)
            print(f"â© Fold {fold+1}/{N_FOLDS} ã‚¹ã‚­ãƒƒãƒ— (å®Œäº†æ¸ˆã¿)")
            continue

        fold_start = datetime.now()
        print(f"\nğŸ“‚ Fold {fold+1}/{N_FOLDS}...")
        
        X_tr = X_train.iloc[train_idx].copy()
        X_val = X_train.iloc[val_idx].copy()
        y_tr = y_train[train_idx]
        y_val = y_train[val_idx]
        
        # LightGBM
        _, pred_lgb, m_lgb = train_lgb_fold(fold, X_tr, y_tr, X_val, y_val, cat_cols)
        oof_lgb[val_idx] = pred_lgb
        models_lgb.append(m_lgb)
        print(f"   LightGBM AUC: {roc_auc_score(y_val, pred_lgb):.4f}")
        
        # CatBoost
        _, pred_cat, m_cat = train_cat_fold(fold, X_tr, y_tr, X_val, y_val, cat_cols)
        oof_cat[val_idx] = pred_cat
        models_cat.append(m_cat)
        print(f"   CatBoost AUC: {roc_auc_score(y_val, pred_cat):.4f}")
        
        # TabNet
        _, pred_tab, m_tab, tab_pre = train_tabnet_fold(fold, X_tr, y_tr, X_val, y_val, cat_cols, num_cols)
        oof_tab[val_idx] = pred_tab
        models_tab.append(m_tab)
        tab_preprocessors.append(tab_pre)
        print(f"   TabNet AUC:   {roc_auc_score(y_val, pred_tab):.4f}")
        
        # MLP (v2: Target Encoding)
        _, pred_mlp, m_mlp, mlp_pre = train_mlp_fold(fold, X_tr, y_tr, X_val, y_val, num_cols, cat_cols)
        oof_mlp[val_idx] = pred_mlp
        models_mlp.append(m_mlp)
        mlp_preprocessors.append(mlp_pre)
        print(f"   MLP AUC:      {roc_auc_score(y_val, pred_mlp):.4f}")
        
        fold_elapsed = (datetime.now() - fold_start).total_seconds()
        print(f"   â±ï¸ Fold {fold+1} å®Œäº†: {fold_elapsed:.1f}ç§’")
        gc.collect()
    
    # OOFè©•ä¾¡
    print("\n" + "=" * 70)
    print(" ğŸ“Š OOFè©•ä¾¡çµæœ")
    print("=" * 70)
    
    auc_lgb = roc_auc_score(y_train, oof_lgb)
    auc_cat = roc_auc_score(y_train, oof_cat)
    auc_tab = roc_auc_score(y_train, oof_tab)
    auc_mlp = roc_auc_score(y_train, oof_mlp)
    
    print(f"   LightGBM: {auc_lgb:.4f}")
    print(f"   CatBoost: {auc_cat:.4f}")
    print(f"   TabNet:   {auc_tab:.4f}")
    print(f"   MLP:      {auc_mlp:.4f}")
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ– (v2: LogLossä½¿ç”¨)
    print("\nğŸ” ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–ä¸­ (LogLoss)...")
    
    def loss_fn(w):
        """LogLossãƒ™ãƒ¼ã‚¹ã®æå¤±é–¢æ•°ï¼ˆé–¾å€¤éä¾å­˜ï¼‰"""
        w = np.clip(w, 0.01, 0.99)
        w = w / (w.sum() + 1e-8)
        ens = w[0] * oof_lgb + w[1] * oof_cat + w[2] * oof_tab + w[3] * oof_mlp
        ens = np.clip(ens, 1e-7, 1 - 1e-7)  # log(0)é˜²æ­¢
        return log_loss(y_train, ens)
    
    result = minimize(
        loss_fn, [0.25, 0.25, 0.25, 0.25],
        method='SLSQP',
        bounds=[(0.01, 0.99)] * 4,
        constraints={'type': 'eq', 'fun': lambda w: 1 - sum(w)}
    )
    
    best_w = np.clip(result.x, 0, 1)
    best_w = best_w / best_w.sum()
    
    print(f"   æœ€é©é‡ã¿: LGB={best_w[0]:.3f}, Cat={best_w[1]:.3f}, Tab={best_w[2]:.3f}, MLP={best_w[3]:.3f}")
    
    oof_ensemble = best_w[0] * oof_lgb + best_w[1] * oof_cat + best_w[2] * oof_tab + best_w[3] * oof_mlp
    auc_ens = roc_auc_score(y_train, oof_ensemble)
    logloss_ens = log_loss(y_train, np.clip(oof_ensemble, 1e-7, 1-1e-7))
    print(f"   ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« OOF AUC: {auc_ens:.4f}, LogLoss: {logloss_ens:.4f}")
    
    # Testè©•ä¾¡
    print("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡...")
    
    # LightGBM Test
    test_lgb = np.zeros(len(y_test))
    X_test_lgb = X_test.copy()
    for col in cat_cols:
        if col in X_test_lgb.columns:
            X_test_lgb[col] = X_test_lgb[col].astype('category')
    for m in models_lgb:
        test_lgb += m.predict_proba(X_test_lgb)[:, 1] / N_FOLDS
    
    # CatBoost Test
    test_cat = np.zeros(len(y_test))
    for m in models_cat:
        test_cat += m.predict_proba(X_test)[:, 1] / N_FOLDS
    
    # TabNet Test
    test_tab = np.zeros(len(y_test))
    for m, (imp, scl, enc) in zip(models_tab, tab_preprocessors):
        X_num = scl.transform(imp.transform(X_test[num_cols].values))
        X_cat = enc.transform(X_test[cat_cols].values) + 1
        X_t = np.hstack([X_num, X_cat]).astype(np.float32)
        test_tab += m.predict_proba(X_t)[:, 1] / N_FOLDS
    
    # MLP Test (v2: Target Encodingä½¿ç”¨)
    test_mlp = np.zeros(len(y_test))
    for m, (te, imp, scl) in zip(models_mlp, mlp_preprocessors):
        X_test_enc = te.transform(X_test.copy())
        X_test_enc[num_cols] = imp.transform(X_test_enc[num_cols])
        X_t = scl.transform(X_test_enc.values.astype(np.float32))
        m.eval()
        with torch.no_grad():
            test_mlp += torch.sigmoid(m(torch.tensor(X_t, dtype=torch.float32).to(DEVICE))).cpu().numpy().flatten() / N_FOLDS
    
    test_ens = best_w[0] * test_lgb + best_w[1] * test_cat + best_w[2] * test_tab + best_w[3] * test_mlp
    
    test_auc_lgb = roc_auc_score(y_test, test_lgb)
    test_auc_cat = roc_auc_score(y_test, test_cat)
    test_auc_tab = roc_auc_score(y_test, test_tab)
    test_auc_mlp = roc_auc_score(y_test, test_mlp)
    test_auc_ens = roc_auc_score(y_test, test_ens)
    
    print(f"\n   ğŸ“Š Test Set AUC:")
    print(f"   LightGBM: {test_auc_lgb:.4f}")
    print(f"   CatBoost: {test_auc_cat:.4f}")
    print(f"   TabNet:   {test_auc_tab:.4f}")
    print(f"   MLP:      {test_auc_mlp:.4f}")
    print(f"   ğŸ† Ensemble: {test_auc_ens:.4f}")
    
    elapsed = (datetime.now() - start).total_seconds()
    
    # çµæœä¿å­˜
    results = {
        'oof_auc': {
            'lightgbm': auc_lgb, 'catboost': auc_cat, 'tabnet': auc_tab, 'mlp': auc_mlp, 'ensemble': auc_ens
        },
        'test_auc': {
            'lightgbm': test_auc_lgb, 'catboost': test_auc_cat, 'tabnet': test_auc_tab, 'mlp': test_auc_mlp, 'ensemble': test_auc_ens
        },
        'ensemble_weights': {
            'lightgbm': float(best_w[0]), 'catboost': float(best_w[1]), 'tabnet': float(best_w[2]), 'mlp': float(best_w[3])
        },
        'elapsed_seconds': elapsed
    }
    
    with open(OUTPUT_DIR / "results_4model_ensemble.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«åˆ¤å®š
    test_aucs = {'LightGBM': test_auc_lgb, 'CatBoost': test_auc_cat, 'TabNet': test_auc_tab, 'MLP': test_auc_mlp}
    best_single = max(test_aucs, key=test_aucs.get)
    best_single_auc = test_aucs[best_single]
    ens_improvement = (test_auc_ens - best_single_auc) * 100
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = f"""# 4ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ + ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ (v2)

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å®Ÿè¡Œæ™‚é–“**: {elapsed:.1f}ç§’
**ãƒ‡ãƒ¼ã‚¿**: honhyo_for_analysis_with_traffic_hospital_no_leakage.csv

## ä¿®æ­£ç‚¹ (v2)
- **MLP**: Target Encodingä½¿ç”¨ï¼ˆLabel Encodingã®é †åºæ€§å•é¡Œã‚’è§£æ¶ˆï¼‰
- **MLP**: Train/Valåˆ†å‰²å¾Œã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’fitï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: LogLossãƒ™ãƒ¼ã‚¹ã§é‡ã¿æœ€é©åŒ–ï¼ˆé–¾å€¤ä¾å­˜ã‚’æ’é™¤ï¼‰

## OOFè©•ä¾¡ (5-Fold CV)

| ãƒ¢ãƒ‡ãƒ« | ROC-AUC |
|--------|---------|
| LightGBM | {auc_lgb:.4f} |
| CatBoost | {auc_cat:.4f} |
| TabNet | {auc_tab:.4f} |
| MLP | {auc_mlp:.4f} |
| **Ensemble** | **{auc_ens:.4f}** |

## ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡

| ãƒ¢ãƒ‡ãƒ« | ROC-AUC |
|--------|---------|
| LightGBM | {test_auc_lgb:.4f} |
| CatBoost | {test_auc_cat:.4f} |
| TabNet | {test_auc_tab:.4f} |
| MLP | {test_auc_mlp:.4f} |
| **Ensemble** | **{test_auc_ens:.4f}** |

## ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ (LogLossæœ€é©åŒ–)

| ãƒ¢ãƒ‡ãƒ« | é‡ã¿ |
|--------|------|
| LightGBM | {best_w[0]:.3f} |
| CatBoost | {best_w[1]:.3f} |
| TabNet | {best_w[2]:.3f} |
| MLP | {best_w[3]:.3f} |

## è€ƒå¯Ÿ

- **æœ€å„ªç§€å˜ä½“ãƒ¢ãƒ‡ãƒ«**: {best_single} ({best_single_auc:.4f})
- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ**: {'+' if ens_improvement > 0 else ''}{ens_improvement:.2f}% (å˜ä½“æœ€é«˜æ¯”)
"""
    
    with open(OUTPUT_DIR / "experiment_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("\n" + "=" * 70)
    print(" âœ… å®Œäº†ï¼")
    print(f"   ç·å®Ÿè¡Œæ™‚é–“: {elapsed:.1f}ç§’")
    print(f"   çµæœä¿å­˜å…ˆ: {OUTPUT_DIR}")
    print("=" * 70)
    
    return results


if __name__ == "__main__":
    main()

"""
å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Phase 1: Integrity Check)
==================================================
1. LightGBMã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦ã®ç¢ºèª
2. Top-k äºˆæ¸¬ã®ç©ºé–“çš„å¤šæ§˜æ€§ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯Geohashæ•°ï¼‰ã®ç¢ºèª
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import pyarrow.parquet as pq
import matplotlib.pyplot as plt
import os
import json
from pathlib import Path

# --- ãƒ‘ã‚¹è¨­å®š ---
DATA_DIR = Path("data/spatio_temporal")
RESULTS_DIR = Path("results/spatio_temporal")
OUTPUT_DIR = RESULTS_DIR / "integrity_check"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'MS Gothic'


def load_data():
    """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train_df = pd.read_parquet(DATA_DIR / "preprocessed_train.parquet")
    test_df = pd.read_parquet(DATA_DIR / "preprocessed_test.parquet")
    print(f"   Train: {len(train_df):,} rows, Test: {len(test_df):,} rows")
    return train_df, test_df


def analyze_feature_importance(train_df: pd.DataFrame, test_df: pd.DataFrame):
    """LightGBMã§ç‰¹å¾´é‡é‡è¦åº¦ã‚’ç®—å‡º"""
    print("\nğŸ” [1/2] ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æä¸­...")
    
    # ç‰¹å¾´é‡ã®ç‰¹å®š
    exclude_cols = ['fatal', 'geohash', 'geohash_fine', 'date', 'year', 'node_id']
    feature_cols = [c for c in train_df.columns if c not in exclude_cols and train_df[c].dtype in ['int64', 'float64', 'float32', 'int32']]
    
    X_train = train_df[feature_cols].values
    y_train = train_df['fatal'].values
    
    # LightGBMå­¦ç¿’
    dtrain = lgb.Dataset(X_train, label=y_train)
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42,
    }
    model = lgb.train(
        params, dtrain,
        num_boost_round=200,
        valid_sets=[dtrain],
        callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(0)]
    )
    
    # é‡è¦åº¦å–å¾—
    importance = model.feature_importance(importance_type='gain')
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    # ä¸Šä½20ä»¶ã®è¡¨ç¤º
    print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ Top 20:")
    print("-" * 60)
    for i, row in importance_df.head(20).iterrows():
        pct = row['importance'] / importance_df['importance'].sum() * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"   {row['feature']:40s} {pct:5.1f}% {bar}")
    
    # ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®æ‡¸å¿µãƒã‚§ãƒƒã‚¯
    leakage_keywords = ['past_30d', 'past_365d', 'fatal', 'te_', 'target_enc']
    top5_features = importance_df.head(5)['feature'].tolist()
    leakage_suspects = [f for f in top5_features if any(kw in f.lower() for kw in leakage_keywords)]
    
    if leakage_suspects:
        print("\nâš ï¸ ã€è­¦å‘Šã€‘ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®ç–‘ã„ãŒã‚ã‚‹ç‰¹å¾´é‡ãŒä¸Šä½ã«å­˜åœ¨ã—ã¾ã™:")
        for f in leakage_suspects:
            print(f"   - {f}")
    else:
        print("\nâœ… ä¸Šä½5ç‰¹å¾´é‡ã«æ˜ã‚‰ã‹ãªãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã¯è¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€‚")
    
    # ä¿å­˜
    importance_df.to_csv(OUTPUT_DIR / "feature_importance.csv", index=False)
    
    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(12, 8))
    top20 = importance_df.head(20)
    plt.barh(top20['feature'][::-1], top20['importance'][::-1])
    plt.xlabel('Importance (Gain)')
    plt.title('Top 20 Feature Importance (LightGBM)')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "feature_importance.png", dpi=150)
    plt.close()
    print(f"   ä¿å­˜: {OUTPUT_DIR / 'feature_importance.png'}")
    
    return importance_df, leakage_suspects


def analyze_spatial_diversity(test_df: pd.DataFrame):
    """Top-käºˆæ¸¬ã®ç©ºé–“çš„å¤šæ§˜æ€§ã‚’ç¢ºèª"""
    print("\nğŸŒ [2/2] ç©ºé–“çš„å¤šæ§˜æ€§ã‚’åˆ†æä¸­...")
    
    # äºˆæ¸¬çµæœã®èª­ã¿è¾¼ã¿
    pred_path = RESULTS_DIR / "test_predictions.parquet"
    if not pred_path.exists():
        print(f"   âŒ äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pred_path}")
        return None
    
    pred_df = pd.read_parquet(pred_path)
    print(f"   äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {len(pred_df):,} rows")
    
    # ä¸Šä½kä»¶ã®åˆ†æ
    k_values = [100, 500, 1000]
    results = {}
    
    # geohashã‚«ãƒ©ãƒ ã®ç¢ºèª
    geohash_col = None
    for col in ['geohash', 'geohash_fine']:
        if col in pred_df.columns:
            geohash_col = col
            break
    
    if geohash_col is None:
        # test_dfã‹ã‚‰geohashã‚’å–å¾—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸€è‡´ã™ã‚‹ã¨ä»®å®šï¼‰
        if 'geohash' in test_df.columns:
            pred_df['geohash'] = test_df['geohash'].values[:len(pred_df)]
            geohash_col = 'geohash'
        else:
            print("   âŒ geohashã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None
    
    # probã¾ãŸã¯predictionã‚«ãƒ©ãƒ ã‚’ä½¿ã†
    prob_col = 'prediction' if 'prediction' in pred_df.columns else 'prob'
    pred_df_sorted = pred_df.sort_values(prob_col, ascending=False)
    
    for k in k_values:
        top_k = pred_df_sorted.head(k)
        unique_geohash = top_k[geohash_col].nunique()
        fatal_count = top_k['label'].sum() if 'label' in top_k.columns else top_k['fatal'].sum()
        precision = fatal_count / k
        
        results[k] = {
            'total': k,
            'unique_geohash': unique_geohash,
            'diversity_ratio': unique_geohash / k,
            'fatal_count': int(fatal_count),
            'precision': precision
        }
        
        print(f"\n   Top-{k}:")
        print(f"      - ãƒ¦ãƒ‹ãƒ¼ã‚¯Geohashæ•°: {unique_geohash} / {k} ({unique_geohash/k*100:.1f}%)")
        print(f"      - æ­£è§£ï¼ˆfatal=1ï¼‰: {int(fatal_count)} ä»¶ (Precision: {precision:.1%})")
    
    # é›†ä¸­åº¦ã®è­¦å‘Š
    top100_diversity = results[100]['diversity_ratio']
    if top100_diversity < 0.3:
        print("\nâš ï¸ ã€è­¦å‘Šã€‘Top-100ã®ç©ºé–“çš„å¤šæ§˜æ€§ãŒéå¸¸ã«ä½ã„ã§ã™ï¼ˆ< 30%ï¼‰")
        print("      ç‰¹å®šã®åœ°ç‚¹ï¼ˆGeohashï¼‰ã«äºˆæ¸¬ãŒé›†ä¸­ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    elif top100_diversity < 0.5:
        print("\nâš ï¸ ã€æ³¨æ„ã€‘Top-100ã®ç©ºé–“çš„å¤šæ§˜æ€§ãŒã‚„ã‚„ä½ã„ã§ã™ï¼ˆ< 50%ï¼‰")
    else:
        print("\nâœ… Top-100ã®ç©ºé–“çš„å¤šæ§˜æ€§ã¯é©åˆ‡ãªãƒ¬ãƒ™ãƒ«ã§ã™ã€‚")
    
    # çµæœä¿å­˜
    with open(OUTPUT_DIR / "spatial_diversity.json", 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\n   ä¿å­˜: {OUTPUT_DIR / 'spatial_diversity.json'}")
    
    return results


def main():
    print("=" * 70)
    print(" ğŸ” Spatio-Temporal Model å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ (Phase 1)")
    print("=" * 70)
    
    train_df, test_df = load_data()
    
    # 1. ç‰¹å¾´é‡é‡è¦åº¦
    importance_df, leakage_suspects = analyze_feature_importance(train_df, test_df)
    
    # 2. ç©ºé–“çš„å¤šæ§˜æ€§
    diversity_results = analyze_spatial_diversity(test_df)
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print(" ğŸ“‹ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ ã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    
    if leakage_suspects:
        print(f"   âš ï¸ ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ç–‘æƒ‘: {leakage_suspects}")
    else:
        print("   âœ… ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ç–‘æƒ‘: ãªã—")
    
    if diversity_results:
        dr = diversity_results[100]['diversity_ratio']
        print(f"   ğŸ“ Top-100 å¤šæ§˜æ€§: {dr:.1%} (ãƒ¦ãƒ‹ãƒ¼ã‚¯Geohashæ¯”ç‡)")
        if dr >= 0.5:
            print("   âœ… ç©ºé–“çš„å¤šæ§˜æ€§: è‰¯å¥½")
        else:
            print("   âš ï¸ ç©ºé–“çš„å¤šæ§˜æ€§: è¦ç¢ºèª")
    
    print("\nğŸ‰ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†ï¼")
    print(f"   çµæœã¯ {OUTPUT_DIR} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()

"""
Spatio-Temporal Stage2 å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
======================================
å…¨ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»æ¯”è¼ƒ
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ä»˜ã
"""

import os
import sys
import argparse
import json
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import warnings
import gc

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter

warnings.filterwarnings('ignore')

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from utils.checkpoint import CheckpointManager, EarlyStopping, set_seed
from evaluate import evaluate_model, ModelEvaluator
from visualize import Visualizer

# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
RANDOM_SEED = 42


class SpatioTemporalTrainer:
    """
    Spatio-Temporal ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¯ãƒ©ã‚¹
    
    - è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¯”è¼ƒ
    - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†
    - TensorBoardãƒ­ã‚°
    """
    
    def __init__(
        self,
        data_dir: str = "data/spatio_temporal",
        output_dir: str = "results/spatio_temporal",
        model_type: str = "knn_gnn",  # 'lstm', 'tgcn', 'gat', 'knn_gnn'
        config: Optional[Dict] = None,
        device: str = "auto",
    ):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.model_type = model_type
        self.config = config or self._default_config()
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"   ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        self.ckpt_manager = CheckpointManager(
            self.output_dir / "checkpoints" / model_type
        )
        
        # TensorBoardãƒ©ã‚¤ã‚¿ãƒ¼
        self.writer = SummaryWriter(self.output_dir / "logs" / model_type)
        
        # ã‚·ãƒ¼ãƒ‰å›ºå®š
        set_seed(RANDOM_SEED)
    
    def _default_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            'hidden_dim': 128,
            'num_layers': 2,
            'dropout': 0.3,
            'learning_rate': 0.001,
            'batch_size': 1024,
            'epochs': 100,
            'patience': 15,
            'focal_alpha': 0.75,
            'focal_gamma': 2.0,
            'k_neighbors': 8,
        }
    
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        train_df = pd.read_parquet(self.data_dir / "preprocessed_train.parquet")
        val_df = pd.read_parquet(self.data_dir / "preprocessed_val.parquet")
        test_df = pd.read_parquet(self.data_dir / "preprocessed_test.parquet")
        
        print(f"   Train: {len(train_df):,} è¡Œ")
        print(f"   Val:   {len(val_df):,} è¡Œ")
        print(f"   Test:  {len(test_df):,} è¡Œ")
        
        return train_df, val_df, test_df
    
    def prepare_features(
        self,
        df: pd.DataFrame,
        target_col: str = 'fatal',
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™"""
        
        # é™¤å¤–åˆ—
        exclude_cols = [target_col, 'date', 'lat', 'lon', 'geohash', 'geohash_fine', 'year']
        
        feature_cols = [c for c in df.columns if c not in exclude_cols]
        
        X = df[feature_cols].values.astype(np.float32)
        y = df[target_col].values.astype(np.float32)
        
        # NaNå‡¦ç†
        X = np.nan_to_num(X, nan=0.0)
        
        return X, y
    
    def create_model(self, input_dim: int):
        """ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        
        if self.model_type == 'knn_gnn':
            from models.knn_gnn import KNNGraphGNN
            model = KNNGraphGNN(
                input_dim=input_dim,
                hidden_dim=self.config['hidden_dim'],
                num_layers=self.config['num_layers'],
                dropout=self.config['dropout'],
            )
        elif self.model_type == 'mlp':
            # ã‚·ãƒ³ãƒ—ãƒ«ãªMLPãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            model = nn.Sequential(
                nn.Linear(input_dim, self.config['hidden_dim']),
                nn.ReLU(),
                nn.BatchNorm1d(self.config['hidden_dim']),
                nn.Dropout(self.config['dropout']),
                nn.Linear(self.config['hidden_dim'], self.config['hidden_dim'] // 2),
                nn.ReLU(),
                nn.BatchNorm1d(self.config['hidden_dim'] // 2),
                nn.Dropout(self.config['dropout']),
                nn.Linear(self.config['hidden_dim'] // 2, 1),
            )
        elif self.model_type == 'lstm':
            from models.lstm_geohash import GeoHashLSTM
            # LSTMã®å ´åˆã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªMLPã‚’ä½¿ç”¨
            model = nn.Sequential(
                nn.Linear(input_dim, self.config['hidden_dim']),
                nn.ReLU(),
                nn.Dropout(self.config['dropout']),
                nn.Linear(self.config['hidden_dim'], self.config['hidden_dim'] // 2),
                nn.ReLU(),
                nn.Dropout(self.config['dropout']),
                nn.Linear(self.config['hidden_dim'] // 2, 1),
            )
        elif self.model_type == 'tgcn':
            from models.temporal_gnn import SimpleTGCN
            model = SimpleTGCN(
                input_dim=input_dim,
                hidden_dim=self.config['hidden_dim'],
                num_layers=self.config['num_layers'],
                dropout=self.config['dropout'],
            )
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
        
        return model.to(self.device)
    
    def train_simple_model(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
    ) -> Tuple[nn.Module, Dict]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªNN/MLPãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        ï¼ˆGNNã‚’ä½¿ã‚ãªã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
        """
        print(f"\nğŸŒ¿ {self.model_type} ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_dataset = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
        )
        val_dataset = TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=0,
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=0,
        )
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = self.create_model(X_train.shape[1])
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        optimizer = optim.Adam(model.parameters(), lr=self.config['learning_rate'])
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )
        
        # æå¤±é–¢æ•°ï¼ˆFocal Lossï¼‰
        from models.knn_gnn import FocalLoss
        criterion = FocalLoss(
            alpha=self.config['focal_alpha'],
            gamma=self.config['focal_gamma']
        )
        
        # Early Stopping
        early_stopping = EarlyStopping(patience=self.config['patience'], mode='max')
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å†é–‹
        resume_info = self.ckpt_manager.get_resume_info()
        start_epoch = 0
        
        if resume_info['should_resume']:
            checkpoint = self.ckpt_manager.load_checkpoint(model, optimizer, scheduler)
            start_epoch = checkpoint['epoch'] + 1
            print(f"   å†é–‹: epoch {start_epoch} ã‹ã‚‰")
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        best_val_auc = 0.0
        
        for epoch in range(start_epoch, self.config['epochs']):
            # å­¦ç¿’
            model.train()
            train_loss = 0.0
            
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # æ¤œè¨¼
            model.eval()
            val_preds = []
            val_targets = []
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x = batch_x.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    
                    probs = torch.sigmoid(outputs)
                    val_preds.extend(probs.cpu().numpy().flatten())
                    val_targets.extend(batch_y.cpu().numpy().flatten())
            
            val_loss /= len(val_loader)
            val_preds = np.array(val_preds)
            val_targets = np.array(val_targets)
            
            # è©•ä¾¡æŒ‡æ¨™
            from sklearn.metrics import roc_auc_score, average_precision_score
            val_auc = roc_auc_score(val_targets, val_preds)
            val_pr_auc = average_precision_score(val_targets, val_preds)
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°
            scheduler.step(val_loss)
            
            # ãƒ­ã‚°
            self.writer.add_scalar('Loss/train', train_loss, epoch)
            self.writer.add_scalar('Loss/val', val_loss, epoch)
            self.writer.add_scalar('AUC/val', val_auc, epoch)
            self.writer.add_scalar('PR-AUC/val', val_pr_auc, epoch)
            
            # æ”¹å–„ãƒã‚§ãƒƒã‚¯
            is_best = early_stopping(val_auc)
            
            if is_best:
                best_val_auc = val_auc
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if epoch % 5 == 0 or is_best:
                self.ckpt_manager.save_checkpoint(
                    model, optimizer, epoch, 0,
                    {'val_auc': val_auc, 'val_pr_auc': val_pr_auc, 'val_loss': val_loss},
                    self.config, scheduler, is_best
                )
            
            # é€²æ—è¡¨ç¤º
            if epoch % 5 == 0:
                print(f"   Epoch {epoch:3d}: Loss={train_loss:.4f}/{val_loss:.4f}, "
                      f"AUC={val_auc:.4f}, PR-AUC={val_pr_auc:.4f}")
            
            # Early Stopping
            if early_stopping.should_stop:
                print(f"   â¹ï¸ Early Stopping at epoch {epoch}")
                break
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        self.ckpt_manager.load_best_model(model)
        
        return model, {'best_val_auc': best_val_auc}
    
    def train_gnn_model(
        self,
        train_df: pd.DataFrame,
        val_df: pd.DataFrame,
        edge_index: torch.Tensor,
    ) -> Tuple[nn.Module, Dict]:
        """
        GNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆInductive: å…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚°ãƒ©ãƒ• + ãƒãƒ¼ãƒ‰ãƒã‚¹ã‚¯æ–¹å¼ï¼‰
        """
        print(f"\nğŸŒ¿ GNN ({self.model_type}) Inductiveå­¦ç¿’ä¸­...")
        
        # ç‰¹å¾´é‡æº–å‚™ (train + val ã‚’çµåˆã—ãŸ combined_df ã‚’ä½¿ã†)
        # ã“ã“ã§ã¯ combined_df ã¨ã—ã¦æ¸¡ã•ã‚ŒãŸ train_df ã‚’ä½¿ç”¨
        X_all, y_all = self.prepare_features(train_df)  # train_df ã¯å®Ÿéš›ã«ã¯ combined_df
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        if self.model_type == 'knn_gnn':
            from models.knn_gnn import KNNGraphGNN, FocalLoss
            model = KNNGraphGNN(
                input_dim=X_all.shape[1],
                hidden_dim=self.config['hidden_dim'],
                num_layers=self.config['num_layers'],
                dropout=self.config['dropout'],
            ).to(self.device)
        elif self.model_type == 'tgcn':
            from models.temporal_gnn import SimpleTGCN
            model = SimpleTGCN(
                input_dim=X_all.shape[1],
                hidden_dim=self.config['hidden_dim'],
                num_layers=self.config['num_layers'],
                dropout=self.config['dropout'],
            ).to(self.device)
        else:
            raise ValueError(f"Unknown GNN model type: {self.model_type}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
        X_all_t = torch.tensor(X_all, dtype=torch.float32).to(self.device)
        y_all_t = torch.tensor(y_all, dtype=torch.float32).unsqueeze(1).to(self.device)
        edge_index = edge_index.to(self.device)
        
        # ãƒã‚¹ã‚¯ã‚‚GPUã«è»¢é€ï¼ˆval_dfã«ãƒã‚¹ã‚¯ãŒå«ã¾ã‚Œã¦ã„ã‚‹æƒ³å®šï¼‰
        train_mask = val_df['train_mask'].to(self.device)
        val_mask = val_df['val_mask'].to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        optimizer = optim.Adam(model.parameters(), lr=self.config['learning_rate'])
        
        # æå¤±é–¢æ•°
        from models.knn_gnn import FocalLoss
        criterion = FocalLoss(
            alpha=self.config['focal_alpha'],
            gamma=self.config['focal_gamma']
        )
        
        # Early Stopping
        early_stopping = EarlyStopping(patience=self.config['patience'], mode='max')
        
        best_val_auc = 0.0
        
        for epoch in range(self.config['epochs']):
            # å­¦ç¿’
            model.train()
            optimizer.zero_grad()
            
            # å…¨ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦ forward
            outputs = model(X_all_t, edge_index)
            
            # Train ãƒã‚¹ã‚¯ã®ãƒãƒ¼ãƒ‰ã®ã¿ã§æå¤±è¨ˆç®—
            train_outputs = outputs[train_mask]
            train_targets = y_all_t[train_mask]
            loss = criterion(train_outputs, train_targets)
            
            loss.backward()
            optimizer.step()
            
            # æ¤œè¨¼ (Val ãƒã‚¹ã‚¯ã®ãƒãƒ¼ãƒ‰ã§è©•ä¾¡)
            model.eval()
            with torch.no_grad():
                val_outputs = outputs[val_mask]
                val_preds = torch.sigmoid(val_outputs).cpu().numpy().flatten()
                val_targets_np = y_all_t[val_mask].cpu().numpy().flatten()
            
            from sklearn.metrics import roc_auc_score, average_precision_score
            val_auc = roc_auc_score(val_targets_np, val_preds)
            val_pr_auc = average_precision_score(val_targets_np, val_preds)
            
            # ãƒ­ã‚°
            self.writer.add_scalar('Loss/train', loss.item(), epoch)
            self.writer.add_scalar('AUC/val', val_auc, epoch)
            self.writer.add_scalar('PR-AUC/val', val_pr_auc, epoch)
            
            is_best = early_stopping(val_auc)
            if is_best:
                best_val_auc = val_auc
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if epoch % 10 == 0 or is_best:
                self.ckpt_manager.save_checkpoint(
                    model, optimizer, epoch, 0,
                    {'val_auc': val_auc, 'val_pr_auc': val_pr_auc},
                    self.config, is_best=is_best
                )
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch:3d}: Loss={loss.item():.4f}, "
                      f"AUC={val_auc:.4f}, PR-AUC={val_pr_auc:.4f}")
            
            if early_stopping.should_stop:
                print(f"   â¹ï¸ Early Stopping at epoch {epoch}")
                break
        
        self.ckpt_manager.load_best_model(model)
        
        return model, {'best_val_auc': best_val_auc}
    
    def evaluate_on_test(
        self,
        model: nn.Module,
        test_df: pd.DataFrame,
        edge_index: Optional[torch.Tensor] = None,
    ) -> Tuple[np.ndarray, Dict]:
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡"""
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ä¸­...")
        
        X_test, y_test = self.prepare_features(test_df)
        
        model.eval()
        
        if edge_index is not None and self.model_type in ['knn_gnn', 'tgcn']:
            # GNNãƒ¢ãƒ‡ãƒ«
            X_test_t = torch.tensor(X_test, dtype=torch.float32).to(self.device)
            edge_index = edge_index.to(self.device)
            
            with torch.no_grad():
                outputs = model(X_test_t, edge_index)
                predictions = torch.sigmoid(outputs).cpu().numpy().flatten()
        else:
            # é€šå¸¸ã®NNãƒ¢ãƒ‡ãƒ«
            X_test_t = torch.tensor(X_test, dtype=torch.float32).to(self.device)
            
            with torch.no_grad():
                outputs = model(X_test_t)
                predictions = torch.sigmoid(outputs).cpu().numpy().flatten()
        
        # è©•ä¾¡
        metrics = evaluate_model(y_test, predictions)
        
        print(f"   PR-AUC: {metrics['pr_auc']:.4f}")
        print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"   ECE: {metrics['ece']:.4f}")
        
        return predictions, metrics
    
    def run(self):
        """å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        train_df, val_df, test_df = self.load_data()
        
        # ç‰¹å¾´é‡æº–å‚™
        X_train, y_train = self.prepare_features(train_df)
        X_val, y_val = self.prepare_features(val_df)
        
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {X_train.shape[1]}")
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        if self.model_type in ['lstm', 'mlp']:
            model, train_info = self.train_simple_model(X_train, y_train, X_val, y_val)
            edge_index = None
            graph_data = None
        else:
            # Inductive ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼‰
            from graph_builder import build_inductive_graph
            
            graph_data = build_inductive_graph(
                train_df, 
                val_df,
                test_df,
                k=self.config['k_neighbors'],
                output_dir=self.data_dir
            )
            edge_index = graph_data['edge_index']
            
            # ãƒã‚¹ã‚¯æƒ…å ±ã‚’è¾æ›¸ã¨ã—ã¦æ¸¡ã™
            mask_info = {
                'train_mask': graph_data['train_mask'],
                'val_mask': graph_data['val_mask'],
                'test_mask': graph_data['test_mask'],
            }
            
            # çµåˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
            combined_df = graph_data['combined_df']
            
            model, train_info = self.train_gnn_model(combined_df, mask_info, edge_index)
        
        # ãƒ†ã‚¹ãƒˆè©•ä¾¡
        if graph_data is not None:
            # GNN ã®å ´åˆã¯ combined_df ã¨ test_mask ã§è©•ä¾¡
            predictions, test_metrics = self.evaluate_on_test_gnn(
                model, 
                graph_data['combined_df'], 
                edge_index,
                graph_data['test_mask']
            )
            # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®ã¿æŠ½å‡º
            test_indices = graph_data['test_mask'].numpy()
            test_df_with_pred = test_df.copy()
            test_df_with_pred['prediction'] = predictions
        else:
            predictions, test_metrics = self.evaluate_on_test(model, test_df, edge_index)
            test_df_with_pred = test_df.copy()
            test_df_with_pred['prediction'] = predictions
        
        # çµæœä¿å­˜
        test_df_with_pred.to_parquet(self.output_dir / "test_predictions.parquet")
        
        # çµæœã‚µãƒãƒª
        results = {
            'model_type': self.model_type,
            'config': self.config,
            'train_info': train_info,
            'test_metrics': test_metrics,
            'elapsed_seconds': (datetime.now() - start_time).total_seconds(),
        }
        
        with open(self.output_dir / f"results_{self.model_type}.json", 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # TensorBoardã‚¯ãƒ­ãƒ¼ã‚º
        self.writer.close()
        
        print("\n" + "=" * 70)
        print(f"âœ… {self.model_type} å­¦ç¿’å®Œäº†ï¼")
        print(f"   PR-AUC: {test_metrics['pr_auc']:.4f}")
        print(f"   æ‰€è¦æ™‚é–“: {results['elapsed_seconds']:.1f}ç§’")
        print("=" * 70)
        
        return results
    
    def evaluate_on_test_gnn(
        self,
        model: nn.Module,
        combined_df: pd.DataFrame,
        edge_index: torch.Tensor,
        test_mask: torch.Tensor,
    ) -> Tuple[np.ndarray, Dict]:
        """GNNãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ï¼ˆãƒã‚¹ã‚¯æ–¹å¼ï¼‰"""
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ä¸­ï¼ˆInductive GNNï¼‰...")
        
        X_all, y_all = self.prepare_features(combined_df)
        
        model.eval()
        
        X_all_t = torch.tensor(X_all, dtype=torch.float32).to(self.device)
        edge_index = edge_index.to(self.device)
        test_mask = test_mask.to(self.device)
        
        with torch.no_grad():
            outputs = model(X_all_t, edge_index)
            test_outputs = outputs[test_mask]
            predictions = torch.sigmoid(test_outputs).cpu().numpy().flatten()
        
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        y_test = y_all[test_mask.cpu().numpy()]
        
        # è©•ä¾¡
        metrics = evaluate_model(y_test, predictions)
        
        print(f"   PR-AUC: {metrics['pr_auc']:.4f}")
        print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"   ECE: {metrics['ece']:.4f}")
        
        return predictions, metrics



def main():
    parser = argparse.ArgumentParser(description="Spatio-Temporal Model Training")
    parser.add_argument('--data-dir', type=str, default="data/spatio_temporal")
    parser.add_argument('--output-dir', type=str, default="results/spatio_temporal")
    parser.add_argument('--model', type=str, default="knn_gnn",
                        choices=['lstm', 'tgcn', 'gat', 'knn_gnn', 'mlp', 'all'])
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=1024)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--hidden-dim', type=int, default=128)
    parser.add_argument('--k', type=int, default=8, help="k for kNN graph")
    parser.add_argument('--debug', action='store_true', help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆå°‘ãªã„ã‚¨ãƒãƒƒã‚¯ï¼‰")
    
    args = parser.parse_args()
    
    config = {
        'hidden_dim': args.hidden_dim,
        'num_layers': 2,
        'dropout': 0.3,
        'learning_rate': args.lr,
        'batch_size': args.batch_size,
        'epochs': 2 if args.debug else args.epochs,
        'patience': 15,
        'focal_alpha': 0.75,
        'focal_gamma': 2.0,
        'k_neighbors': args.k,
    }
    
    if args.model == 'all':
        models = ['mlp', 'knn_gnn']
        all_results = {}
        
        for model_type in models:
            print(f"\n{'='*70}")
            print(f"ãƒ¢ãƒ‡ãƒ«: {model_type}")
            print(f"{'='*70}")
            
            trainer = SpatioTemporalTrainer(
                data_dir=args.data_dir,
                output_dir=args.output_dir,
                model_type=model_type,
                config=config,
            )
            
            results = trainer.run()
            all_results[model_type] = results
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
        for model_type, results in all_results.items():
            print(f"   {model_type}: PR-AUC={results['test_metrics']['pr_auc']:.4f}")
    else:
        trainer = SpatioTemporalTrainer(
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            model_type=args.model,
            config=config,
        )
        
        trainer.run()


if __name__ == "__main__":
    main()

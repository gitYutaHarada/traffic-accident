"""
Spatio-Temporal Stage2 å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
======================================
- ã‚¸ã‚ªãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
- éå»ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®äº‹æ•…ä»¶æ•°é›†è¨ˆï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
- æ™‚ç³»åˆ—ç‰¹å¾´é‡ç”Ÿæˆ
- ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- æ™‚é–“ãƒ™ãƒ¼ã‚¹åˆ†å‰²
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Tuple, List, Dict, Optional
import warnings
import joblib
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import KFold
import geohash2 as geohash

warnings.filterwarnings('ignore')

# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰å›ºå®š
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)


class SpatioTemporalPreprocessor:
    """ç©ºé–“ãƒ»æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        data_path: str = "data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv",
        output_dir: str = "data/spatio_temporal",
        target_col: str = "fatal",
        train_years: Tuple[int, int] = (2018, 2019),
        val_years: Tuple[int, int] = (2020, 2020),
        test_years: Tuple[int, int] = (2021, 2024),
        geohash_precision: int = 6,
        past_windows: List[int] = [30, 365],
        high_cardinality_threshold: int = 20,
    ):
        self.data_path = data_path
        self.output_dir = Path(output_dir)
        self.target_col = target_col
        self.train_years = train_years
        self.val_years = val_years
        self.test_years = test_years
        self.geohash_precision = geohash_precision
        self.past_windows = past_windows
        self.high_cardinality_threshold = high_cardinality_threshold
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€é¡
        self.scaler = StandardScaler()
        self.ohe = None
        self.target_encoders = {}
        
        print("=" * 70)
        print("Spatio-Temporal å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
        print(f"Train: {train_years[0]}-{train_years[1]}")
        print(f"Val:   {val_years[0]}-{val_years[1]}")
        print(f"Test:  {test_years[0]}-{test_years[1]}")
        print("=" * 70)
    
    def load_data(self) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        df = pd.read_csv(self.data_path)
        print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(df):,} è¡Œ, {len(df.columns)} åˆ—")
        return df
    
    def convert_lat_lon(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç·¯åº¦çµŒåº¦ã‚’åº¦æ•°æ³•ã«å¤‰æ›ï¼ˆãƒ™ã‚¯ãƒˆãƒ«å‡¦ç†ã€ãƒ­ãƒã‚¹ãƒˆç‰ˆï¼‰"""
        print("\nğŸŒ ç·¯åº¦çµŒåº¦å¤‰æ›ä¸­...")
        
        lat_col = 'åœ°ç‚¹ã€€ç·¯åº¦ï¼ˆåŒ—ç·¯ï¼‰'
        lon_col = 'åœ°ç‚¹ã€€çµŒåº¦ï¼ˆæ±çµŒï¼‰'
        
        def convert_coord_vectorized(series):
            """ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸåº§æ¨™å¤‰æ›ï¼ˆæ··åœ¨ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
            result = pd.Series(index=series.index, dtype=float)
            
            # æ¬ æå€¤ã‚’é™¤å¤–
            valid_mask = series.notna()
            valid_vals = series[valid_mask].astype(float)
            
            # æ•´æ•°å½¢å¼ï¼ˆ>1000000ï¼‰ã¨åº¦æ•°æ³•ï¼ˆ<1000ï¼‰ã‚’åˆ¤åˆ¥
            is_integer_format = valid_vals > 1000000
            
            # æ•´æ•°å½¢å¼ã®å¤‰æ› (dddmmssss)
            int_vals = valid_vals[is_integer_format].astype(int)
            deg = int_vals // 10000000
            remainder = int_vals % 10000000
            minutes = remainder // 100000
            seconds = (remainder % 100000) / 1000
            result.loc[valid_vals[is_integer_format].index] = deg + minutes / 60 + seconds / 3600
            
            # æ—¢ã«åº¦æ•°æ³•ã®ã‚‚ã®
            result.loc[valid_vals[~is_integer_format].index] = valid_vals[~is_integer_format]
            
            return result
        
        df['lat'] = convert_coord_vectorized(df[lat_col])
        df['lon'] = convert_coord_vectorized(df[lon_col])
        
        print(f"   ç·¯åº¦ç¯„å›²: {df['lat'].min():.4f} - {df['lat'].max():.4f}")
        print(f"   çµŒåº¦ç¯„å›²: {df['lon'].min():.4f} - {df['lon'].max():.4f}")
        
        return df
    
    def filter_invalid_coords(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç„¡åŠ¹ãªåº§æ¨™ã‚’é™¤å»ï¼ˆæ—¥æœ¬é ˜åŸŸå¤–ï¼‰"""
        print("\nğŸ” åº§æ¨™å¤–ã‚Œå€¤é™¤å»ä¸­...")
        
        original_len = len(df)
        
        # æ—¥æœ¬ã®ç·¯åº¦çµŒåº¦ç¯„å›²
        lat_min, lat_max = 24.0, 46.0
        lon_min, lon_max = 122.0, 146.0
        
        df = df[
            (df['lat'] >= lat_min) & (df['lat'] <= lat_max) &
            (df['lon'] >= lon_min) & (df['lon'] <= lon_max) &
            (df['lat'].notna()) & (df['lon'].notna())
        ].copy()
        
        removed = original_len - len(df)
        print(f"   é™¤å»: {removed:,} è¡Œ ({removed/original_len*100:.2f}%)")
        
        return df
    
    def generate_geohash(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¸ã‚ªãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        print(f"\nğŸ“ ã‚¸ã‚ªãƒãƒƒã‚·ãƒ¥ç”Ÿæˆä¸­ (precision={self.geohash_precision})...")
        
        def get_geohash(row):
            try:
                return geohash.encode(row['lat'], row['lon'], precision=self.geohash_precision)
            except:
                return None
        
        df['geohash'] = df.apply(get_geohash, axis=1)
        
        # é«˜ç²¾åº¦ç‰ˆã‚‚ç”Ÿæˆ
        def get_geohash_fine(row):
            try:
                return geohash.encode(row['lat'], row['lon'], precision=7)
            except:
                return None
        
        df['geohash_fine'] = df.apply(get_geohash_fine, axis=1)
        
        n_unique = df['geohash'].nunique()
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¸ã‚ªãƒãƒƒã‚·ãƒ¥æ•°: {n_unique:,}")
        
        return df
    
    def create_date_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ—¥ä»˜åˆ—ã®ä½œæˆ"""
        print("\nğŸ“… æ—¥ä»˜åˆ—ä½œæˆä¸­...")
        
        df['date'] = pd.to_datetime(
            df['year'].astype(str) + '-' + 
            df['month'].astype(str).str.zfill(2) + '-' + 
            df['day'].astype(str).str.zfill(2),
            errors='coerce'
        )
        
        # ç„¡åŠ¹ãªæ—¥ä»˜ã‚’é™¤å»
        df = df[df['date'].notna()].copy()
        df = df.sort_values('date').reset_index(drop=True)
        
        print(f"   æ—¥ä»˜ç¯„å›²: {df['date'].min()} - {df['date'].max()}")
        
        return df
    
    def generate_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ç”Ÿæˆ"""
        print("\nâ° æ™‚ç³»åˆ—ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        
        # hour ã‚’ sin/cos ã«å¤‰æ›
        if 'hour' in df.columns:
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        
        # æ›œæ—¥ã‚’ sin/cos ã«å¤‰æ›
        if 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)' in df.columns:
            # æ›œæ—¥ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆã‚«ãƒ†ã‚´ãƒªã®å ´åˆï¼‰
            weekday_map = {'æ—¥': 0, 'æœˆ': 1, 'ç«': 2, 'æ°´': 3, 'æœ¨': 4, 'é‡‘': 5, 'åœŸ': 6,
                           'æ—¥æ›œ': 0, 'æœˆæ›œ': 1, 'ç«æ›œ': 2, 'æ°´æ›œ': 3, 'æœ¨æ›œ': 4, 'é‡‘æ›œ': 5, 'åœŸæ›œ': 6,
                           0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}
            df['weekday_num'] = df['æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)'].map(weekday_map).fillna(0).astype(int)
            df['weekday_sin'] = np.sin(2 * np.pi * df['weekday_num'] / 7)
            df['weekday_cos'] = np.cos(2 * np.pi * df['weekday_num'] / 7)
        
        # month ã‚’ sin/cos ã«å¤‰æ›
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # ç¥æ—¥ãƒ»æ˜¼å¤œã®ãƒã‚¤ãƒŠãƒªç‰¹å¾´é‡
        if 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)' in df.columns:
            df['is_holiday'] = (df['ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)'] == 1).astype(int)
        
        if 'æ˜¼å¤œ' in df.columns:
            # æ˜¼å¤œã‚³ãƒ¼ãƒ‰ã‚’æ•°å€¤ã«ï¼ˆ1: æ˜¼, 2: å¤œ ãªã©ï¼‰
            df['is_night'] = (df['æ˜¼å¤œ'].isin([2, 3, 4, 5, 6, 7, 8, 9, 10])).astype(int)
        
        print("   ç”Ÿæˆå®Œäº†: hour_sin/cos, weekday_sin/cos, month_sin/cos, is_holiday, is_night")
        
        return df
    
    def generate_spatial_temporal_aggregates(self, df: pd.DataFrame) -> pd.DataFrame:
        """éå»ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã®ã‚¸ã‚ªãƒãƒƒã‚·ãƒ¥ã”ã¨ã®äº‹æ•…é›†è¨ˆï¼ˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ»é«˜é€Ÿç‰ˆï¼‰"""
        print("\nğŸ“Š ç©ºé–“ãƒ»æ™‚ç³»åˆ—é›†ç´„ç‰¹å¾´é‡ç”Ÿæˆä¸­ (ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹)...")
        
        # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
        df = df.sort_values('date').reset_index(drop=True)
        
        # å…¨æœŸé–“ã®æ—¥ä»˜ãƒ¬ãƒ³ã‚¸ã‚’ç”Ÿæˆ
        all_dates = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')
        all_geohashes = df['geohash'].dropna().unique()
        
        print(f"   æ—¥ä»˜ç¯„å›²: {all_dates.min()} - {all_dates.max()} ({len(all_dates)}æ—¥é–“)")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯Geohash: {len(all_geohashes):,}")
        
        # ========================================
        # å…¨äº‹æ•…ã®ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè¡Œ: æ—¥ä»˜, åˆ—: Geohashï¼‰
        # ========================================
        daily_counts = df.groupby(['date', 'geohash']).size().unstack(fill_value=0)
        
        # å…¨æ—¥ä»˜ã§reindexï¼ˆäº‹æ•…ãŒãªã„æ—¥ã‚’0ã§åŸ‹ã‚ã‚‹ï¼‰
        daily_counts = daily_counts.reindex(all_dates, fill_value=0)
        
        # å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ãƒ­ãƒ¼ãƒªãƒ³ã‚°é›†è¨ˆ
        for window in self.past_windows:
            print(f"   éå»{window}æ—¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (å…¨äº‹æ•…)...")
            
            # shift(1)ã§æœªæ¥æƒ…å ±ã‚’é™¤å¤–ã—ã€rolling sum
            rolled = daily_counts.shift(1).rolling(window=window, min_periods=1).sum()
            
            # ç¸¦æŒã¡ï¼ˆLongå½¢å¼ï¼‰ã«æˆ»ã™
            rolled_long = rolled.stack().reset_index()
            rolled_long.columns = ['date', 'geohash', f'geohash_accidents_past_{window}d']
            
            # å…ƒãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸
            df = df.merge(rolled_long, on=['date', 'geohash'], how='left')
            df[f'geohash_accidents_past_{window}d'] = df[f'geohash_accidents_past_{window}d'].fillna(0)
        
        # ========================================
        # æ­»äº¡äº‹æ•…ã®ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
        # ========================================
        fatal_df = df[df[self.target_col] == 1]
        if len(fatal_df) > 0:
            daily_fatal = fatal_df.groupby(['date', 'geohash']).size().unstack(fill_value=0)
            daily_fatal = daily_fatal.reindex(all_dates, fill_value=0)
            
            # å­˜åœ¨ã—ãªã„Geohashã‚’0ã§åŸ‹ã‚ã‚‹
            missing_geohashes = [g for g in all_geohashes if g not in daily_fatal.columns]
            for g in missing_geohashes:
                daily_fatal[g] = 0
            
            for window in self.past_windows:
                print(f"   éå»{window}æ—¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ (æ­»äº¡äº‹æ•…)...")
                
                rolled_fatal = daily_fatal.shift(1).rolling(window=window, min_periods=1).sum()
                rolled_fatal_long = rolled_fatal.stack().reset_index()
                rolled_fatal_long.columns = ['date', 'geohash', f'geohash_fatal_past_{window}d']
                
                df = df.merge(rolled_fatal_long, on=['date', 'geohash'], how='left')
                df[f'geohash_fatal_past_{window}d'] = df[f'geohash_fatal_past_{window}d'].fillna(0)
        else:
            for window in self.past_windows:
                df[f'geohash_fatal_past_{window}d'] = 0
        
        print("   é›†ç´„ç‰¹å¾´é‡ç”Ÿæˆå®Œäº† âœ…")
        
        return df
    
    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¬ æå€¤å‡¦ç†"""
        print("\nğŸ”§ æ¬ æå€¤å‡¦ç†ä¸­...")
        
        for col in df.columns:
            if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                # æ•°å€¤åˆ—: ä¸­å¤®å€¤ã§è£œå®Œ
                if df[col].isna().any():
                    median_val = df[col].median()
                    df[col] = df[col].fillna(median_val)
            elif df[col].dtype == 'object':
                # ã‚«ãƒ†ã‚´ãƒªåˆ—: "_missing" ã§è£œå®Œ
                if df[col].isna().any():
                    df[col] = df[col].fillna("_missing")
        
        print("   æ¬ æå€¤å‡¦ç†å®Œäº†")
        
        return df
    
    def identify_column_types(self, df: pd.DataFrame) -> Tuple[List[str], List[str], List[str]]:
        """ã‚«ãƒ©ãƒ ã‚¿ã‚¤ãƒ—ã®è­˜åˆ¥"""
        
        # é™¤å¤–ã™ã‚‹åˆ—
        exclude_cols = [
            self.target_col, 'date', 'lat', 'lon', 
            'åœ°ç‚¹ã€€ç·¯åº¦ï¼ˆåŒ—ç·¯ï¼‰', 'åœ°ç‚¹ã€€çµŒåº¦ï¼ˆæ±çµŒï¼‰',
            'geohash', 'geohash_fine', 'year'
        ]
        
        # æ—¢çŸ¥ã®ã‚«ãƒ†ã‚´ãƒªåˆ—
        known_categoricals = [
            'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¿¡å·æ©Ÿ', 'åœ°å½¢',
            'å¤©å€™', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'é“è·¯ç·šå½¢', 'è¡çªåœ°ç‚¹',
            'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†', 'æ˜¼å¤œ', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
            'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'area_id', 'road_type', 'terrain_id',
            'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€æ¨™è­˜ï¼ˆå½“äº‹è€…Aï¼‰', 'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€æ¨™è­˜ï¼ˆå½“äº‹è€…Bï¼‰',
            'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€è¡¨ç¤ºï¼ˆå½“äº‹è€…Aï¼‰', 'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€è¡¨ç¤ºï¼ˆå½“äº‹è€…Bï¼‰',
            'å½“äº‹è€…ç¨®åˆ¥ï¼ˆå½“äº‹è€…Aï¼‰', 'ç”¨é€”åˆ¥ï¼ˆå½“äº‹è€…Aï¼‰'
        ]
        
        low_cardinality_cats = []
        high_cardinality_cats = []
        numerical_cols = []
        
        for col in df.columns:
            if col in exclude_cols:
                continue
            
            if col in known_categoricals or df[col].dtype == 'object':
                nunique = df[col].nunique()
                if nunique < self.high_cardinality_threshold:
                    low_cardinality_cats.append(col)
                else:
                    high_cardinality_cats.append(col)
            else:
                numerical_cols.append(col)
        
        print(f"\nğŸ“‹ ã‚«ãƒ©ãƒ ã‚¿ã‚¤ãƒ—:")
        print(f"   æ•°å€¤åˆ—: {len(numerical_cols)}")
        print(f"   ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£: {len(low_cardinality_cats)}")
        print(f"   é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£: {len(high_cardinality_cats)}")
        
        return numerical_cols, low_cardinality_cats, high_cardinality_cats
    
    def split_by_time(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """æ™‚é–“ãƒ™ãƒ¼ã‚¹ã§ã®åˆ†å‰²"""
        print("\nâœ‚ï¸ æ™‚é–“ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä¸­...")
        
        train_mask = (df['year'] >= self.train_years[0]) & (df['year'] <= self.train_years[1])
        val_mask = (df['year'] >= self.val_years[0]) & (df['year'] <= self.val_years[1])
        test_mask = (df['year'] >= self.test_years[0]) & (df['year'] <= self.test_years[1])
        
        train_df = df[train_mask].copy()
        val_df = df[val_mask].copy()
        test_df = df[test_mask].copy()
        
        print(f"   Train: {len(train_df):,} (Fatal: {train_df[self.target_col].sum():,})")
        print(f"   Val:   {len(val_df):,} (Fatal: {val_df[self.target_col].sum():,})")
        print(f"   Test:  {len(test_df):,} (Fatal: {test_df[self.target_col].sum():,})")
        
        return train_df, val_df, test_df
    
    def fit_encoders(self, train_df: pd.DataFrame, numerical_cols: List[str], 
                     low_cardinality_cats: List[str], high_cardinality_cats: List[str]):
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å­¦ç¿’"""
        print("\nğŸ“ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å­¦ç¿’ä¸­...")
        
        # StandardScaler
        if numerical_cols:
            self.scaler.fit(train_df[numerical_cols])
            print(f"   StandardScaler: {len(numerical_cols)} åˆ—")
        
        # One-Hot Encoder
        if low_cardinality_cats:
            # ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            train_cats = train_df[low_cardinality_cats].astype(str)
            self.ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            self.ohe.fit(train_cats)
            print(f"   OneHotEncoder: {len(low_cardinality_cats)} åˆ— â†’ {len(self.ohe.get_feature_names_out())} ç‰¹å¾´é‡")
        
        # Target Encoder (K-Foldæ–¹å¼ã§ãƒªãƒ¼ã‚¯é˜²æ­¢)
        if high_cardinality_cats:
            self.target_encoders = {}  # åˆæœŸåŒ–
            n_te_folds = 5
            
            for col in high_cardinality_cats:
                global_mean = train_df[self.target_col].mean()
                
                # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®å¹³å‡ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
                category_means = train_df.groupby(col)[self.target_col].mean().to_dict()
                
                # K-Foldç”¨ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å€¤ã‚’è¨ˆç®—ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿å†…ã§ã®ä½¿ç”¨ï¼‰
                kfold_encoded = pd.Series(index=train_df.index, dtype=float)
                kf = KFold(n_splits=n_te_folds, shuffle=True, random_state=RANDOM_SEED)
                
                for tr_idx, val_idx in kf.split(train_df):
                    tr_data = train_df.iloc[tr_idx]
                    val_data = train_df.iloc[val_idx]
                    
                    # Foldå†…ã®å¹³å‡ã‚’è¨ˆç®—
                    fold_means = tr_data.groupby(col)[self.target_col].mean()
                    
                    # Valéƒ¨åˆ†ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæœªçŸ¥ã‚«ãƒ†ã‚´ãƒªã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹³å‡ï¼‰
                    kfold_encoded.iloc[val_idx] = val_data[col].map(fold_means).fillna(global_mean)
                
                self.target_encoders[col] = {
                    'global_mean': global_mean,
                    'category_means': category_means,
                    'kfold_encoded': kfold_encoded,  # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”¨
                }
            
            print(f"   TargetEncoder (K-Fold): {len(high_cardinality_cats)} åˆ—")
    
    def transform_data(self, df: pd.DataFrame, numerical_cols: List[str],
                       low_cardinality_cats: List[str], high_cardinality_cats: List[str],
                       is_train: bool = False) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å¤‰æ›"""
        
        result_dfs = []
        
        # åŸºæœ¬æƒ…å ±ã‚’ä¿æŒ
        meta_cols = ['lat', 'lon', 'geohash', 'geohash_fine', 'date', self.target_col, 'year']
        meta_df = df[[c for c in meta_cols if c in df.columns]].copy()
        result_dfs.append(meta_df)
        
        # æ•°å€¤åˆ—ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if numerical_cols:
            scaled = self.scaler.transform(df[numerical_cols])
            scaled_df = pd.DataFrame(scaled, columns=[f"{c}_scaled" for c in numerical_cols], index=df.index)
            result_dfs.append(scaled_df)
        
        # One-Hot Encoding
        if low_cardinality_cats and self.ohe is not None:
            cats = df[low_cardinality_cats].astype(str)
            ohe_transformed = self.ohe.transform(cats)
            ohe_df = pd.DataFrame(
                ohe_transformed, 
                columns=self.ohe.get_feature_names_out(),
                index=df.index
            )
            result_dfs.append(ohe_df)
        
        # Target Encoding (K-Foldæ–¹å¼: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯KFoldã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å€¤ã€ãƒ†ã‚¹ãƒˆ/Valã¯å…¨ä½“å¹³å‡)
        if high_cardinality_cats:
            for col in high_cardinality_cats:
                encoder = self.target_encoders.get(col)
                if encoder:
                    if is_train and 'kfold_encoded' in encoder:
                        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: K-Foldã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸå€¤ã‚’ä½¿ç”¨ï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
                        df[f"{col}_te"] = encoder['kfold_encoded'].reindex(df.index).fillna(encoder['global_mean'])
                    else:
                        # ãƒ†ã‚¹ãƒˆ/Valãƒ‡ãƒ¼ã‚¿: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®å¹³å‡å€¤ã‚’ä½¿ç”¨
                        df[f"{col}_te"] = df[col].map(encoder['category_means']).fillna(encoder['global_mean'])
                    result_dfs.append(df[[f"{col}_te"]])
        
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’å«ã‚ã‚‹
        temporal_cols = [c for c in df.columns if any(x in c for x in 
                        ['_sin', '_cos', 'is_holiday', 'is_night', 'past_'])]
        if temporal_cols:
            result_dfs.append(df[temporal_cols])
        
        result = pd.concat(result_dfs, axis=1)
        
        # é‡è¤‡åˆ—ã‚’å‰Šé™¤
        result = result.loc[:, ~result.columns.duplicated()]
        
        return result
    
    def save_outputs(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                     test_df: pd.DataFrame, numerical_cols: List[str],
                     low_cardinality_cats: List[str], high_cardinality_cats: List[str]):
        """å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        print("\nğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­...")
        
        # Parquetå½¢å¼ã§ä¿å­˜
        train_df.to_parquet(self.output_dir / "preprocessed_train.parquet", index=False)
        val_df.to_parquet(self.output_dir / "preprocessed_val.parquet", index=False)
        test_df.to_parquet(self.output_dir / "preprocessed_test.parquet", index=False)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ä¿å­˜
        joblib.dump(self.scaler, self.output_dir / "scaler.joblib")
        if self.ohe is not None:
            joblib.dump(self.ohe, self.output_dir / "ohe.joblib")
        joblib.dump(self.target_encoders, self.output_dir / "target_encoder.joblib")
        
        # ã‚«ãƒ©ãƒ æƒ…å ±ã®ä¿å­˜
        column_info = {
            'numerical_cols': numerical_cols,
            'low_cardinality_cats': low_cardinality_cats,
            'high_cardinality_cats': high_cardinality_cats,
            'target_col': self.target_col,
        }
        joblib.dump(column_info, self.output_dir / "column_info.joblib")
        
        print(f"   ä¿å­˜å…ˆ: {self.output_dir}")
        print(f"   - preprocessed_train.parquet ({len(train_df):,} è¡Œ)")
        print(f"   - preprocessed_val.parquet ({len(val_df):,} è¡Œ)")
        print(f"   - preprocessed_test.parquet ({len(test_df):,} è¡Œ)")
        print(f"   - scaler.joblib, ohe.joblib, target_encoder.joblib")
    
    def save_raw_outputs(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                         test_df: pd.DataFrame):
        """GBDTç”¨ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‰ï¼‰ã‚’ä¿å­˜"""
        print("\nğŸ’¾ GBDTç”¨ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¯ãã®ã¾ã¾ã€æ™‚ç©ºé–“ç‰¹å¾´é‡ã¯ä»˜ä¸
        meta_cols = ['lat', 'lon', 'geohash', 'geohash_fine', 'date', self.target_col, 'year']
        
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡ã¨ç©ºé–“é›†ç´„ç‰¹å¾´é‡ã‚’å«ã‚ã‚‹
        temporal_cols = [c for c in train_df.columns if any(x in c for x in 
                        ['_sin', '_cos', 'is_holiday', 'is_night', 'past_'])]
        
        # é™¤å¤–ã™ã‚‹åˆ—
        exclude_cols = ['date', 'åœ°ç‚¹ã€€ç·¯åº¦ï¼ˆåŒ—ç·¯ï¼‰', 'åœ°ç‚¹ã€€çµŒåº¦ï¼ˆæ±çµŒï¼‰']
        
        # ä¿å­˜ã™ã‚‹åˆ—ã‚’æ±ºå®š
        keep_cols = [c for c in train_df.columns if c not in exclude_cols]
        
        # Parquetå½¢å¼ã§ä¿å­˜
        train_df[keep_cols].to_parquet(self.output_dir / "raw_train.parquet", index=False)
        val_df[keep_cols].to_parquet(self.output_dir / "raw_val.parquet", index=False)
        test_df[keep_cols].to_parquet(self.output_dir / "raw_test.parquet", index=False)
        
        print(f"   ä¿å­˜å…ˆ: {self.output_dir}")
        print(f"   - raw_train.parquet ({len(train_df):,} è¡Œ, {len(keep_cols)} åˆ—)")
        print(f"   - raw_val.parquet ({len(val_df):,} è¡Œ)")
        print(f"   - raw_test.parquet ({len(test_df):,} è¡Œ)")
    
    def run(self, output_raw: bool = True) -> Dict:
        """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = self.load_data()
        
        # 2. ç·¯åº¦çµŒåº¦å¤‰æ›
        df = self.convert_lat_lon(df)
        
        # 3. ç„¡åŠ¹åº§æ¨™é™¤å»
        df = self.filter_invalid_coords(df)
        
        # 4. ã‚¸ã‚ªãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        df = self.generate_geohash(df)
        
        # 5. æ—¥ä»˜åˆ—ä½œæˆ
        df = self.create_date_column(df)
        
        # 6. æ™‚ç³»åˆ—ç‰¹å¾´é‡
        df = self.generate_temporal_features(df)
        
        # 7. ç©ºé–“ãƒ»æ™‚ç³»åˆ—é›†ç´„
        df = self.generate_spatial_temporal_aggregates(df)
        
        # 8. æ¬ æå€¤å‡¦ç†
        df = self.handle_missing_values(df)
        
        # 9. ã‚«ãƒ©ãƒ ã‚¿ã‚¤ãƒ—è­˜åˆ¥
        numerical_cols, low_cardinality_cats, high_cardinality_cats = self.identify_column_types(df)
        
        # 10. æ™‚é–“ãƒ™ãƒ¼ã‚¹åˆ†å‰²
        train_df, val_df, test_df = self.split_by_time(df)
        
        # 11. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å­¦ç¿’
        self.fit_encoders(train_df, numerical_cols, low_cardinality_cats, high_cardinality_cats)
        
        # 12. ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        train_transformed = self.transform_data(train_df, numerical_cols, low_cardinality_cats, high_cardinality_cats, is_train=True)
        val_transformed = self.transform_data(val_df, numerical_cols, low_cardinality_cats, high_cardinality_cats)
        test_transformed = self.transform_data(test_df, numerical_cols, low_cardinality_cats, high_cardinality_cats)
        
        # 13. ä¿å­˜
        self.save_outputs(train_transformed, val_transformed, test_transformed,
                         numerical_cols, low_cardinality_cats, high_cardinality_cats)
        
        # GBDTç”¨ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
        if output_raw:
            self.save_raw_outputs(train_df, val_df, test_df)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        print("\n" + "=" * 70)
        print(f"âœ… å‰å‡¦ç†å®Œäº†ï¼ (æ‰€è¦æ™‚é–“: {elapsed:.1f}ç§’)")
        print("=" * 70)
        
        return {
            'train_size': len(train_transformed),
            'val_size': len(val_transformed),
            'test_size': len(test_transformed),
            'n_features': len(train_transformed.columns),
            'elapsed_seconds': elapsed,
        }


def main():
    parser = argparse.ArgumentParser(description="Spatio-Temporal Preprocessing")
    parser.add_argument('--data-path', type=str, 
                        default="data/processed/honhyo_for_analysis_with_traffic_hospital_no_leakage.csv")
    parser.add_argument('--output-dir', type=str, default="data/spatio_temporal")
    parser.add_argument('--train-years', type=str, default="2018,2019")
    parser.add_argument('--val-years', type=str, default="2020,2020")
    parser.add_argument('--test-years', type=str, default="2021,2024")
    parser.add_argument('--geohash-precision', type=int, default=6)
    parser.add_argument('--test', action='store_true', help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå°è¦æ¨¡ã‚µãƒ–ã‚»ãƒƒãƒˆã§å®Ÿè¡Œï¼‰")
    
    args = parser.parse_args()
    
    train_years = tuple(map(int, args.train_years.split(',')))
    val_years = tuple(map(int, args.val_years.split(',')))
    test_years = tuple(map(int, args.test_years.split(',')))
    
    preprocessor = SpatioTemporalPreprocessor(
        data_path=args.data_path,
        output_dir=args.output_dir,
        train_years=train_years,
        val_years=val_years,
        test_years=test_years,
        geohash_precision=args.geohash_precision,
    )
    
    result = preprocessor.run()
    print(f"\nçµæœ: {result}")


if __name__ == "__main__":
    main()

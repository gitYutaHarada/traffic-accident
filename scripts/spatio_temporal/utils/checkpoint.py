"""
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ»å†é–‹æ©Ÿèƒ½
=======================
"""

import os
import json
import torch
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime


class CheckpointManager:
    """
    å­¦ç¿’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ç®¡ç†
    
    - ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ä¿å­˜
    - ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®çŠ¶æ…‹ä¿å­˜
    - å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ä¿å­˜
    - ä¸­æ–­ã‹ã‚‰ã®å†é–‹
    """
    
    def __init__(
        self,
        checkpoint_dir: str,
        max_checkpoints: int = 5,
    ):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.max_checkpoints = max_checkpoints
        self.status_file = self.checkpoint_dir / "training_status.json"
    
    def save_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        epoch: int,
        step: int,
        metrics: Dict[str, float],
        config: Dict[str, Any],
        scheduler: Optional[Any] = None,
        is_best: bool = False,
    ):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metrics': metrics,
            'config': config,
            'timestamp': datetime.now().isoformat(),
        }
        
        if scheduler is not None:
            checkpoint['scheduler_state_dict'] = scheduler.state_dict()
        
        # é€šå¸¸ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
        
        # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        latest_path = self.checkpoint_dir / "latest_checkpoint.pt"
        torch.save(checkpoint, latest_path)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
        self._update_status(epoch, step, metrics)
        
        # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤
        self._cleanup_old_checkpoints()
        
        print(f"   ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: epoch={epoch}, step={step}")
    
    def load_checkpoint(
        self,
        model: torch.nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        checkpoint_path: Optional[str] = None,
    ) -> Dict[str, Any]:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        
        if checkpoint_path is None:
            checkpoint_path = self.checkpoint_dir / "latest_checkpoint.pt"
        else:
            checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            print("   â„¹ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦é–‹å§‹ã—ã¾ã™ã€‚")
            return {'epoch': 0, 'step': 0, 'metrics': {}}
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã®å¾©å…ƒ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã®å¾©å…ƒ
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©çŠ¶æ…‹ã®å¾©å…ƒ
        if scheduler is not None and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"   ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: epoch={checkpoint['epoch']}")
        
        return checkpoint
    
    def load_best_model(self, model: torch.nn.Module) -> Dict[str, Any]:
        """ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        best_path = self.checkpoint_dir / "best_model.pt"
        
        if not best_path.exists():
            print("   âš ï¸ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        checkpoint = torch.load(best_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"   ğŸ“‚ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: epoch={checkpoint['epoch']}")
        
        return checkpoint
    
    def get_resume_info(self) -> Dict[str, Any]:
        """å†é–‹æƒ…å ±ã®å–å¾—"""
        if not self.status_file.exists():
            return {'should_resume': False, 'epoch': 0, 'step': 0}
        
        with open(self.status_file, 'r') as f:
            status = json.load(f)
        
        return {
            'should_resume': True,
            'epoch': status.get('epoch', 0),
            'step': status.get('step', 0),
            'metrics': status.get('metrics', {}),
        }
    
    def _update_status(self, epoch: int, step: int, metrics: Dict):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°"""
        status = {
            'epoch': epoch,
            'step': step,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat(),
        }
        
        with open(self.status_file, 'w') as f:
            json.dump(status, f, indent=2)
    
    def _cleanup_old_checkpoints(self):
        """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‰Šé™¤"""
        checkpoints = list(self.checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        for old_ckpt in checkpoints[self.max_checkpoints:]:
            old_ckpt.unlink()
    
    def clear_checkpoints(self):
        """å…¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‰Šé™¤"""
        for f in self.checkpoint_dir.glob("*.pt"):
            f.unlink()
        if self.status_file.exists():
            self.status_file.unlink()
        print("   ğŸ—‘ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢")


class EarlyStopping:
    """
    Early Stopping
    
    æ¤œè¨¼æå¤±ãŒæ”¹å–„ã—ãªã„å ´åˆã«å­¦ç¿’ã‚’åœæ­¢
    """
    
    def __init__(
        self,
        patience: int = 10,
        min_delta: float = 0.0001,
        mode: str = 'min',  # 'min' or 'max'
    ):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        
        self.counter = 0
        self.best_value = None
        self.should_stop = False
    
    def __call__(self, value: float) -> bool:
        """
        å€¤ã‚’è©•ä¾¡ã—ã¦ã‚¹ãƒˆãƒƒãƒ—ã™ã‚‹ã‹åˆ¤å®š
        
        Returns:
            True: æ”¹å–„ã‚ã‚Šï¼ˆis_bestï¼‰
            False: æ”¹å–„ãªã—
        """
        if self.best_value is None:
            self.best_value = value
            return True
        
        if self.mode == 'min':
            improved = value < self.best_value - self.min_delta
        else:
            improved = value > self.best_value + self.min_delta
        
        if improved:
            self.best_value = value
            self.counter = 0
            return True
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
            return False
    
    def reset(self):
        self.counter = 0
        self.best_value = None
        self.should_stop = False


def set_seed(seed: int = 42):
    """ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã®å›ºå®š"""
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    print(f"   ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š: {seed}")

"""
è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
=============
PR-AUC, ROC-AUC, Precision@k, Recall@k, ECE, Brier Score
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_recall_curve,
    roc_curve, brier_score_loss, precision_score, recall_score, f1_score
)
import time


def compute_pr_auc(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """PR-AUC (Average Precision) ã®è¨ˆç®—"""
    return average_precision_score(y_true, y_pred)


def compute_roc_auc(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """ROC-AUC ã®è¨ˆç®—"""
    return roc_auc_score(y_true, y_pred)


def compute_precision_at_k(y_true: np.ndarray, y_pred: np.ndarray, k: int) -> float:
    """
    Precision@k: Top-käºˆæ¸¬ã®ã†ã¡æ­£ä¾‹ã®å‰²åˆ
    """
    n = len(y_true)
    k = min(k, n)
    
    # äºˆæ¸¬ç¢ºç‡ã§ã‚½ãƒ¼ãƒˆã—ã¦Top-kã‚’å–å¾—
    top_k_indices = np.argsort(y_pred)[::-1][:k]
    
    return y_true[top_k_indices].sum() / k


def compute_recall_at_k(y_true: np.ndarray, y_pred: np.ndarray, k: int) -> float:
    """
    Recall@k: å…¨æ­£ä¾‹ã®ã†ã¡Top-kã«å«ã¾ã‚Œã‚‹å‰²åˆ
    """
    n = len(y_true)
    k = min(k, n)
    
    top_k_indices = np.argsort(y_pred)[::-1][:k]
    
    total_positives = y_true.sum()
    if total_positives == 0:
        return 0.0
    
    return y_true[top_k_indices].sum() / total_positives


def compute_ece(y_true: np.ndarray, y_pred: np.ndarray, n_bins: int = 10) -> float:
    """
    Expected Calibration Error (ECE)
    
    äºˆæ¸¬ç¢ºç‡ã¨å®Ÿéš›ã®é »åº¦ã®ãšã‚Œã‚’æ¸¬å®š
    """
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    
    ece = 0.0
    total_samples = len(y_true)
    
    for i in range(n_bins):
        bin_lower = bin_boundaries[i]
        bin_upper = bin_boundaries[i + 1]
        
        # ã“ã®ãƒ“ãƒ³ã«å…¥ã‚‹ã‚µãƒ³ãƒ—ãƒ«
        in_bin = (y_pred >= bin_lower) & (y_pred < bin_upper)
        n_in_bin = in_bin.sum()
        
        if n_in_bin > 0:
            # å®Ÿéš›ã®æ­£ä¾‹ç‡
            accuracy_in_bin = y_true[in_bin].mean()
            # äºˆæ¸¬ç¢ºç‡ã®å¹³å‡
            confidence_in_bin = y_pred[in_bin].mean()
            
            ece += (n_in_bin / total_samples) * abs(accuracy_in_bin - confidence_in_bin)
    
    return ece


def compute_brier_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Brier Score ã®è¨ˆç®—"""
    return brier_score_loss(y_true, y_pred)


def compute_metrics_at_threshold(
    y_true: np.ndarray, 
    y_pred: np.ndarray, 
    threshold: float
) -> Dict[str, float]:
    """ç‰¹å®šã®é–¾å€¤ã§ã®è©•ä¾¡æŒ‡æ¨™"""
    y_pred_binary = (y_pred >= threshold).astype(int)
    
    return {
        'precision': precision_score(y_true, y_pred_binary, zero_division=0),
        'recall': recall_score(y_true, y_pred_binary, zero_division=0),
        'f1': f1_score(y_true, y_pred_binary, zero_division=0),
    }


def find_threshold_for_recall(
    y_true: np.ndarray, 
    y_pred: np.ndarray, 
    target_recall: float
) -> Tuple[float, float]:
    """
    ç›®æ¨™Recallã‚’é”æˆã™ã‚‹é–¾å€¤ã¨ãã®ã¨ãã®Precisionã‚’è¿”ã™
    """
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)
    
    # target_recallä»¥ä¸Šã‚’é”æˆã™ã‚‹é–¾å€¤ã‚’æ¢ã™
    valid_idx = np.where(recalls >= target_recall)[0]
    
    if len(valid_idx) == 0:
        return 0.0, 0.0
    
    # PrecisionãŒæœ€å¤§ã¨ãªã‚‹é–¾å€¤
    best_idx = valid_idx[np.argmax(precisions[valid_idx])]
    
    if best_idx < len(thresholds):
        return thresholds[best_idx], precisions[best_idx]
    else:
        return 0.0, precisions[best_idx]


def evaluate_model(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    k_values: List[int] = [100, 500, 1000],
    recall_targets: List[float] = [0.99, 0.95, 0.90],
) -> Dict:
    """
    åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    
    Returns:
        metrics: è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
    """
    metrics = {}
    
    # åŸºæœ¬æŒ‡æ¨™
    metrics['pr_auc'] = compute_pr_auc(y_true, y_pred)
    metrics['roc_auc'] = compute_roc_auc(y_true, y_pred)
    metrics['ece'] = compute_ece(y_true, y_pred)
    metrics['brier_score'] = compute_brier_score(y_true, y_pred)
    
    # Precision@k, Recall@k
    for k in k_values:
        metrics[f'precision_at_{k}'] = compute_precision_at_k(y_true, y_pred, k)
        metrics[f'recall_at_{k}'] = compute_recall_at_k(y_true, y_pred, k)
    
    # å‹•çš„é–¾å€¤è©•ä¾¡
    for target_recall in recall_targets:
        thresh, prec = find_threshold_for_recall(y_true, y_pred, target_recall)
        metrics[f'threshold_at_recall_{int(target_recall*100)}'] = thresh
        metrics[f'precision_at_recall_{int(target_recall*100)}'] = prec
    
    # å›ºå®šé–¾å€¤è©•ä¾¡
    for thresh in [0.3, 0.5, 0.7]:
        thresh_metrics = compute_metrics_at_threshold(y_true, y_pred, thresh)
        for k, v in thresh_metrics.items():
            metrics[f'{k}_at_{thresh}'] = v
    
    return metrics


def measure_inference_time(
    model,
    sample_input,
    n_samples: int = 100,
    warmup: int = 10,
) -> Dict:
    """
    æ¨è«–æ™‚é–“ã®è¨ˆæ¸¬
    
    Returns:
        timing: æ¨è«–æ™‚é–“çµ±è¨ˆ
    """
    import torch
    
    # Warmup
    with torch.no_grad():
        for _ in range(warmup):
            _ = model(*sample_input) if isinstance(sample_input, tuple) else model(sample_input)
    
    # è¨ˆæ¸¬
    times = []
    with torch.no_grad():
        for _ in range(n_samples):
            start = time.perf_counter()
            _ = model(*sample_input) if isinstance(sample_input, tuple) else model(sample_input)
            times.append(time.perf_counter() - start)
    
    times = np.array(times) * 1000  # ms
    
    return {
        'mean_ms': float(np.mean(times)),
        'std_ms': float(np.std(times)),
        'median_ms': float(np.median(times)),
        'min_ms': float(np.min(times)),
        'max_ms': float(np.max(times)),
    }


def generate_evaluation_report(
    results: Dict[str, Dict],
    output_path: str,
) -> str:
    """
    è©•ä¾¡çµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    report = []
    report.append("# Spatio-Temporal Model è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\n")
    report.append(f"ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
    report.append("\n## ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\n")
    
    # ä¸»è¦æŒ‡æ¨™
    report.append("### ä¸»è¦è©•ä¾¡æŒ‡æ¨™\n")
    report.append("| ãƒ¢ãƒ‡ãƒ« | PR-AUC | ROC-AUC | ECE | Brier Score |\n")
    report.append("|--------|--------|---------|-----|-------------|\n")
    
    for model_name, metrics in results.items():
        report.append(
            f"| {model_name} | {metrics.get('pr_auc', 0):.4f} | "
            f"{metrics.get('roc_auc', 0):.4f} | {metrics.get('ece', 0):.4f} | "
            f"{metrics.get('brier_score', 0):.4f} |\n"
        )
    
    # Precision/Recall@k
    report.append("\n### Precision/Recall@k\n")
    report.append("| ãƒ¢ãƒ‡ãƒ« | P@100 | R@100 | P@500 | R@500 | P@1000 | R@1000 |\n")
    report.append("|--------|-------|-------|-------|-------|--------|--------|\n")
    
    for model_name, metrics in results.items():
        report.append(
            f"| {model_name} | "
            f"{metrics.get('precision_at_100', 0):.4f} | {metrics.get('recall_at_100', 0):.4f} | "
            f"{metrics.get('precision_at_500', 0):.4f} | {metrics.get('recall_at_500', 0):.4f} | "
            f"{metrics.get('precision_at_1000', 0):.4f} | {metrics.get('recall_at_1000', 0):.4f} |\n"
        )
    
    # å‹•çš„é–¾å€¤
    report.append("\n### å‹•çš„é–¾å€¤è©•ä¾¡\n")
    report.append("| ãƒ¢ãƒ‡ãƒ« | Recall=99% Precision | Recall=95% Precision | Recall=90% Precision |\n")
    report.append("|--------|---------------------|---------------------|---------------------|\n")
    
    for model_name, metrics in results.items():
        report.append(
            f"| {model_name} | "
            f"{metrics.get('precision_at_recall_99', 0):.4f} | "
            f"{metrics.get('precision_at_recall_95', 0):.4f} | "
            f"{metrics.get('precision_at_recall_90', 0):.4f} |\n"
        )
    
    report_text = "".join(report)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    return report_text


class ModelEvaluator:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "results/spatio_temporal"):
        self.output_dir = output_dir
        self.results = {}
    
    def add_result(self, model_name: str, y_true: np.ndarray, y_pred: np.ndarray):
        """è©•ä¾¡çµæœã‚’è¿½åŠ """
        metrics = evaluate_model(y_true, y_pred)
        self.results[model_name] = metrics
        
        print(f"\nğŸ“Š {model_name} è©•ä¾¡çµæœ:")
        print(f"   PR-AUC: {metrics['pr_auc']:.4f}")
        print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"   ECE: {metrics['ece']:.4f}")
    
    def generate_report(self):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        import os
        os.makedirs(self.output_dir, exist_ok=True)
        
        report_path = os.path.join(self.output_dir, "evaluation_report.md")
        report = generate_evaluation_report(self.results, report_path)
        
        print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_path}")
        
        return report
    
    def save_results(self):
        """çµæœã‚’JSONå½¢å¼ã§ä¿å­˜"""
        import json
        import os
        
        os.makedirs(self.output_dir, exist_ok=True)
        results_path = os.path.join(self.output_dir, "results_summary.json")
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ çµæœä¿å­˜: {results_path}")

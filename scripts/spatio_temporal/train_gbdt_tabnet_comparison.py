"""
GBDT vs TabNet å…¬å¹³æ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
=================================
TabNetã¨åŒã˜ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ (honhyo_clean_with_features.csv) ã‚’ä½¿ç”¨ã—ã¦
LightGBMã‚’å­¦ç¿’ã—ã€å…¬å¹³ãªæ¯”è¼ƒã‚’è¡Œã†ã€‚

æ¯”è¼ƒæ¡ä»¶ã‚’çµ±ä¸€:
- åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: honhyo_clean_with_features.csv
- åŒã˜Train/Teståˆ†å‰²: 80/20 random split (stratified)
- åŒã˜5-Fold CV
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import json
import os
from pathlib import Path
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_recall_curve, 
    precision_score, recall_score, f1_score, brier_score_loss
)
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# --- ãƒ‘ã‚¹è¨­å®š ---
DATA_PATH = Path("data/processed/honhyo_clean_with_features.csv")
RESULTS_DIR = Path("results/spatio_temporal")
OUTPUT_DIR = RESULTS_DIR / "gbdt_tabnet_comparison"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'MS Gothic'

# --- ãƒªãƒ¼ã‚¯é˜²æ­¢ ---
FORBIDDEN_COLUMNS = [
    'äº‹æ•…å†…å®¹',
    'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'äººèº«æå‚·ç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',
    'è² å‚·è€…æ•°',
    'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Aï¼‰', 'è»Šä¸¡ã®æå£Šç¨‹åº¦ï¼ˆå½“äº‹è€…Bï¼‰',
    'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Aï¼‰', 'è»Šä¸¡ã®è¡çªéƒ¨ä½ï¼ˆå½“äº‹è€…Bï¼‰',
    'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰', 'ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
    'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Aï¼‰', 'ã‚µã‚¤ãƒ‰ã‚¨ã‚¢ãƒãƒƒã‚°ã®è£…å‚™ï¼ˆå½“äº‹è€…Bï¼‰',
]


def load_data():
    """TabNetã¨åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ (honhyo_clean_with_features.csv)...")
    df = pd.read_csv(DATA_PATH)
    print(f"   ãƒ‡ãƒ¼ã‚¿: {len(df):,} è¡Œ, {len(df.columns)} åˆ—")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—
    target_col = 'æ­»è€…æ•°'
    y = (df[target_col] > 0).astype(int)
    
    # ç‰¹å¾´é‡
    X = df.drop(columns=[target_col])
    if 'ç™ºç”Ÿæ—¥æ™‚' in X.columns:
        X = X.drop(columns=['ç™ºç”Ÿæ—¥æ™‚'])
    
    # ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯
    leaked = [col for col in FORBIDDEN_COLUMNS if col in X.columns]
    if leaked:
        print(f"   âš ï¸ ãƒªãƒ¼ã‚¯è­¦å‘Š: {leaked}")
        X = X.drop(columns=leaked)
    
    print(f"   ç‰¹å¾´é‡: {len(X.columns)} åˆ—")
    print(f"   ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: 0={sum(y==0):,}, 1={sum(y==1):,} ({sum(y==1)/len(y)*100:.2f}%)")
    
    return X, y


def prepare_features(X):
    """TabNetã¨åŒæ§˜ã®å‰å‡¦ç†ï¼ˆLightGBMç”¨ï¼‰"""
    print("\nğŸ”§ ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ—ã¨æ•°å€¤åˆ—ã‚’è­˜åˆ¥
    known_categoricals = [
        'éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰', 'å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰', 'è­¦å¯Ÿç½²ç­‰ã‚³ãƒ¼ãƒ‰',
        'æ˜¼å¤œ', 'å¤©å€™', 'åœ°å½¢', 'è·¯é¢çŠ¶æ…‹', 'é“è·¯å½¢çŠ¶', 'ä¿¡å·æ©Ÿ',
        'è¡çªåœ°ç‚¹', 'ã‚¾ãƒ¼ãƒ³è¦åˆ¶', 'ä¸­å¤®åˆ†é›¢å¸¯æ–½è¨­ç­‰', 'æ­©è»Šé“åŒºåˆ†',
        'äº‹æ•…é¡å‹', 'æ›œæ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)', 'ç¥æ—¥(ç™ºç”Ÿå¹´æœˆæ—¥)',
        'road_type', 'area_id', 'åœ°ç‚¹ã‚³ãƒ¼ãƒ‰', 'é“è·¯ç·šå½¢',
        'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€æ¨™è­˜ï¼ˆå½“äº‹è€…Aï¼‰', 'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€æ¨™è­˜ï¼ˆå½“äº‹è€…Bï¼‰',
        'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€è¡¨ç¤ºï¼ˆå½“äº‹è€…Aï¼‰', 'ä¸€æ™‚åœæ­¢è¦åˆ¶ã€€è¡¨ç¤ºï¼ˆå½“äº‹è€…Bï¼‰'
    ]
    
    categorical_cols = []
    numeric_cols = []
    
    for col in X.columns:
        if col in known_categoricals or X[col].dtype == 'object':
            categorical_cols.append(col)
        else:
            numeric_cols.append(col)
    
    print(f"   ã‚«ãƒ†ã‚´ãƒªåˆ—: {len(categorical_cols)}")
    print(f"   æ•°å€¤åˆ—: {len(numeric_cols)}")
    
    # LightGBMç”¨: categoryå‹ã«å¤‰æ›
    X_lgb = X.copy()
    for col in categorical_cols:
        X_lgb[col] = X_lgb[col].astype('category')
    for col in numeric_cols:
        X_lgb[col] = pd.to_numeric(X_lgb[col], errors='coerce').astype(np.float32)
    
    # æ¬ æå€¤è£œå®Œ (æ•°å€¤åˆ—ã®ã¿)
    for col in numeric_cols:
        if X_lgb[col].isna().any():
            X_lgb[col] = X_lgb[col].fillna(X_lgb[col].median())
    
    return X_lgb, categorical_cols, numeric_cols


def train_lightgbm_cv(X_train, y_train, n_folds=5, random_state=42):
    """5-Fold CVã§LightGBMã‚’å­¦ç¿’"""
    print(f"\nğŸŒ² LightGBM {n_folds}-Fold CV å­¦ç¿’ä¸­...")
    
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    
    oof_proba = np.zeros(len(y_train))
    feature_importances = np.zeros(X_train.shape[1])
    models = []
    
    lgb_params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 63,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_child_samples': 20,
        'verbose': -1,
        'random_state': random_state,
        'is_unbalance': True,
    }
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
        print(f"   Fold {fold+1}/{n_folds}...")
        
        X_tr = X_train.iloc[train_idx]
        X_val = X_train.iloc[val_idx]
        y_tr = y_train.iloc[train_idx]
        y_val = y_train.iloc[val_idx]
        
        dtrain = lgb.Dataset(X_tr, label=y_tr)
        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)
        
        model = lgb.train(
            lgb_params, dtrain,
            num_boost_round=1000,
            valid_sets=[dtrain, dval],
            valid_names=['train', 'valid'],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(0)
            ]
        )
        
        oof_proba[val_idx] = model.predict(X_val)
        feature_importances += model.feature_importance(importance_type='gain') / n_folds
        models.append(model)
        
        fold_auc = roc_auc_score(y_val, oof_proba[val_idx])
        print(f"      Fold {fold+1} AUC: {fold_auc:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    importance_df = pd.DataFrame({
        'feature': X_train.columns,
        'importance': feature_importances
    }).sort_values('importance', ascending=False)
    
    return models, oof_proba, importance_df


def evaluate_metrics(y_true, y_pred_proba):
    """è©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    # åŸºæœ¬æŒ‡æ¨™
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    pr_auc = average_precision_score(y_true, y_pred_proba)
    brier = brier_score_loss(y_true, y_pred_proba)
    
    # Top-k Precision
    sorted_indices = np.argsort(y_pred_proba)[::-1]
    top_k_results = {}
    for k in [100, 500, 1000]:
        if k <= len(y_true):
            top_k_idx = sorted_indices[:k]
            top_k_precision = y_true.iloc[top_k_idx].sum() / k
            top_k_results[f'precision_at_{k}'] = float(top_k_precision)
    
    # ç‰¹å®šRecallã§ã®é–¾å€¤ã¨Precision
    precision_curve, recall_curve, thresholds = precision_recall_curve(y_true, y_pred_proba)
    recall_targets = {}
    for target_recall in [0.99, 0.95, 0.90]:
        idx = np.searchsorted(recall_curve[::-1], target_recall)
        if idx < len(thresholds):
            thresh = thresholds[::-1][idx] if idx < len(thresholds) else 0.0
            prec = precision_curve[::-1][idx] if idx < len(precision_curve) else 0.0
            recall_targets[f'threshold_at_recall_{int(target_recall*100)}'] = float(thresh)
            recall_targets[f'precision_at_recall_{int(target_recall*100)}'] = float(prec)
    
    # Best F1
    f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-15)
    best_f1_idx = np.argmax(f1_scores)
    best_f1 = f1_scores[best_f1_idx]
    best_thresh = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5
    best_prec = precision_curve[best_f1_idx]
    best_rec = recall_curve[best_f1_idx]
    
    metrics = {
        'roc_auc': roc_auc,
        'pr_auc': pr_auc,
        'brier_score': brier,
        'best_f1': best_f1,
        'best_f1_threshold': best_thresh,
        'best_f1_precision': best_prec,
        'best_f1_recall': best_rec,
        **top_k_results,
        **recall_targets,
    }
    
    return metrics


def compare_with_tabnet(lgb_metrics):
    """TabNetçµæœã¨æ¯”è¼ƒ"""
    print("\n" + "=" * 70)
    print(" ğŸ”„ LightGBM vs TabNet æ¯”è¼ƒ (åŒä¸€ãƒ‡ãƒ¼ã‚¿ãƒ»åŒä¸€æ¡ä»¶)")
    print("=" * 70)
    
    # TabNetçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    tabnet_paths = [
        Path("results/two_stage_model/tabnet_pipeline/experiment_report.md"),
        Path("results/oof/oof_stage2_tabnet.csv"),
    ]
    
    # TabNetå‹•çš„é–¾å€¤è©•ä¾¡çµæœ (comparison_mlp_tabnet.md ã‹ã‚‰)
    # Recall 95%æ™‚ã®Precision: 2.58%
    tabnet_results = {
        'roc_auc': 0.8393,  # Stage 2 (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ), from comparison report
        'precision_at_recall_95': 0.0258,  # from comparison report
    }
    
    comparisons = [
        ('ROC-AUC', 'roc_auc', tabnet_results.get('roc_auc', 0)),
        ('Recall 95% Precision', 'precision_at_recall_95', tabnet_results.get('precision_at_recall_95', 0)),
        ('Best F1', 'best_f1', 0),  # TabNetã®Best F1ã¯åˆ¥é€”å–å¾—å¿…è¦
    ]
    
    print(f"\n   {'æŒ‡æ¨™':<30} {'TabNet':<15} {'LightGBM':<15} {'å·®åˆ†':<10}")
    print("   " + "-" * 70)
    
    comparison_results = {}
    for name, key, tabnet_val in comparisons:
        lgb_val = lgb_metrics.get(key, 0)
        diff = lgb_val - tabnet_val
        diff_str = f"+{diff:.4f}" if diff >= 0 else f"{diff:.4f}"
        print(f"   {name:<30} {tabnet_val:<15.4f} {lgb_val:<15.4f} {diff_str:<10}")
        comparison_results[key] = {'tabnet': tabnet_val, 'lgb': lgb_val, 'diff': diff}
    
    return comparison_results


def save_results(models, oof_proba, y_train, metrics, importance_df, comparison_results):
    """çµæœã‚’ä¿å­˜"""
    print("\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    for i, model in enumerate(models):
        model.save_model(str(OUTPUT_DIR / f"lightgbm_fold{i+1}.txt"))
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
    results = {
        'model_type': 'lightgbm',
        'data_source': 'honhyo_clean_with_features.csv',
        'comparison_note': 'TabNetã¨åŒã˜ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§å­¦ç¿’',
        'oof_metrics': metrics,
        'comparison_with_tabnet': comparison_results,
    }
    with open(OUTPUT_DIR / "results_lightgbm_tabnet_comparison.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜
    importance_df.to_csv(OUTPUT_DIR / "feature_importance.csv", index=False, encoding='utf-8-sig')
    
    # OOFäºˆæ¸¬ä¿å­˜
    oof_df = pd.DataFrame({
        'true_label': y_train.values,
        'prob': oof_proba
    })
    oof_df.to_csv(OUTPUT_DIR / "oof_predictions.csv", index=False)
    
    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(12, 8))
    top20 = importance_df.head(20)
    plt.barh(top20['feature'][::-1], top20['importance'][::-1])
    plt.xlabel('Importance (Gain)')
    plt.title('Top 20 Feature Importance (LightGBM - TabNet Comparison)')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "feature_importance.png", dpi=150)
    plt.close()
    
    print(f"   çµæœä¿å­˜å…ˆ: {OUTPUT_DIR}")


def main():
    print("=" * 70)
    print(" ğŸŒ² LightGBM vs TabNet å…¬å¹³æ¯”è¼ƒ")
    print(" (åŒã˜ãƒ‡ãƒ¼ã‚¿: honhyo_clean_with_features.csv)")
    print("=" * 70)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    X, y = load_data()
    
    # 2. å‰å‡¦ç†
    X_lgb, categorical_cols, numeric_cols = prepare_features(X)
    
    # 3. Train/Teståˆ†å‰² (TabNetã¨åŒã˜: 80/20)
    print("\nâœ‚ï¸ Train/Teståˆ†å‰² (80/20)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_lgb, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"   Train: {len(y_train):,} (Fatal: {y_train.sum():,})")
    print(f"   Test:  {len(y_test):,} (Fatal: {y_test.sum():,})")
    
    # 4. 5-Fold CVã§å­¦ç¿’
    models, oof_proba, importance_df = train_lightgbm_cv(X_train, y_train)
    
    # 5. OOFè©•ä¾¡
    print("\nğŸ“Š OOFè©•ä¾¡ (Cross Validation)...")
    oof_metrics = evaluate_metrics(y_train, oof_proba)
    
    print(f"\n   ROC-AUC: {oof_metrics['roc_auc']:.4f}")
    print(f"   PR-AUC:  {oof_metrics['pr_auc']:.4f}")
    print(f"   Best F1: {oof_metrics['best_f1']:.4f} (é–¾å€¤: {oof_metrics['best_f1_threshold']:.4f})")
    print(f"   Recall 95% Precision: {oof_metrics.get('precision_at_recall_95', 0):.4f}")
    
    # 6. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡
    print("\nğŸ“Š ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡...")
    test_proba = np.zeros(len(y_test))
    for model in models:
        test_proba += model.predict(X_test) / len(models)
    
    test_metrics = evaluate_metrics(y_test, test_proba)
    print(f"   Test ROC-AUC: {test_metrics['roc_auc']:.4f}")
    print(f"   Test PR-AUC:  {test_metrics['pr_auc']:.4f}")
    print(f"   Test Best F1: {test_metrics['best_f1']:.4f}")
    print(f"   Test Recall 95% Precision: {test_metrics.get('precision_at_recall_95', 0):.4f}")
    
    # 7. TabNetã¨æ¯”è¼ƒ
    comparison_results = compare_with_tabnet(oof_metrics)
    
    # 8. çµæœä¿å­˜
    save_results(models, oof_proba, y_train, oof_metrics, importance_df, comparison_results)
    
    print("\nğŸ‰ LightGBM vs TabNet æ¯”è¼ƒå®Œäº†ï¼")
    
    # ã‚µãƒãƒªãƒ¼å‡ºåŠ›
    print("\n" + "=" * 70)
    print(" ğŸ“‹ ã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    print(f"   ãƒ‡ãƒ¼ã‚¿: honhyo_clean_with_features.csv ({len(X):,} ä»¶)")
    print(f"   ç‰¹å¾´é‡: {len(X.columns)} åˆ— (ã‚«ãƒ†ã‚´ãƒª: {len(categorical_cols)}, æ•°å€¤: {len(numeric_cols)})")
    print(f"   LightGBM OOF ROC-AUC: {oof_metrics['roc_auc']:.4f}")
    print(f"   LightGBM OOF PR-AUC:  {oof_metrics['pr_auc']:.4f}")
    print(f"   LightGBM Test ROC-AUC: {test_metrics['roc_auc']:.4f}")


if __name__ == "__main__":
    main()

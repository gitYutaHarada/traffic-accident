# Stage 2 アンサンブル実験レポート (TabNet + LightGBM)

**実行日時**: 2025-12-19
**目的**: Stage 2 (負傷 vs 死亡) におけるTabNetとLightGBMのアンサンブル効果の検証および実装された途中再開機能の確認

## 1. 個別モデルの評価結果 (CV OOF Result)

Scoreは5-Fold Cross ValidationのOut-Of-Fold予測に基づく。

| モデル | AUC | F1 Score | Precision | Recall |
| :--- | :--- | :--- | :--- | :--- |
| **TabNet** | **0.8382** | **0.2464** | 0.2112 | 0.2957 |
| LightGBM | 0.7770 | 0.1857 | 0.1435 | 0.2632 |

- **TabNet** が単体性能でLightGBMを大きく上回った。
- TabNetはF1スコアで約0.06、AUCで約0.06の差をつけており、今回のタスク（不均衡データの分類）においてこの差は非常に大きい。
- LightGBMはRecall重視でチューニングされているが、全体の識別能力（AUC）でTabNetに劣る結果となった。

## 2. アンサンブル評価

**モデル間予測値の相関**: `0.7393`
- 中程度の相関。全く異なる予測をしているわけではないが、ある程度の補完効果が期待できる相関レベル。

### 最適重み探索 (F1 Score 最大化)
評価式: `Ensemble_Prob = w * LightGBM + (1-w) * TabNet`

| Weight (LGB) | AUC | F1 Score | 構成比 |
| :--- | :--- | :--- | :--- |
| 0.0 (TabNetのみ) | 0.8382 | 0.2464 | TabNet 100% |
| **0.1** | **0.8382** | **0.2472** | **TabNet 90% + LGB 10%** |
| 0.2 | 0.8380 | 0.2468 | TabNet 80% + LGB 20% |
| 0.5 | 0.8347 | 0.2291 | TabNet 50% + LGB 50% |
| 1.0 (LGBのみ) | 0.7770 | 0.1857 | LGB 100% |

- **Best Ensemble**: **TabNet 90% + LightGBM 10%**
- F1スコアがTabNet単体 (0.2464) から **0.2472** へわずかに向上。
- TabNetが支配的であり、LightGBMはわずかな補正役として機能している。

### High Recall 領域での評価 (安全性重視)

死亡事故の見逃しを防ぐため、Recall ≥ 99% を維持した際の Precision を比較。

| モデル | Threshold | Precision @ R99 |
| :--- | :--- | :--- |
| TabNet | 0.1186 | 0.0198 |
| LightGBM | 0.0177 | 0.0190 |
| **Ensemble** | 0.0298 | **0.0199** |

- アンサンブルがHigh Recall領域でも僅差で最高性能を記録。
- 0.0001ポイントの差ではあるが、検出精度は最も高い。

## 3. 実装機能の検証結果

### 途中再開機能 (Resume Capability)
長時間の学習に対する耐性を高めるため実装された機能の検証を行った。

- **TabNet (`train_stage2_tabnet.py`)**:
    - **問題**: `save_model` が自動で拡張子 `.zip` を付与するため、コード内で `.zip` を明示すると `filename.zip.zip` となりロードに失敗する問題が発生。
    - **修正**: モデル保存パスの指定から拡張子を削除し、存在チェックロジックを修正。
    - **結果**: 修正後、既存の学習済みモデル (Fold 1-5) を正しく検知し、学習フェーズをスキップして評価フェーズから再開できることを確認。
- **LightGBM (`train_stage2_multiclass.py`)**:
    - `joblib` を用いた保存/ロード機能が一発で正常に動作。
    - 既存モデル検知と学習スキップを確認。

### バグ修正
- **変数名エラー**: 両スクリプトの `evaluate` メソッド内で未定義変数 `y_binary_stage2` の参照エラーが発生したが、正しい変数名 `y_s2_bin` (TabNet) / `y_s2_bin` (LGB) に修正し解決済み。

## 4. 結論と考察

1.  **TabNetの優位性**:
    - 本データセットにおいて、Deep LearningベースのTabNetがGBDT(LightGBM)よりも高い性能を示した。
    - カテゴリカル変数のEmbeddingや、Feature Transformerによる非線形な相互作用の学習が効果を発揮していると考えられる。

2.  **アンサンブルの推奨**:
    - **性能重視**: TabNet 90% + LightGBM 10% のアンサンブル採用を推奨。F1スコア、高Recall域でのPrecisionともに最高値。
    - **運用コスト重視**: 改善幅が微小であるため、運用・保守コストを抑えたい場合は **TabNet単体** でも十分な性能と言える。LightGBMを除外することで推論パイプラインを簡素化できる。

3.  **今後の展望**:
    - 現状のアンサンブル相関 (0.74) は比較的高いため、モデルの多様性が不足している可能性がある。
    - ニューラルネットワーク(TabNet)とGBDT(LightGBM)の組み合わせは定石だが、TabNetが強すぎるためGBDTの寄与が薄い。
    - さらなる精度向上には、Denoising Autoencoder (DAE) 特徴量の追加や、全く異なるアプローチ（例: 単純なMLPや、さらに別のTabular NNアーキテクチャ）の導入が検討に値する。

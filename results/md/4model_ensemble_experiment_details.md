# 4モデル比較・アンサンブル実験 (Stage 2) 詳細ドキュメント

## 1. 実験の目的
交通事故の「死亡(Fatal)」か「負傷(Injury)」かを判別する Stage 2 モデルにおいて、最適なアルゴリズムとその組み合わせ（アンサンブル）を探索します。
特に、**高性能PC (Intel Core Ultra 9 285K)** の計算資源を最大限に活用し、GBDT（決定木）系とDeep Learning系の異なる性質を持つモデルを組み合わせることで、単体モデルの限界を超える精度を目指します。

## 2. 使用データ
- **ファイル名**: `honhyo_for_analysis_with_traffic_hospital_no_leakage.csv`
- **特徴**: 
    - 通常の事故データに加え、**周辺の病院情報**や**交通量データ**が含まれたリッチなデータセット。
    - リーク（予測時に知り得ない未来の情報）が厳密に排除されています。
- **ターゲット**: `fatal` (1=死亡事故, 0=負傷事故)
- **データ規模**: 約190万件（不均衡データ：死亡事故は約0.86%）

## 3. 比較対象モデルと設定 (v2 修正版)

本実験では、以下の4つの強力なモデルを比較・統合します。

| モデル | 種類 | 特徴・役割 | v2での重要な修正点 |
| :--- | :--- | :--- | :--- |
| **1. LightGBM** | GBDT | **高速・高精度**。不均衡データに強い `scale_pos_weight` を適用。ベースラインとして信頼性が高い。 | 特になし（安定版を使用） |
| **2. CatBoost** | GBDT | **カテゴリ変数に極めて強い**。Overfittingを防ぐOrdered Boostingを採用。現在の実験で単体最高性能を記録中。 | 特になし |
| **3. TabNet** | NN | **テーブルデータ特化型Deep Learning**。決定木のような解釈性とNNの表現力を兼ね備える。 | データ量が多いため、バッチサイズを8192に拡大し学習を安定化。 |
| **4. MLP** | NN | **シンプルな多層パーセプトロン**。非線形な関係を捉える純粋なニューラルネット。 | **Target Encoding + リーク防止実装**<br>カテゴリ変数を単なる数値として扱う間違い（Label Encoding）を修正し、正しく確率情報として入力することで精度が大幅向上。 |

## 4. 実験手法・プロセス

### A. 評価方法 (Validation Strategy)
- **5-Fold Stratified Cross Validation**: データを5分割し、ターゲットの比率（死亡事故の割合）を保ったまま検証。偶然の偏りを排除。
- **ホールドアウト検証**: 学習に一切使わないテストデータ（全体の20%）を確保し、最終的な汎化性能を確認。

### B. 実装の工夫 (High-Performance Optimization)
- **並列処理**: 20コアを使用し、データ読み込みや前処理を高速化。
- **GPU活用**: TabNet, MLP, CatBoost, LightGBM 全てでGPU (CUDA) 加速を利用（可能な場合）。

### C. アンサンブル手法 (Optimization Strategy)
- **LogLossによる重み最適化**: 
    - 単純なF1スコア（閾値0.5固定）での最適化は、不均衡データにおいて不適切でした（全ての予測を0にしてしまうリスクがあるため）。
    - 修正版(v2)では、確率の正確さを測る **LogLoss** を最小化するように、各モデルの重み（例: LightGBM 30%, CatBoost 50%...）を決定します。

## 5. 現在の進捗状況 (途中経過)

実験は順調に進行中で、現在 **Fold 3** が佳境に入っています（全5 Folds）。

| モデル | Fold 1 AUC | Fold 2 AUC | Fold 3 AUC (速報) | 特記事項 |
| :--- | :--- | :--- | :--- | :--- |
| **CatBoost** | **0.9077** | **0.9022** | **0.9063** | **圧倒的に高性能**。単体での採用候補筆頭。 |
| **MLP (v2)** | 0.8916 | 0.8850 | (計算中) | 修正により精度が急上昇。CatBoostに次ぐ2番手。 |
| **LightGBM** | 0.8855 | 0.8712 | 0.8785 | 安定しているが、CatBoost/MLPには及ばず。 |
| **TabNet** | 0.8785 | 0.8760 | 0.8652 | 健闘しているが、Deep Learning枠ではMLPに劣る傾向。 |

**暫定的な結論**:
「病院・交通量データ」を最も上手く使えているのは **CatBoost** です。また、**MLP** もTarget Encoding修正のおかげで非常に高い性能を発揮しており、この2つを軸にしたアンサンブルが最終的な最高精度を出すと予想されます。

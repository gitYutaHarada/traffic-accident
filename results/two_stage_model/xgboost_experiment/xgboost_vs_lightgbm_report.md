# Stage 1 モデル比較実験レポート: LightGBM vs XGBoost

**日付:** 2025年12月18日
**目的:** Stage 1における不要データ除去率（フィルタリング能力）の向上を目指し、XGBoostの導入およびアンサンブルの有効性を検証する。

## 1. 実験概要

2段階モデルのStage 1（Recall 99%を維持した上での負例除去）において、現在のLightGBMに加えてXGBoostを導入し、以下の2点を検証した。

1.  **単独性能:** XGBoostはLightGBMより高性能か？
2.  **相補性 (多様性):** 両モデルの予測に違いがあり、アンサンブル（平均化）による精度向上が見込めるか？

### モデル設定
| 項目 | LightGBM (Baseline) | XGBoost (Challenger) |
| :--- | :--- | :--- |
| **アルゴリズム** | Gradient Boosting (Leaf-wise) | Gradient Boosting (Level-wise) |
| **パラメータ** | `num_leaves=31` | `max_depth=8`, `tree_method='hist'` |
| **サンプリング** | 1:2 Under-sampling | 1:2 Under-sampling |
| **不均衡対策** | なし | `scale_pos_weight=2.0` |

---

## 2. 実験結果

### 定量評価 (Recall 99%基準)

| 指標 | LightGBM | XGBoost | 評価 |
| :--- | :--- | :--- | :--- |
| **OOF AUC** | **0.9047** | 0.8990 | LightGBMがわずかに優れる |
| **除外率 (Filter Rate)** | **25.25%** | 23.39% | LightGBMが約1.8pt高い |
| **除外数** | **約1,416,700件** | 約1,452,000件 | LightGBMの方が約3.5万件多く除去 |

> **除外率とは**: 死亡事故を99%以上見逃さない（Recall >= 0.99）閾値を設定した際、全データのうち何%を「安全」として捨てられるか。高いほど良い。

### 相関分析 (Diversity Analysis)

両モデルの予測確率（OOF Probability）の相関係数を算出した。

-   **相関係数: 0.9659**

---

## 3. 考察

### 多様性の不足
相関係数が **0.966** と極めて高い値を示した。これは、LightGBMとXGBoostが「ほぼ同じデータに対して、同じような確信度で」予測していることを意味する。
アンサンブル（Model Averaging）が効果を発揮するためには、相関係数が0.90以下（できれば0.80台）であることが望ましいとされる。今回の結果からは、これらを組み合わせても大幅な精度向上は期待できない。

### 単独性能の差
AUCおよび除外率において、LightGBMがXGBoostを上回った。
-   LightGBMの方が大規模データにおける複雑な境界線の学習に適している可能性がある。
-   XGBoostに導入した不均衡対策（`scale_pos_weight`）が、逆に「迷い」を生じさせ、スコアの分離を悪化させた可能性もある。

## 4. 結論と推奨

1.  **LightGBMの継続採用:**
    単独での性能が高く、計算速度にも優れるLightGBMを引き続きStage 1のメインモデルとして採用することを推奨する。

2.  **アンサンブルの見送り:**
    XGBoostとのアンサンブルによるメリットは、計算コストの増加（学習時間2倍）に見合わないため、現時点では採用しない。

3.  **今後の方向性:**
    もし今後さらにフィルタリング精度を追求する場合、決定木ベースではないモデル（Logistic RegressionやNeural Network）とのアンサンブルを検討すべきである。

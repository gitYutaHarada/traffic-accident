# モデル比較検証結果レポート

**実施日**: 2025年12月13日
**使用データ**: `honhyo_clean_road_type.csv`

## 1. 3モデル比較結果

**🏆 最優秀モデル: Random Forest**

Random Forestが全ての指標において、LightGBMとロジスティック回帰を上回る結果となりました。

| 順位 | モデル | PR-AUC | ROC-AUC | F1 Score |
| :--- | :--- | :--- | :--- | :--- |
| **1位** | **Random Forest** | **0.1271** | **0.8892** | **0.1293** |
| 2位 | LightGBM | 0.0677 | 0.8490 | 0.0881 |
| 3位 | ロジスティック回帰 | 0.0501 | 0.8054 | 0.0467 |

### 📊 結果の考察

- **Random Forestの圧勝**:
  - ベースラインのロジスティック回帰と比較して、PR-AUCで約2.5倍の性能を示しました。
  - 驚くべきことに、通常高性能なLightGBMよりも優れた結果を出しています。

- **LightGBMの苦戦**:
  - 期待されたほどの性能が出ませんでした。
  - **考えられる原因**:
    1. 前回のCatBoostエラー修正時に追加した「カテゴリカル変数のOrdinal Encoding処理」が、LightGBM本来のカテゴリ処理よりも精度を落としている可能性があります。
    2. 使用しているハイパーパラメータが、今回のデータセット（`honhyo_clean_road_type.csv`）に最適化されていない可能性があります。

- **統計的有意差**:
  - Friedman検定の結果（p値 < 0.01）、この性能差は偶然ではなく、統計的に有意であることが確認されました。

## 2. 5モデル比較結果

**⚠️ 結果ファイルが見つかりませんでした**

- `compare_five_models.py` の実行結果（CSV/レポート）が出力されていません。
- **原因**: 昨夜のエラーログにある通り、`CatBoost` ライブラリが見つからないため、スクリプトが開始直後に停止した可能性が高いです。

### 今後のアクション提案

1. **Random Forestの採用**: 現時点ではRandom Forestが最も信頼できるモデルです。
2. **5モデル比較の再実行**: どうしても他モデル（XGBoost, CatBoost）も見たい場合は、ライブラリのインストール問題を解決してから再実行が必要です。
